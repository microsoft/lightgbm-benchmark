{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LightGBM benchmarking suite The LightGBM benchmark aims at providing tools and automation to compare implementations of lightgbm and other boosting-tree-based algorithms for both training and inferencing. The focus is on production use cases, and the evaluation on both model quality (validation metrics) and computing performance (training speed, compute hours, inferencing latency, etc). The goal is to support the community of developers of LightGBM by providing tools and a methodology for evaluating new releases of LightGBM on a standard and reproducible benchmark. Documentation Please find the full documentation of this project at microsoft.github.io/lightgbm-benchmark Contributing This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Trademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines . Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.","title":"Home"},{"location":"#lightgbm-benchmarking-suite","text":"The LightGBM benchmark aims at providing tools and automation to compare implementations of lightgbm and other boosting-tree-based algorithms for both training and inferencing. The focus is on production use cases, and the evaluation on both model quality (validation metrics) and computing performance (training speed, compute hours, inferencing latency, etc). The goal is to support the community of developers of LightGBM by providing tools and a methodology for evaluating new releases of LightGBM on a standard and reproducible benchmark.","title":"LightGBM benchmarking suite"},{"location":"#documentation","text":"Please find the full documentation of this project at microsoft.github.io/lightgbm-benchmark","title":"Documentation"},{"location":"#contributing","text":"This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"Contributing"},{"location":"#trademarks","text":"This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines . Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.","title":"Trademarks"},{"location":"lightgbm-benchmark-project/","text":"LightGBM benchmark project: goals and methodology The LightGBM benchmark aims at providing tools and automation to compare implementations of lightgbm and other boosting-tree-based algorithms for both training and inferencing. The focus is on production use cases, and the evaluation on both model quality (validation metrics) and computing performance (training speed, compute hours, inferencing latency, etc). We'll focus on LightGBM first, and expand it to other learners as we go along. The goal is to support the community of developers of LightGBM by providing tools and a methodology for evaluating new releases of LightGBM on a standard and reproducible benchmark. Implementation Goals In addition of the description above, the code provided in this repo follows three goals, in the decreasing order of priority: Automation of routines - The code should provide means to automate usual benchmark routines such as reporting metrics, orchestrating the benchmark on multiple data sources and multiple configurations, comparing/analyzing multiple runs, etc. Reproducible and self-service - The benchmark code should allow users to reproduce the results on their end, or adapt it to another environment to run the full benchmark on their data. Act as reference - The scripts provided in the benchmark could be used as reference / standard implementations in python for training and inferencing. The code should be readable, documented and unit tested. It should make the best and up-to-date use of the python libraries it proposes to evaluate. Running the benchmark We propose the benchmark to be available both for manual or orchestrated (cloud-based) run. Manual - Users can reproduce the benchmark results while running the code locally to a given VM. Orchestrated - Users can run the benchmark in a cloud-based ML orchestration platform (ex: AzureML). We also want to identify how to enable users to run the benchmark on custom lightgbm builds (either local or from a github branch), so that they can assess the performance of their changes, or use different compilation options. Data and tasks The users should be able to run the benchmark on multiple data sources. Not only we want to provide synthetic and public data benchmarks, but we also propose to make sure users can use the benchmark on their on data sets. This can be facilitated by using a cloud-based ML platform like AzureML. Synthetic Data - Generated fake data, in particular to test scalability, inferencing latency. Public Data - Example public datasets used in the community and literature to benchmark the learners. Production Data - Provide the ability for users to run the benchmark on their own data. Code Structure To allow for modularity and standardization, we propose to structure the benchmark into a collection of independent python scripts (specs in a separate document). Each script will have its own environment, inputs, outputs and parameters. All scripts will eventually draw from common helper classes to support implementation routines (reporting, argument parsing, etc). Benchmarking scenarios and reporting The benchmark could be used in the following comparison scenarios: Training framework comparison - Compare lightgbm versus other training (or inferencing) ML framework. This comparison will be mainly based on model quality, but also training computing performance. It will run those framework on typical public datasets. Lightgbm performance and scalability - Compare lightgbm (stable or custom) to itself while changing compiling options, VMs and environments. This will likely use synthetic data or large public datasets. This is where allowing users to run on their own production data makes more sense. Lightgbm \"master\" vs lightgbm \"custom\" - Compare any new implementation of lightgbm against the current stable release. This comparison will ensure model quality is equivalent, and will focus on compute performance. For each of those scenarios, we'll need to report both the variables subject to change (framework, lightgbm version, VMs, dependencies, etc) and the metrics produces during training or inferencing. A list of those variables and metrics is provided in the Reporting Guide . Phases and Milestones We propose the following process to iterate on implementation. 1. Foundations phase In this first phase, we'll implement the foundations to allow the benchmark to grow later. We'll start with: - a couple minimal scripts for generating synthetic data, training lightgbm and inferencing - some common libraries around metrics reporting - a first benchmark pipeline in either AzureML SDK1.5 or SDK2.0 to run end-to-end and report metrics - enabling both manual and orchestrated runs of the benchmark - a build with a first set of unit tests - documentation of the goals, developer guide and common routines Milestone : an end-to-end run of the benchmark both locally (VM) and in the cloud, reporting a minimal set of wall time metrics, running on synthetic data, producing numbers we manually report in the repo as markdown. 2. Standardization phase This phase will see the implementation of features to automate or speed-up typical user routines. In particular, we anticipate supporting the following use cases: - running the benchmark on custom lightgbm builds and/or branches - analyzing the benchmark results in a systematic script for crunching numbers and compile a readable report - a nice report format for comparing pairs of lightgbm implementations (ex: master vs branch), or multiple implementations against one. - systematic testing of every script in a consistent unit test methodology - identifying ways to standardize code to avoid duplication Milestone : an end-to-end benchmark run producing report automatically, comparing pairs of variants of lightgbm (ex: stable vs custom build). 3. Expansion phase In this phase, we will expand the number of benchmark scripts and pipelines to cover the following: - implementation of multi-node lightgbm training and inferencing - implementation of xgboost, catboost, and others - implementation of more public datasets, data processors Milestone : an end-to-end benchmark pipeline running on 3+ public datasets, comparing 3+ tree-based frameworks. Appendix Example existing benchmarks https://github.com/catboost/benchmarks https://github.com/guolinke/boosting_tree_benchmarks https://sites.google.com/view/lauraepp/benchmarks/xgb-vs-lgb-feb-2017 https://github.com/Laurae2/gbt_benchmarks","title":"About"},{"location":"lightgbm-benchmark-project/#lightgbm-benchmark-project-goals-and-methodology","text":"The LightGBM benchmark aims at providing tools and automation to compare implementations of lightgbm and other boosting-tree-based algorithms for both training and inferencing. The focus is on production use cases, and the evaluation on both model quality (validation metrics) and computing performance (training speed, compute hours, inferencing latency, etc). We'll focus on LightGBM first, and expand it to other learners as we go along. The goal is to support the community of developers of LightGBM by providing tools and a methodology for evaluating new releases of LightGBM on a standard and reproducible benchmark.","title":"LightGBM benchmark project: goals and methodology"},{"location":"lightgbm-benchmark-project/#implementation-goals","text":"In addition of the description above, the code provided in this repo follows three goals, in the decreasing order of priority: Automation of routines - The code should provide means to automate usual benchmark routines such as reporting metrics, orchestrating the benchmark on multiple data sources and multiple configurations, comparing/analyzing multiple runs, etc. Reproducible and self-service - The benchmark code should allow users to reproduce the results on their end, or adapt it to another environment to run the full benchmark on their data. Act as reference - The scripts provided in the benchmark could be used as reference / standard implementations in python for training and inferencing. The code should be readable, documented and unit tested. It should make the best and up-to-date use of the python libraries it proposes to evaluate.","title":"Implementation Goals"},{"location":"lightgbm-benchmark-project/#running-the-benchmark","text":"We propose the benchmark to be available both for manual or orchestrated (cloud-based) run. Manual - Users can reproduce the benchmark results while running the code locally to a given VM. Orchestrated - Users can run the benchmark in a cloud-based ML orchestration platform (ex: AzureML). We also want to identify how to enable users to run the benchmark on custom lightgbm builds (either local or from a github branch), so that they can assess the performance of their changes, or use different compilation options.","title":"Running the benchmark"},{"location":"lightgbm-benchmark-project/#data-and-tasks","text":"The users should be able to run the benchmark on multiple data sources. Not only we want to provide synthetic and public data benchmarks, but we also propose to make sure users can use the benchmark on their on data sets. This can be facilitated by using a cloud-based ML platform like AzureML. Synthetic Data - Generated fake data, in particular to test scalability, inferencing latency. Public Data - Example public datasets used in the community and literature to benchmark the learners. Production Data - Provide the ability for users to run the benchmark on their own data.","title":"Data and tasks"},{"location":"lightgbm-benchmark-project/#code-structure","text":"To allow for modularity and standardization, we propose to structure the benchmark into a collection of independent python scripts (specs in a separate document). Each script will have its own environment, inputs, outputs and parameters. All scripts will eventually draw from common helper classes to support implementation routines (reporting, argument parsing, etc).","title":"Code Structure"},{"location":"lightgbm-benchmark-project/#benchmarking-scenarios-and-reporting","text":"The benchmark could be used in the following comparison scenarios: Training framework comparison - Compare lightgbm versus other training (or inferencing) ML framework. This comparison will be mainly based on model quality, but also training computing performance. It will run those framework on typical public datasets. Lightgbm performance and scalability - Compare lightgbm (stable or custom) to itself while changing compiling options, VMs and environments. This will likely use synthetic data or large public datasets. This is where allowing users to run on their own production data makes more sense. Lightgbm \"master\" vs lightgbm \"custom\" - Compare any new implementation of lightgbm against the current stable release. This comparison will ensure model quality is equivalent, and will focus on compute performance. For each of those scenarios, we'll need to report both the variables subject to change (framework, lightgbm version, VMs, dependencies, etc) and the metrics produces during training or inferencing. A list of those variables and metrics is provided in the Reporting Guide .","title":"Benchmarking scenarios and reporting"},{"location":"lightgbm-benchmark-project/#phases-and-milestones","text":"We propose the following process to iterate on implementation.","title":"Phases and Milestones"},{"location":"lightgbm-benchmark-project/#1-foundations-phase","text":"In this first phase, we'll implement the foundations to allow the benchmark to grow later. We'll start with: - a couple minimal scripts for generating synthetic data, training lightgbm and inferencing - some common libraries around metrics reporting - a first benchmark pipeline in either AzureML SDK1.5 or SDK2.0 to run end-to-end and report metrics - enabling both manual and orchestrated runs of the benchmark - a build with a first set of unit tests - documentation of the goals, developer guide and common routines Milestone : an end-to-end run of the benchmark both locally (VM) and in the cloud, reporting a minimal set of wall time metrics, running on synthetic data, producing numbers we manually report in the repo as markdown.","title":"1. Foundations phase"},{"location":"lightgbm-benchmark-project/#2-standardization-phase","text":"This phase will see the implementation of features to automate or speed-up typical user routines. In particular, we anticipate supporting the following use cases: - running the benchmark on custom lightgbm builds and/or branches - analyzing the benchmark results in a systematic script for crunching numbers and compile a readable report - a nice report format for comparing pairs of lightgbm implementations (ex: master vs branch), or multiple implementations against one. - systematic testing of every script in a consistent unit test methodology - identifying ways to standardize code to avoid duplication Milestone : an end-to-end benchmark run producing report automatically, comparing pairs of variants of lightgbm (ex: stable vs custom build).","title":"2. Standardization phase"},{"location":"lightgbm-benchmark-project/#3-expansion-phase","text":"In this phase, we will expand the number of benchmark scripts and pipelines to cover the following: - implementation of multi-node lightgbm training and inferencing - implementation of xgboost, catboost, and others - implementation of more public datasets, data processors Milestone : an end-to-end benchmark pipeline running on 3+ public datasets, comparing 3+ tree-based frameworks.","title":"3. Expansion phase"},{"location":"lightgbm-benchmark-project/#appendix","text":"","title":"Appendix"},{"location":"lightgbm-benchmark-project/#example-existing-benchmarks","text":"https://github.com/catboost/benchmarks https://github.com/guolinke/boosting_tree_benchmarks https://sites.google.com/view/lauraepp/benchmarks/xgb-vs-lgb-feb-2017 https://github.com/Laurae2/gbt_benchmarks","title":"Example existing benchmarks"},{"location":"contribute/architecture-guide/","text":"Architecture of benchmark scripts The benchmark repo follows an architecture where: All scripts must run both locally and in AzureML. All scripts should follow a standard reporting framework. Code duplication should be kept minimal. To achieve that, we have based our code on an object oriented architecture. The following document provides a walkthrough of that architecture to help you locate where you can contribute. 1. Structure of the repository Directory Content /.github/workflows/ github workflows for this project /conf/ Configuration files for pipelines and experiments running in AzureML. /data/ Use to create local outputs, or find local data. This directory is in .gitignore to avoid pushing data in git. /docker/ Example environments to run some of the modules (ex: multiple containers for different versions of lightgbm). /docs/ Documentation in markdown /src/ /src/binaries/ C++ code for some binaries used in the benchmark (ex: lightgbm C API inferencing). /src/common/ Common libraries for all benchmark scripts /src/pipelines/ Scripts or config files for running the benchmark in AzureML. /src/scripts/ Benchmark scripts /tests/ Unit tests using pytest 2. Architecture of runnable scripts in src/scripts/ Every script under src/scripts/ can be run manually. Each runnable script inherits from RunnableScript helper class. These classes provides all the standard reporting features, and contains the boilerplate code that would otherwise be duplicated. 2.1. There's a helper class for each implementation scenario The benchmark src/common/ folder contains multiple \"helper classes\" that all derive from RunnableScript , and cover some typical implementation scenarios: single node, multi node using mpi, multi node using Ray, and others (TBD). Each script will always have at least the following methods: - __init__() : this will just call the super class __init__(task, framework, version) with the right arguments. - get_arg_parser() : this populates an argument parser with the specific arguments of this script, and uses super class to populate generic arguments. - run(args, logger, metrics_logger, unknown_args) : this is the \"main\" method of the script, the helper class will call it after initializing all resources, will take care of reporting exceptions and resource utilization while running, and will finalize resources upon return. The arguments of the run() method provide the loggers needed by the script to report logs, or metrics, using our common mlflow helping methods. The script doesn't have a specific main() method. Instead, they rely on the helper class main() method which will orchestrate script initialization and finalization. 2.2. Scripts rely on standard arguments Each script is runnable locally. They will have multiple sets of arguments, some are specific to the script itself (ex: lightgbm number of trees), some others are generic to the implementation scenario (ex: which metric reporting scheme to use). When you display usage of a script, for instance python . \\s rc \\s cripts \\t raining \\l ightgbm_python \\t rain.py -h watch out for arguments groups showing which argument relates to which layer of the architecture. For instance below (extract): optional arguments: -h, --help show this help message and exit General parameters [common.components:RunnableScript]: --verbose VERBOSE set True to show DEBUG logs --custom_properties CUSTOM_PROPERTIES provide custom properties as json dict --disable_perf_metrics DISABLE_PERF_METRICS disable performance metrics (default: False) --metrics_driver {mlflow,azureml} which class to use to report metrics mlflow or azureml MultiNode runtime parameters [common.distributed:MultiNodeScript]: --multinode_driver {mpi,socket} --multinode_machines MULTINODE_MACHINES list of machines, use only when running locally, default will use 'auto' to discover --multinode_listen_port MULTINODE_LISTEN_PORT used for socket only, default 12345 LightGBM learning parameters [__main__:LightGBMPythonMultiNodeTrainingScript]: --objective OBJECTIVE --metric METRIC --boosting_type BOOSTING_TYPE --tree_learner TREE_LEARNER --label_gain LABEL_GAIN --num_trees NUM_TREES --num_leaves NUM_LEAVES --min_data_in_leaf MIN_DATA_IN_LEAF --learning_rate LEARNING_RATE --max_bin MAX_BIN --feature_fraction FEATURE_FRACTION --device_type DEVICE_TYPE --custom_params CUSTOM_PARAMS You'll notice 3 argument groups: - General parameters derive from the class [ common.components:RunnableScript ] - MultiNode runtime parameters derive from the class [ common.distributed:MultiNodeScript ] - LightGBM learning parameters are the __main__ arguments from this particular script. 2.3. All scripts report the same performance metrics Thanks to the scripts using a common set of helper classes, they report a unified set of metrics for performance monitoring. You can find a list of those metrics in the reference docs . Those metrics are reported using mlflow. When running locally, you can use mlflow to query the values and artifacts reported by each script. When running in AzureML, those values will flow in the UI for you to browse. 3. Architecture of pipelines WORK IN PROGRESS","title":"Architecture Guide"},{"location":"contribute/architecture-guide/#architecture-of-benchmark-scripts","text":"The benchmark repo follows an architecture where: All scripts must run both locally and in AzureML. All scripts should follow a standard reporting framework. Code duplication should be kept minimal. To achieve that, we have based our code on an object oriented architecture. The following document provides a walkthrough of that architecture to help you locate where you can contribute.","title":"Architecture of benchmark scripts"},{"location":"contribute/architecture-guide/#1-structure-of-the-repository","text":"Directory Content /.github/workflows/ github workflows for this project /conf/ Configuration files for pipelines and experiments running in AzureML. /data/ Use to create local outputs, or find local data. This directory is in .gitignore to avoid pushing data in git. /docker/ Example environments to run some of the modules (ex: multiple containers for different versions of lightgbm). /docs/ Documentation in markdown /src/ /src/binaries/ C++ code for some binaries used in the benchmark (ex: lightgbm C API inferencing). /src/common/ Common libraries for all benchmark scripts /src/pipelines/ Scripts or config files for running the benchmark in AzureML. /src/scripts/ Benchmark scripts /tests/ Unit tests using pytest","title":"1. Structure of the repository"},{"location":"contribute/architecture-guide/#2-architecture-of-runnable-scripts-in-srcscripts","text":"Every script under src/scripts/ can be run manually. Each runnable script inherits from RunnableScript helper class. These classes provides all the standard reporting features, and contains the boilerplate code that would otherwise be duplicated.","title":"2. Architecture of runnable scripts in src/scripts/"},{"location":"contribute/architecture-guide/#21-theres-a-helper-class-for-each-implementation-scenario","text":"The benchmark src/common/ folder contains multiple \"helper classes\" that all derive from RunnableScript , and cover some typical implementation scenarios: single node, multi node using mpi, multi node using Ray, and others (TBD). Each script will always have at least the following methods: - __init__() : this will just call the super class __init__(task, framework, version) with the right arguments. - get_arg_parser() : this populates an argument parser with the specific arguments of this script, and uses super class to populate generic arguments. - run(args, logger, metrics_logger, unknown_args) : this is the \"main\" method of the script, the helper class will call it after initializing all resources, will take care of reporting exceptions and resource utilization while running, and will finalize resources upon return. The arguments of the run() method provide the loggers needed by the script to report logs, or metrics, using our common mlflow helping methods. The script doesn't have a specific main() method. Instead, they rely on the helper class main() method which will orchestrate script initialization and finalization.","title":"2.1. There's a helper class for each implementation scenario"},{"location":"contribute/architecture-guide/#22-scripts-rely-on-standard-arguments","text":"Each script is runnable locally. They will have multiple sets of arguments, some are specific to the script itself (ex: lightgbm number of trees), some others are generic to the implementation scenario (ex: which metric reporting scheme to use). When you display usage of a script, for instance python . \\s rc \\s cripts \\t raining \\l ightgbm_python \\t rain.py -h watch out for arguments groups showing which argument relates to which layer of the architecture. For instance below (extract): optional arguments: -h, --help show this help message and exit General parameters [common.components:RunnableScript]: --verbose VERBOSE set True to show DEBUG logs --custom_properties CUSTOM_PROPERTIES provide custom properties as json dict --disable_perf_metrics DISABLE_PERF_METRICS disable performance metrics (default: False) --metrics_driver {mlflow,azureml} which class to use to report metrics mlflow or azureml MultiNode runtime parameters [common.distributed:MultiNodeScript]: --multinode_driver {mpi,socket} --multinode_machines MULTINODE_MACHINES list of machines, use only when running locally, default will use 'auto' to discover --multinode_listen_port MULTINODE_LISTEN_PORT used for socket only, default 12345 LightGBM learning parameters [__main__:LightGBMPythonMultiNodeTrainingScript]: --objective OBJECTIVE --metric METRIC --boosting_type BOOSTING_TYPE --tree_learner TREE_LEARNER --label_gain LABEL_GAIN --num_trees NUM_TREES --num_leaves NUM_LEAVES --min_data_in_leaf MIN_DATA_IN_LEAF --learning_rate LEARNING_RATE --max_bin MAX_BIN --feature_fraction FEATURE_FRACTION --device_type DEVICE_TYPE --custom_params CUSTOM_PARAMS You'll notice 3 argument groups: - General parameters derive from the class [ common.components:RunnableScript ] - MultiNode runtime parameters derive from the class [ common.distributed:MultiNodeScript ] - LightGBM learning parameters are the __main__ arguments from this particular script.","title":"2.2. Scripts rely on standard arguments"},{"location":"contribute/architecture-guide/#23-all-scripts-report-the-same-performance-metrics","text":"Thanks to the scripts using a common set of helper classes, they report a unified set of metrics for performance monitoring. You can find a list of those metrics in the reference docs . Those metrics are reported using mlflow. When running locally, you can use mlflow to query the values and artifacts reported by each script. When running in AzureML, those values will flow in the UI for you to browse.","title":"2.3. All scripts report the same performance metrics"},{"location":"contribute/architecture-guide/#3-architecture-of-pipelines","text":"WORK IN PROGRESS","title":"3. Architecture of pipelines"},{"location":"contribute/developer-guide/","text":"Developer guide The following document details the proposed structure for this repo, the benchmark scripts and libraries. Hint For a quickstart: please check out /src/scripts/sample/sample.py for a full sample python script implementing the recommendations below. General design and motivations The goal of the benchmark repo is to provide scripts to compare tree-based machine learning framework on similar tasks and environment. The benchmark is made of a collection of scripts. These scripts can be run locally, or manually from within a remote VM. But these scripts will eventually be submitted as a pipeline or workflow chaining scripts in a sequence: data generation, data processing, training, inferencing. In order to do that, we need each step for benchmarking a given framework to be implemented as a single script with clear inputs, outputs and parameters. We organize those into sub-directories under /src/scripts/ . For the benchmark to automate the analysis of the results and the production of a benchmark report, we need each script to report a common set of metrics or tags. Proposed structure of the repo Directory Content /.github/workflows/ github workflows for this project /docs/ Documentation in markdown /pipelines/ (coming) Scripts or config files for running the benchmark in the cloud (ex: AzureML) /src/ /src/common/ Common libraries for all benchmark scripts /src/scripts/ Benchmark scripts /tests/ Unit tests using pytest Proposed structure of the scripts Scripts will be written in python . Even if the module is calling a CLI command (ex: lightgbm.exe), writing it in python will allow us to wrap it with metrics reporting, consistent unit tests, etc. Scripts will have a minimum of three functions get_arg_parser() , run() and main() (see below code template). The value of this code template is: - this template is the current structure expected by the shrike contrib package, letting us automate a lot of the unit testing, integration testing, etc, - we can create unit tests easily to test all script arguments for consistency, - the arg parsers can also be built in conjonction with other parsers (ex: a super script adding all the arguments from multiple scripts). The main() function is likely to be the same in each script. The alternative could be to create a script class with pre-implemented main() method. But each script could become a little less readable for beginner users wanting to run and edit those scripts. Each script should be runnable on its own, and provide a clear set of usage instructions when using -h option. Scripts inputs, outputs and parameters Each script get_arg_parser() function builds an argparse for the script which details the inputs, outputs and parameters. For inputs and outputs, we propose to use directories exclusively . The goal is to be compatible with pipeline orchestration platforms (like AzureML) where inputs can be either file paths or directory paths, depending on the platform. For this, each script input will be considered a distinct directory containing at least one file (several if needed). Each script output will be provided as a directory in which the script will create one or multiple files on its own. Two helper functions in the common library will help to handle that type of i/o arguments consistently through the benchmark. Common libraries We'll build common libraries under /src/common/ to help script developers to achieve common routines in a standard and consistent manner. For instance: - mlflow metrics recording - managing typical arguments (ex: input/output folders with single file) Reporting standards Each script takes care of reporting relevant properties, parameters and metrics. How to set those is detailed in the reporting guide (work in progress).","title":"Developer Guide"},{"location":"contribute/developer-guide/#developer-guide","text":"The following document details the proposed structure for this repo, the benchmark scripts and libraries. Hint For a quickstart: please check out /src/scripts/sample/sample.py for a full sample python script implementing the recommendations below.","title":"Developer guide"},{"location":"contribute/developer-guide/#general-design-and-motivations","text":"The goal of the benchmark repo is to provide scripts to compare tree-based machine learning framework on similar tasks and environment. The benchmark is made of a collection of scripts. These scripts can be run locally, or manually from within a remote VM. But these scripts will eventually be submitted as a pipeline or workflow chaining scripts in a sequence: data generation, data processing, training, inferencing. In order to do that, we need each step for benchmarking a given framework to be implemented as a single script with clear inputs, outputs and parameters. We organize those into sub-directories under /src/scripts/ . For the benchmark to automate the analysis of the results and the production of a benchmark report, we need each script to report a common set of metrics or tags.","title":"General design and motivations"},{"location":"contribute/developer-guide/#proposed-structure-of-the-repo","text":"Directory Content /.github/workflows/ github workflows for this project /docs/ Documentation in markdown /pipelines/ (coming) Scripts or config files for running the benchmark in the cloud (ex: AzureML) /src/ /src/common/ Common libraries for all benchmark scripts /src/scripts/ Benchmark scripts /tests/ Unit tests using pytest","title":"Proposed structure of the repo"},{"location":"contribute/developer-guide/#proposed-structure-of-the-scripts","text":"Scripts will be written in python . Even if the module is calling a CLI command (ex: lightgbm.exe), writing it in python will allow us to wrap it with metrics reporting, consistent unit tests, etc. Scripts will have a minimum of three functions get_arg_parser() , run() and main() (see below code template). The value of this code template is: - this template is the current structure expected by the shrike contrib package, letting us automate a lot of the unit testing, integration testing, etc, - we can create unit tests easily to test all script arguments for consistency, - the arg parsers can also be built in conjonction with other parsers (ex: a super script adding all the arguments from multiple scripts). The main() function is likely to be the same in each script. The alternative could be to create a script class with pre-implemented main() method. But each script could become a little less readable for beginner users wanting to run and edit those scripts. Each script should be runnable on its own, and provide a clear set of usage instructions when using -h option.","title":"Proposed structure of the scripts"},{"location":"contribute/developer-guide/#scripts-inputs-outputs-and-parameters","text":"Each script get_arg_parser() function builds an argparse for the script which details the inputs, outputs and parameters. For inputs and outputs, we propose to use directories exclusively . The goal is to be compatible with pipeline orchestration platforms (like AzureML) where inputs can be either file paths or directory paths, depending on the platform. For this, each script input will be considered a distinct directory containing at least one file (several if needed). Each script output will be provided as a directory in which the script will create one or multiple files on its own. Two helper functions in the common library will help to handle that type of i/o arguments consistently through the benchmark.","title":"Scripts inputs, outputs and parameters"},{"location":"contribute/developer-guide/#common-libraries","text":"We'll build common libraries under /src/common/ to help script developers to achieve common routines in a standard and consistent manner. For instance: - mlflow metrics recording - managing typical arguments (ex: input/output folders with single file)","title":"Common libraries"},{"location":"contribute/developer-guide/#reporting-standards","text":"Each script takes care of reporting relevant properties, parameters and metrics. How to set those is detailed in the reporting guide (work in progress).","title":"Reporting standards"},{"location":"contribute/reporting-guide/","text":"Reporting for the benchmark This document details a proposed framework to report values (parameters, metrics) so they can be compared or aggregated during the benchmark analysis. This reporting is facilitated by a library under /src/common/metrics.py . This page first introduces the specifications of the reporting for each benchmark script, then documents the common library functions to implement this reporting. Specifications of reporting As mentioned in the project definition , we'd like to address three benchmarking scenarios: 1. Training framework comparison (lightgbm versus other ML frameworks) 2. Lightgbm performance and scalability (lightgbm on different compute types) 3. Lightgbm \"master\" vs lightgbm \"custom\" (measuring progress of lightgbm versions) In order to do support those, we propose to report 3 kind of content: - properties : used to segment the analysis, they will be properties of the script (framework, version) or properties of the environment (VM types, dependencies, compilation settings, etc). - parameters : in particular for training, any relevant parameter passed to the script (ex: learning rate). - metrics : measures taken during the script, in particular various execution times or custom validation metrics (ex: RMSE). For all scripts, we'd like to have a minimal set of typical properties, parameters and metrics that each script will report. See /src/scripts/lightgbm_python/train.py for an example implementation of all of those. The following tables details each reporting entry, with their type and description. Common properties The purpose of properties is to let us segment the benchmarking analysis. For instance, comparing different frameworks against one another, or compare two lightgbm versions. Some of those properties can be reported by the scripts themselves (ex: python api version), some others will have to be reported by the orchestrator (ex: VM type on which the script is run). Entry Type Description task property the task of the script, picked in ['generate', 'train', 'score'] framework property an identifier for the ML algorithm being benchmarked (ex: lightgbm_python , treelite ). framework_version property the version of the framework (ex: \"3.2.1\" ). environment property Optional: log relevant dependencies and their version numbers as a dictionary. In order to facilitate recording all those, we could add as many system information we could get from python modules like platform . To learn how to report properties, see common library below. Common metrics The common metrics capture various times that we'll compare accross frameworks. If possible, we'd like the training and inferencing times to be distinct from data loading. If that's not possible, then to not report any data loading time and we'll figure out how to compare those during analysis. Entry Type Description time_data_loading metric time for loading the data before executing the task time_data_generation metric time for generating data (for task generate ) time_training metric time for training on previously loaded data (for task training ) time_inferencing metric time for inferencing on previously loaded data (for task inferencing ) To learn how to implement reporting those metrics, see common libary below. Parameters There's no common parameters yet. You can report anything as parameters. See how below. Using common report library To use the common report library, first import: from common.metrics import MetricsLogger Then, a typical logging session works as follows. 1. Open a session # initializes reporting of metrics with a session name metrics_logger = MetricsLogger ( \"lightgbm_python.score\" ) 2. Add common properties Make sure to provide the properties expected per specifications above. # add the common properties to the session metrics_logger . set_properties ( task = 'score' , framework = 'lightgbm_python' , framework_version = lightgbm . __version__ ) You can capture all relevant platform/system info by using helper code function: # will capture platform info and record as properties metrics_logger . set_platform_properties () Optionally, you can provide custom properties using json (for instance from CLI arguments), and report those using: # logger will parse the json metrics_logger . set_properties_from_json ( json_string ) 3. Add any parameters Any keyword arg of log_parameters() is submitted as a parameter in mlflow. metrics_logger . log_parameters ( ** lgbm_params ) 4. Compute wall-time using with statement To compute wall time, the MetricsLogger class provide a helper method you can use within a with statement: with metrics_logger . log_time_block ( \"time_training\" ): # anything within this code block will count in wall time booster = lightgbm . train ( lgbm_params , train_data , valid_sets = val_data ) # anything outside of that will not count This will record a metric \"time_training\" measuring the time spent for the execution of this code block (only).","title":"Reporting Guide"},{"location":"contribute/reporting-guide/#reporting-for-the-benchmark","text":"This document details a proposed framework to report values (parameters, metrics) so they can be compared or aggregated during the benchmark analysis. This reporting is facilitated by a library under /src/common/metrics.py . This page first introduces the specifications of the reporting for each benchmark script, then documents the common library functions to implement this reporting.","title":"Reporting for the benchmark"},{"location":"contribute/reporting-guide/#specifications-of-reporting","text":"As mentioned in the project definition , we'd like to address three benchmarking scenarios: 1. Training framework comparison (lightgbm versus other ML frameworks) 2. Lightgbm performance and scalability (lightgbm on different compute types) 3. Lightgbm \"master\" vs lightgbm \"custom\" (measuring progress of lightgbm versions) In order to do support those, we propose to report 3 kind of content: - properties : used to segment the analysis, they will be properties of the script (framework, version) or properties of the environment (VM types, dependencies, compilation settings, etc). - parameters : in particular for training, any relevant parameter passed to the script (ex: learning rate). - metrics : measures taken during the script, in particular various execution times or custom validation metrics (ex: RMSE). For all scripts, we'd like to have a minimal set of typical properties, parameters and metrics that each script will report. See /src/scripts/lightgbm_python/train.py for an example implementation of all of those. The following tables details each reporting entry, with their type and description.","title":"Specifications of reporting"},{"location":"contribute/reporting-guide/#common-properties","text":"The purpose of properties is to let us segment the benchmarking analysis. For instance, comparing different frameworks against one another, or compare two lightgbm versions. Some of those properties can be reported by the scripts themselves (ex: python api version), some others will have to be reported by the orchestrator (ex: VM type on which the script is run). Entry Type Description task property the task of the script, picked in ['generate', 'train', 'score'] framework property an identifier for the ML algorithm being benchmarked (ex: lightgbm_python , treelite ). framework_version property the version of the framework (ex: \"3.2.1\" ). environment property Optional: log relevant dependencies and their version numbers as a dictionary. In order to facilitate recording all those, we could add as many system information we could get from python modules like platform . To learn how to report properties, see common library below.","title":"Common properties"},{"location":"contribute/reporting-guide/#common-metrics","text":"The common metrics capture various times that we'll compare accross frameworks. If possible, we'd like the training and inferencing times to be distinct from data loading. If that's not possible, then to not report any data loading time and we'll figure out how to compare those during analysis. Entry Type Description time_data_loading metric time for loading the data before executing the task time_data_generation metric time for generating data (for task generate ) time_training metric time for training on previously loaded data (for task training ) time_inferencing metric time for inferencing on previously loaded data (for task inferencing ) To learn how to implement reporting those metrics, see common libary below.","title":"Common metrics"},{"location":"contribute/reporting-guide/#parameters","text":"There's no common parameters yet. You can report anything as parameters. See how below.","title":"Parameters"},{"location":"contribute/reporting-guide/#using-common-report-library","text":"To use the common report library, first import: from common.metrics import MetricsLogger Then, a typical logging session works as follows.","title":"Using common report library"},{"location":"contribute/reporting-guide/#1-open-a-session","text":"# initializes reporting of metrics with a session name metrics_logger = MetricsLogger ( \"lightgbm_python.score\" )","title":"1. Open a session"},{"location":"contribute/reporting-guide/#2-add-common-properties","text":"Make sure to provide the properties expected per specifications above. # add the common properties to the session metrics_logger . set_properties ( task = 'score' , framework = 'lightgbm_python' , framework_version = lightgbm . __version__ ) You can capture all relevant platform/system info by using helper code function: # will capture platform info and record as properties metrics_logger . set_platform_properties () Optionally, you can provide custom properties using json (for instance from CLI arguments), and report those using: # logger will parse the json metrics_logger . set_properties_from_json ( json_string )","title":"2. Add common properties"},{"location":"contribute/reporting-guide/#3-add-any-parameters","text":"Any keyword arg of log_parameters() is submitted as a parameter in mlflow. metrics_logger . log_parameters ( ** lgbm_params )","title":"3. Add any parameters"},{"location":"contribute/reporting-guide/#4-compute-wall-time-using-with-statement","text":"To compute wall time, the MetricsLogger class provide a helper method you can use within a with statement: with metrics_logger . log_time_block ( \"time_training\" ): # anything within this code block will count in wall time booster = lightgbm . train ( lgbm_params , train_data , valid_sets = val_data ) # anything outside of that will not count This will record a metric \"time_training\" measuring the time spent for the execution of this code block (only).","title":"4. Compute wall-time using with statement"},{"location":"references/common/aml/","text":"This script contains methods to handle connection to AzureML, such as registering Datasets or obtaining a Dataset handler from a given workspace. dataset_from_dstore_path ( workspace , datastore , datastore_path , validate = True ) Obtains a local reference for a given datastore and path Parameters: Name Type Description Default datastore str name of the AzureML datastore required datastore_path str path in datastore to register as Dataset required validate bool validate files exist or not True Returns: Type Description azureml.core.Dataset: registered Dataset object Source code in src/common/aml.py def dataset_from_dstore_path ( workspace , datastore , datastore_path , validate = True ): \"\"\" Obtains a local reference for a given datastore and path Args: datastore (str): name of the AzureML datastore datastore_path (str): path in datastore to register as Dataset validate (bool): validate files exist or not Returns: azureml.core.Dataset: registered Dataset object \"\"\" logger = logging . getLogger ( __name__ ) logger . info ( f \"Connecting to Datastore { datastore } ...\" ) datastore = Datastore . get ( workspace , datastore ) logger . info ( f \"Reading path { datastore_path } ...\" ) remote_ds_path = [( datastore , datastore_path )] logger . info ( f \"Registering as dataset...\" ) remote_dataset = Dataset . File . from_files ( path = remote_ds_path , validate = validate ) return remote_dataset load_dataset_from_data_input_spec ( workspace , data_input_spec ) Loads a dataset based on config object data_input_spec (see tasks.py data_input_spec) Parameters: Name Type Description Default workspace azureml . core . Workspace connector to an AzureML workspace required data_input_spec OmegaConf . DictConfig config Hydra dataclass data_input_spec (see tasks.py) required Returns: Type Description azureml.core.Dataset: registered Dataset object Source code in src/common/aml.py def load_dataset_from_data_input_spec ( workspace , data_input_spec ): \"\"\" Loads a dataset based on config object data_input_spec (see tasks.py data_input_spec) Args: workspace (azureml.core.Workspace): connector to an AzureML workspace data_input_spec (OmegaConf.DictConfig): config Hydra dataclass data_input_spec (see tasks.py) Returns: azureml.core.Dataset: registered Dataset object \"\"\" logger = logging . getLogger ( __name__ ) if data_input_spec . name : logger . info ( f \"Reading dataset from name= { data_input_spec . name } version= { data_input_spec . version } \" ) loaded_dataset = Dataset . get_by_name ( workspace , name = data_input_spec . name , version = data_input_spec . version ) elif data_input_spec . uuid : logger . info ( f \"Reading dataset from uuid\" ) loaded_dataset = Dataset . get_by_id ( workspace , id = data_input_spec . uuid ) elif data_input_spec . datastore and data_input_spec . path : logger . info ( f \"Connecting to Datastore { data_input_spec . datastore } ...\" ) datastore = Datastore . get ( workspace , data_input_spec . datastore ) logger . info ( f \"Reading path { data_input_spec . path } ...\" ) remote_ds_path = [( datastore , data_input_spec . path )] logger . info ( f \"Registering as dataset...\" ) loaded_dataset = Dataset . File . from_files ( path = remote_ds_path , validate = data_input_spec . validate ) else : raise ValueError ( \"To load a dataset using data_input_spec, you need to provide either a name, a uuid or a datastore+path (provided config = {data_input_spec} )\" ) return loaded_dataset apply_sweep_settings ( step , sweep_settings_config ) Applies the settings to a sweep step based on a config dataclass. Parameters: Name Type Description Default step PipelineStep the instance of the step required sweep_settings_config OmegaConf . DictConfig schema specified in src.common.tasks.sweep_runsettings required Source code in src/common/aml.py def apply_sweep_settings ( step , sweep_settings_config ): \"\"\"Applies the settings to a sweep step based on a config dataclass. Args: step (PipelineStep): the instance of the step sweep_settings_config (OmegaConf.DictConfig): schema specified in src.common.tasks.sweep_runsettings \"\"\" if ( not sweep_settings_config . primary_metric ) or ( not sweep_settings_config . goal ): raise ValueError ( \"in sweep settings, you need to provide a primary_metric and a goal settings.\" ) else : step . runsettings . sweep . objective . configure ( primary_metric = sweep_settings_config . primary_metric , goal = sweep_settings_config . goal , ) if not sweep_settings_config . algorithm : raise ValueError ( \"in sweep settings, you need to provide an algorithm setting.\" ) else : step . runsettings . sweep . algorithm = sweep_settings_config . algorithm if sweep_settings_config . limits : step . runsettings . sweep . limits . configure ( max_total_trials = sweep_settings_config . limits . max_total_trials , max_concurrent_trials = sweep_settings_config . limits . max_concurrent_trials , timeout_minutes = sweep_settings_config . limits . timeout_minutes , ) if sweep_settings_config . early_termination : if sweep_settings_config . early_termination . policy_type == \"median_stopping\" : step . runsettings . sweep . early_termination . configure ( policy_type = \"median_stopping\" , evaluation_interval = sweep_settings_config . early_termination . evaluation_interval , delay_evaluation = sweep_settings_config . early_termination . delay_evaluation ) elif sweep_settings_config . early_termination . policy_type == \"bandit\" : step . runsettings . sweep . early_termination . configure ( policy_type = \"bandit\" , slack_factor = sweep_settings_config . early_termination . slack_factor , evaluation_interval = sweep_settings_config . early_termination . evaluation_interval , delay_evaluation = sweep_settings_config . early_termination . delay_evaluation ) elif sweep_settings_config . early_termination . policy_type == \"truncation_selection\" : step . runsettings . sweep . early_termination . configure ( policy_type = \"truncation_selection\" , truncation_percentage = sweep_settings_config . early_termination . truncation_percentage , evaluation_interval = sweep_settings_config . early_termination . evaluation_interval , delay_evaluation = sweep_settings_config . early_termination . delay_evaluation ) elif sweep_settings_config . early_termination . policy_type == \"default\" : pass elif sweep_settings_config . early_termination . policy_type == None : pass else : raise NotImplementedError ( f \"sweep settings early_termination policy_type= { sweep_settings_config . early_termination . policy_type } is not implemented.\" ) format_run_name ( run_name ) Formats a run name to fit with AzureML constraints. Parameters: Name Type Description Default run_name str string to be formatted. required Returns: Type Description formatted_run_name (str) Notes Node name must start with a letter, and can only contain letters, numbers, underscores, within 1-255 characters. Source code in src/common/aml.py def format_run_name ( run_name : str ): \"\"\"Formats a run name to fit with AzureML constraints. Args: run_name (str): string to be formatted. Returns: formatted_run_name (str) Notes: Node name must start with a letter, and can only contain letters, numbers, underscores, within 1-255 characters. \"\"\" # removing all chars not allowed, use underscore instead formatted_run_name = re . sub ( r '[^a-zA-Z0-9_]' , '_' , run_name ) # cutting to first 255 chars if len ( formatted_run_name ) > 255 : formatted_run_name = formatted_run_name [ 0 : 255 ] return formatted_run_name","title":"aml.py"},{"location":"references/common/aml/#src.common.aml.dataset_from_dstore_path","text":"Obtains a local reference for a given datastore and path Parameters: Name Type Description Default datastore str name of the AzureML datastore required datastore_path str path in datastore to register as Dataset required validate bool validate files exist or not True Returns: Type Description azureml.core.Dataset: registered Dataset object Source code in src/common/aml.py def dataset_from_dstore_path ( workspace , datastore , datastore_path , validate = True ): \"\"\" Obtains a local reference for a given datastore and path Args: datastore (str): name of the AzureML datastore datastore_path (str): path in datastore to register as Dataset validate (bool): validate files exist or not Returns: azureml.core.Dataset: registered Dataset object \"\"\" logger = logging . getLogger ( __name__ ) logger . info ( f \"Connecting to Datastore { datastore } ...\" ) datastore = Datastore . get ( workspace , datastore ) logger . info ( f \"Reading path { datastore_path } ...\" ) remote_ds_path = [( datastore , datastore_path )] logger . info ( f \"Registering as dataset...\" ) remote_dataset = Dataset . File . from_files ( path = remote_ds_path , validate = validate ) return remote_dataset","title":"dataset_from_dstore_path()"},{"location":"references/common/aml/#src.common.aml.load_dataset_from_data_input_spec","text":"Loads a dataset based on config object data_input_spec (see tasks.py data_input_spec) Parameters: Name Type Description Default workspace azureml . core . Workspace connector to an AzureML workspace required data_input_spec OmegaConf . DictConfig config Hydra dataclass data_input_spec (see tasks.py) required Returns: Type Description azureml.core.Dataset: registered Dataset object Source code in src/common/aml.py def load_dataset_from_data_input_spec ( workspace , data_input_spec ): \"\"\" Loads a dataset based on config object data_input_spec (see tasks.py data_input_spec) Args: workspace (azureml.core.Workspace): connector to an AzureML workspace data_input_spec (OmegaConf.DictConfig): config Hydra dataclass data_input_spec (see tasks.py) Returns: azureml.core.Dataset: registered Dataset object \"\"\" logger = logging . getLogger ( __name__ ) if data_input_spec . name : logger . info ( f \"Reading dataset from name= { data_input_spec . name } version= { data_input_spec . version } \" ) loaded_dataset = Dataset . get_by_name ( workspace , name = data_input_spec . name , version = data_input_spec . version ) elif data_input_spec . uuid : logger . info ( f \"Reading dataset from uuid\" ) loaded_dataset = Dataset . get_by_id ( workspace , id = data_input_spec . uuid ) elif data_input_spec . datastore and data_input_spec . path : logger . info ( f \"Connecting to Datastore { data_input_spec . datastore } ...\" ) datastore = Datastore . get ( workspace , data_input_spec . datastore ) logger . info ( f \"Reading path { data_input_spec . path } ...\" ) remote_ds_path = [( datastore , data_input_spec . path )] logger . info ( f \"Registering as dataset...\" ) loaded_dataset = Dataset . File . from_files ( path = remote_ds_path , validate = data_input_spec . validate ) else : raise ValueError ( \"To load a dataset using data_input_spec, you need to provide either a name, a uuid or a datastore+path (provided config = {data_input_spec} )\" ) return loaded_dataset","title":"load_dataset_from_data_input_spec()"},{"location":"references/common/aml/#src.common.aml.apply_sweep_settings","text":"Applies the settings to a sweep step based on a config dataclass. Parameters: Name Type Description Default step PipelineStep the instance of the step required sweep_settings_config OmegaConf . DictConfig schema specified in src.common.tasks.sweep_runsettings required Source code in src/common/aml.py def apply_sweep_settings ( step , sweep_settings_config ): \"\"\"Applies the settings to a sweep step based on a config dataclass. Args: step (PipelineStep): the instance of the step sweep_settings_config (OmegaConf.DictConfig): schema specified in src.common.tasks.sweep_runsettings \"\"\" if ( not sweep_settings_config . primary_metric ) or ( not sweep_settings_config . goal ): raise ValueError ( \"in sweep settings, you need to provide a primary_metric and a goal settings.\" ) else : step . runsettings . sweep . objective . configure ( primary_metric = sweep_settings_config . primary_metric , goal = sweep_settings_config . goal , ) if not sweep_settings_config . algorithm : raise ValueError ( \"in sweep settings, you need to provide an algorithm setting.\" ) else : step . runsettings . sweep . algorithm = sweep_settings_config . algorithm if sweep_settings_config . limits : step . runsettings . sweep . limits . configure ( max_total_trials = sweep_settings_config . limits . max_total_trials , max_concurrent_trials = sweep_settings_config . limits . max_concurrent_trials , timeout_minutes = sweep_settings_config . limits . timeout_minutes , ) if sweep_settings_config . early_termination : if sweep_settings_config . early_termination . policy_type == \"median_stopping\" : step . runsettings . sweep . early_termination . configure ( policy_type = \"median_stopping\" , evaluation_interval = sweep_settings_config . early_termination . evaluation_interval , delay_evaluation = sweep_settings_config . early_termination . delay_evaluation ) elif sweep_settings_config . early_termination . policy_type == \"bandit\" : step . runsettings . sweep . early_termination . configure ( policy_type = \"bandit\" , slack_factor = sweep_settings_config . early_termination . slack_factor , evaluation_interval = sweep_settings_config . early_termination . evaluation_interval , delay_evaluation = sweep_settings_config . early_termination . delay_evaluation ) elif sweep_settings_config . early_termination . policy_type == \"truncation_selection\" : step . runsettings . sweep . early_termination . configure ( policy_type = \"truncation_selection\" , truncation_percentage = sweep_settings_config . early_termination . truncation_percentage , evaluation_interval = sweep_settings_config . early_termination . evaluation_interval , delay_evaluation = sweep_settings_config . early_termination . delay_evaluation ) elif sweep_settings_config . early_termination . policy_type == \"default\" : pass elif sweep_settings_config . early_termination . policy_type == None : pass else : raise NotImplementedError ( f \"sweep settings early_termination policy_type= { sweep_settings_config . early_termination . policy_type } is not implemented.\" )","title":"apply_sweep_settings()"},{"location":"references/common/aml/#src.common.aml.format_run_name","text":"Formats a run name to fit with AzureML constraints. Parameters: Name Type Description Default run_name str string to be formatted. required Returns: Type Description formatted_run_name (str) Notes Node name must start with a letter, and can only contain letters, numbers, underscores, within 1-255 characters. Source code in src/common/aml.py def format_run_name ( run_name : str ): \"\"\"Formats a run name to fit with AzureML constraints. Args: run_name (str): string to be formatted. Returns: formatted_run_name (str) Notes: Node name must start with a letter, and can only contain letters, numbers, underscores, within 1-255 characters. \"\"\" # removing all chars not allowed, use underscore instead formatted_run_name = re . sub ( r '[^a-zA-Z0-9_]' , '_' , run_name ) # cutting to first 255 chars if len ( formatted_run_name ) > 255 : formatted_run_name = formatted_run_name [ 0 : 255 ] return formatted_run_name","title":"format_run_name()"},{"location":"references/common/components/","text":"This script contains a class to structure and standardize all scripts in the lightgbm-benchmark repository. This class factors duplicate code to achieve usual routines of every script: logging init, MLFlow init, system properties logging, etc. RunnableScript This class factors duplicate code to achieve usual routines of every script in the lightgbm-benchmark repo: logging init, MLFlow init, system properties logging, etc. Source code in src/common/components.py class RunnableScript (): \"\"\" This class factors duplicate code to achieve usual routines of every script in the lightgbm-benchmark repo: logging init, MLFlow init, system properties logging, etc. \"\"\" def __init__ ( self , task , framework , framework_version , metrics_prefix = None ): \"\"\" Generic initialization for all script classes. Args: task (str): name of task in the pipeline/benchmark (ex: train, score) framework (str): name of ML framework framework_version (str): a version of this framework metrics_prefix (str): any prefix to add to this scripts metrics \"\"\" self . task = task self . framework = framework self . framework_version = framework_version self . metrics_prefix = metrics_prefix self . logger = logging . getLogger ( f \" { framework } . { task } \" ) # default metrics logger is just stdout print self . metrics_logger = MetricsLogger ( f \" { framework } . { task } \" , metrics_prefix = self . metrics_prefix ) self . perf_report_collector = None @classmethod def get_arg_parser ( cls , parser = None ): \"\"\"Adds component/module arguments to a given argument parser. Args: parser (argparse.ArgumentParser): an argument parser instance Returns: ArgumentParser: the argument parser instance Notes: if parser is None, creates a new parser instance \"\"\" # add arguments that are specific to the module if parser is None : parser = argparse . ArgumentParser () # add generic arguments here group_general = parser . add_argument_group ( f \"General parameters [ { __name__ } : { cls . __name__ } ]\" ) group_general . add_argument ( \"--verbose\" , required = False , default = False , type = strtobool , # use this for bool args, do not use action_store=True help = \"set True to show DEBUG logs\" , ) group_general . add_argument ( \"--custom_properties\" , required = False , default = None , type = str , help = \"provide custom properties as json dict\" , ) group_general . add_argument ( \"--disable_perf_metrics\" , required = False , default = False , type = strtobool , help = \"disable performance metrics (default: False)\" , ) group_general . add_argument ( \"--metrics_driver\" , required = False , default = \"mlflow\" , choices = [ 'mlflow' , 'azureml' ], type = str , help = \"which class to use to report metrics mlflow or azureml\" , ) return parser def initialize_run ( self , args ): \"\"\"Initialize the component run, opens/setups what needs to be\"\"\" self . logger . info ( \"Initializing script run...\" ) # initializes reporting of metrics if args . metrics_driver == 'mlflow' : self . metrics_logger = MLFlowMetricsLogger ( f \" { self . framework } . { self . task } \" , metrics_prefix = self . metrics_prefix ) elif args . metrics_driver == 'azureml' : self . metrics_logger = AzureMLRunMetricsLogger ( f \" { self . framework } . { self . task } \" , metrics_prefix = self . metrics_prefix ) else : # use default metrics_logger (stdout print) pass # open mlflow self . metrics_logger . open () # record properties of the run self . metrics_logger . set_properties ( task = self . task , framework = self . framework , framework_version = self . framework_version ) # if provided some custom_properties by the outside orchestrator if args . custom_properties : self . metrics_logger . set_properties_from_json ( args . custom_properties ) # add properties about environment of this script self . metrics_logger . set_platform_properties () # enable perf reporting if not args . disable_perf_metrics : self . perf_report_collector = PerformanceMetricsCollector () self . perf_report_collector . start () def run ( self , args , logger , metrics_logger , unknown_args ): \"\"\"The run function of your script. You are required to override this method with your own implementation. Args: args (argparse.namespace): command line arguments provided to script logger (logging.logger): a logger initialized for this script metrics_logger (common.metrics.MetricLogger): to report metrics for this script, already initialized for MLFlow unknown_args (list[str]): list of arguments not recognized during argparse \"\"\" raise NotImplementedError ( f \"run() method from class { self . __class__ . __name__ } hasn't actually been implemented.\" ) def finalize_run ( self , args ): \"\"\"Finalize the run, close what needs to be\"\"\" self . logger . info ( \"Finalizing script run...\" ) if self . perf_report_collector : self . perf_report_collector . finalize () plotter = PerfReportPlotter ( self . metrics_logger ) plotter . add_perf_reports ( self . perf_report_collector . perf_reports , node = 0 ) plotter . report_nodes_perf () # write perf record as artifact self . metrics_logger . log_artifact ( plotter . save_to ()) # close mlflow self . metrics_logger . close () #################### ### MAIN METHODS ### #################### @classmethod def initialize_root_logger ( cls ): # initialize root logger logger = logging . getLogger () logger . setLevel ( logging . INFO ) console_handler = logging . StreamHandler () formatter = logging . Formatter ( ' %(asctime)s : %(levelname)s : %(name)s : %(message)s ' ) console_handler . setFormatter ( formatter ) logger . addHandler ( console_handler ) return logger @classmethod def parse_class_arguments ( cls , cli_args = None ): logger = logging . getLogger () # show the command used to run if cli_args : logger . info ( f \"Running main() with specific cli args: { cli_args } \" ) else : logger . info ( f \"Running main() with sys.argv= { sys . argv } \" ) # construct arg parser parser = cls . get_arg_parser () # if argument parsing fails, or if unknown arguments, will except args , unknown_args = parser . parse_known_args ( cli_args ) logger . setLevel ( logging . DEBUG if args . verbose else logging . INFO ) return args , unknown_args def _main_run_hook ( self , args , unknown_args ): \"\"\"Run function called from main()\"\"\" self . run ( args , self . logger , self . metrics_logger , unknown_args = unknown_args ) @classmethod def main ( cls , cli_args = None ): \"\"\" Component main function, it is not recommended to override this method. It parses arguments and executes run() with the right arguments. Args: cli_args (List[str], optional): list of args to feed script, useful for debugging. Defaults to None. \"\"\" cls . initialize_root_logger () args , unknown_args = cls . parse_class_arguments ( cli_args ) # create script instance, initialize mlflow script_instance = cls () script_instance . initialize_run ( args ) # catch run function exceptions to properly finalize run (kill/join threads) try : # run the class run method (passthrough) script_instance . _main_run_hook ( args , unknown_args ) except BaseException as e : logging . critical ( f \"Exception occured during run(): \\n { traceback . format_exc () } \" ) script_instance . finalize_run ( args ) raise e # close mlflow script_instance . finalize_run ( args ) # return for unit tests return script_instance __init__ ( task , framework , framework_version , metrics_prefix = None ) Generic initialization for all script classes. Parameters: Name Type Description Default task str name of task in the pipeline/benchmark (ex: train, score) required framework str name of ML framework required framework_version str a version of this framework required metrics_prefix str any prefix to add to this scripts metrics None Source code in src/common/components.py def __init__ ( self , task , framework , framework_version , metrics_prefix = None ): \"\"\" Generic initialization for all script classes. Args: task (str): name of task in the pipeline/benchmark (ex: train, score) framework (str): name of ML framework framework_version (str): a version of this framework metrics_prefix (str): any prefix to add to this scripts metrics \"\"\" self . task = task self . framework = framework self . framework_version = framework_version self . metrics_prefix = metrics_prefix self . logger = logging . getLogger ( f \" { framework } . { task } \" ) # default metrics logger is just stdout print self . metrics_logger = MetricsLogger ( f \" { framework } . { task } \" , metrics_prefix = self . metrics_prefix ) self . perf_report_collector = None get_arg_parser ( parser = None ) classmethod Adds component/module arguments to a given argument parser. Parameters: Name Type Description Default parser argparse . ArgumentParser an argument parser instance None Returns: Name Type Description ArgumentParser the argument parser instance Notes if parser is None, creates a new parser instance Source code in src/common/components.py @classmethod def get_arg_parser ( cls , parser = None ): \"\"\"Adds component/module arguments to a given argument parser. Args: parser (argparse.ArgumentParser): an argument parser instance Returns: ArgumentParser: the argument parser instance Notes: if parser is None, creates a new parser instance \"\"\" # add arguments that are specific to the module if parser is None : parser = argparse . ArgumentParser () # add generic arguments here group_general = parser . add_argument_group ( f \"General parameters [ { __name__ } : { cls . __name__ } ]\" ) group_general . add_argument ( \"--verbose\" , required = False , default = False , type = strtobool , # use this for bool args, do not use action_store=True help = \"set True to show DEBUG logs\" , ) group_general . add_argument ( \"--custom_properties\" , required = False , default = None , type = str , help = \"provide custom properties as json dict\" , ) group_general . add_argument ( \"--disable_perf_metrics\" , required = False , default = False , type = strtobool , help = \"disable performance metrics (default: False)\" , ) group_general . add_argument ( \"--metrics_driver\" , required = False , default = \"mlflow\" , choices = [ 'mlflow' , 'azureml' ], type = str , help = \"which class to use to report metrics mlflow or azureml\" , ) return parser initialize_run ( args ) Initialize the component run, opens/setups what needs to be Source code in src/common/components.py def initialize_run ( self , args ): \"\"\"Initialize the component run, opens/setups what needs to be\"\"\" self . logger . info ( \"Initializing script run...\" ) # initializes reporting of metrics if args . metrics_driver == 'mlflow' : self . metrics_logger = MLFlowMetricsLogger ( f \" { self . framework } . { self . task } \" , metrics_prefix = self . metrics_prefix ) elif args . metrics_driver == 'azureml' : self . metrics_logger = AzureMLRunMetricsLogger ( f \" { self . framework } . { self . task } \" , metrics_prefix = self . metrics_prefix ) else : # use default metrics_logger (stdout print) pass # open mlflow self . metrics_logger . open () # record properties of the run self . metrics_logger . set_properties ( task = self . task , framework = self . framework , framework_version = self . framework_version ) # if provided some custom_properties by the outside orchestrator if args . custom_properties : self . metrics_logger . set_properties_from_json ( args . custom_properties ) # add properties about environment of this script self . metrics_logger . set_platform_properties () # enable perf reporting if not args . disable_perf_metrics : self . perf_report_collector = PerformanceMetricsCollector () self . perf_report_collector . start () run ( args , logger , metrics_logger , unknown_args ) The run function of your script. You are required to override this method with your own implementation. Parameters: Name Type Description Default args argparse . namespace command line arguments provided to script required logger logging . logger a logger initialized for this script required metrics_logger common . metrics . MetricLogger to report metrics for this script, already initialized for MLFlow required unknown_args list [ str ] list of arguments not recognized during argparse required Source code in src/common/components.py def run ( self , args , logger , metrics_logger , unknown_args ): \"\"\"The run function of your script. You are required to override this method with your own implementation. Args: args (argparse.namespace): command line arguments provided to script logger (logging.logger): a logger initialized for this script metrics_logger (common.metrics.MetricLogger): to report metrics for this script, already initialized for MLFlow unknown_args (list[str]): list of arguments not recognized during argparse \"\"\" raise NotImplementedError ( f \"run() method from class { self . __class__ . __name__ } hasn't actually been implemented.\" ) finalize_run ( args ) Finalize the run, close what needs to be Source code in src/common/components.py def finalize_run ( self , args ): \"\"\"Finalize the run, close what needs to be\"\"\" self . logger . info ( \"Finalizing script run...\" ) if self . perf_report_collector : self . perf_report_collector . finalize () plotter = PerfReportPlotter ( self . metrics_logger ) plotter . add_perf_reports ( self . perf_report_collector . perf_reports , node = 0 ) plotter . report_nodes_perf () # write perf record as artifact self . metrics_logger . log_artifact ( plotter . save_to ()) # close mlflow self . metrics_logger . close () main ( cli_args = None ) classmethod Component main function, it is not recommended to override this method. It parses arguments and executes run() with the right arguments. Parameters: Name Type Description Default cli_args List [ str ] list of args to feed script, useful for debugging. Defaults to None. None Source code in src/common/components.py @classmethod def main ( cls , cli_args = None ): \"\"\" Component main function, it is not recommended to override this method. It parses arguments and executes run() with the right arguments. Args: cli_args (List[str], optional): list of args to feed script, useful for debugging. Defaults to None. \"\"\" cls . initialize_root_logger () args , unknown_args = cls . parse_class_arguments ( cli_args ) # create script instance, initialize mlflow script_instance = cls () script_instance . initialize_run ( args ) # catch run function exceptions to properly finalize run (kill/join threads) try : # run the class run method (passthrough) script_instance . _main_run_hook ( args , unknown_args ) except BaseException as e : logging . critical ( f \"Exception occured during run(): \\n { traceback . format_exc () } \" ) script_instance . finalize_run ( args ) raise e # close mlflow script_instance . finalize_run ( args ) # return for unit tests return script_instance","title":"components.py"},{"location":"references/common/components/#src.common.components.RunnableScript","text":"This class factors duplicate code to achieve usual routines of every script in the lightgbm-benchmark repo: logging init, MLFlow init, system properties logging, etc. Source code in src/common/components.py class RunnableScript (): \"\"\" This class factors duplicate code to achieve usual routines of every script in the lightgbm-benchmark repo: logging init, MLFlow init, system properties logging, etc. \"\"\" def __init__ ( self , task , framework , framework_version , metrics_prefix = None ): \"\"\" Generic initialization for all script classes. Args: task (str): name of task in the pipeline/benchmark (ex: train, score) framework (str): name of ML framework framework_version (str): a version of this framework metrics_prefix (str): any prefix to add to this scripts metrics \"\"\" self . task = task self . framework = framework self . framework_version = framework_version self . metrics_prefix = metrics_prefix self . logger = logging . getLogger ( f \" { framework } . { task } \" ) # default metrics logger is just stdout print self . metrics_logger = MetricsLogger ( f \" { framework } . { task } \" , metrics_prefix = self . metrics_prefix ) self . perf_report_collector = None @classmethod def get_arg_parser ( cls , parser = None ): \"\"\"Adds component/module arguments to a given argument parser. Args: parser (argparse.ArgumentParser): an argument parser instance Returns: ArgumentParser: the argument parser instance Notes: if parser is None, creates a new parser instance \"\"\" # add arguments that are specific to the module if parser is None : parser = argparse . ArgumentParser () # add generic arguments here group_general = parser . add_argument_group ( f \"General parameters [ { __name__ } : { cls . __name__ } ]\" ) group_general . add_argument ( \"--verbose\" , required = False , default = False , type = strtobool , # use this for bool args, do not use action_store=True help = \"set True to show DEBUG logs\" , ) group_general . add_argument ( \"--custom_properties\" , required = False , default = None , type = str , help = \"provide custom properties as json dict\" , ) group_general . add_argument ( \"--disable_perf_metrics\" , required = False , default = False , type = strtobool , help = \"disable performance metrics (default: False)\" , ) group_general . add_argument ( \"--metrics_driver\" , required = False , default = \"mlflow\" , choices = [ 'mlflow' , 'azureml' ], type = str , help = \"which class to use to report metrics mlflow or azureml\" , ) return parser def initialize_run ( self , args ): \"\"\"Initialize the component run, opens/setups what needs to be\"\"\" self . logger . info ( \"Initializing script run...\" ) # initializes reporting of metrics if args . metrics_driver == 'mlflow' : self . metrics_logger = MLFlowMetricsLogger ( f \" { self . framework } . { self . task } \" , metrics_prefix = self . metrics_prefix ) elif args . metrics_driver == 'azureml' : self . metrics_logger = AzureMLRunMetricsLogger ( f \" { self . framework } . { self . task } \" , metrics_prefix = self . metrics_prefix ) else : # use default metrics_logger (stdout print) pass # open mlflow self . metrics_logger . open () # record properties of the run self . metrics_logger . set_properties ( task = self . task , framework = self . framework , framework_version = self . framework_version ) # if provided some custom_properties by the outside orchestrator if args . custom_properties : self . metrics_logger . set_properties_from_json ( args . custom_properties ) # add properties about environment of this script self . metrics_logger . set_platform_properties () # enable perf reporting if not args . disable_perf_metrics : self . perf_report_collector = PerformanceMetricsCollector () self . perf_report_collector . start () def run ( self , args , logger , metrics_logger , unknown_args ): \"\"\"The run function of your script. You are required to override this method with your own implementation. Args: args (argparse.namespace): command line arguments provided to script logger (logging.logger): a logger initialized for this script metrics_logger (common.metrics.MetricLogger): to report metrics for this script, already initialized for MLFlow unknown_args (list[str]): list of arguments not recognized during argparse \"\"\" raise NotImplementedError ( f \"run() method from class { self . __class__ . __name__ } hasn't actually been implemented.\" ) def finalize_run ( self , args ): \"\"\"Finalize the run, close what needs to be\"\"\" self . logger . info ( \"Finalizing script run...\" ) if self . perf_report_collector : self . perf_report_collector . finalize () plotter = PerfReportPlotter ( self . metrics_logger ) plotter . add_perf_reports ( self . perf_report_collector . perf_reports , node = 0 ) plotter . report_nodes_perf () # write perf record as artifact self . metrics_logger . log_artifact ( plotter . save_to ()) # close mlflow self . metrics_logger . close () #################### ### MAIN METHODS ### #################### @classmethod def initialize_root_logger ( cls ): # initialize root logger logger = logging . getLogger () logger . setLevel ( logging . INFO ) console_handler = logging . StreamHandler () formatter = logging . Formatter ( ' %(asctime)s : %(levelname)s : %(name)s : %(message)s ' ) console_handler . setFormatter ( formatter ) logger . addHandler ( console_handler ) return logger @classmethod def parse_class_arguments ( cls , cli_args = None ): logger = logging . getLogger () # show the command used to run if cli_args : logger . info ( f \"Running main() with specific cli args: { cli_args } \" ) else : logger . info ( f \"Running main() with sys.argv= { sys . argv } \" ) # construct arg parser parser = cls . get_arg_parser () # if argument parsing fails, or if unknown arguments, will except args , unknown_args = parser . parse_known_args ( cli_args ) logger . setLevel ( logging . DEBUG if args . verbose else logging . INFO ) return args , unknown_args def _main_run_hook ( self , args , unknown_args ): \"\"\"Run function called from main()\"\"\" self . run ( args , self . logger , self . metrics_logger , unknown_args = unknown_args ) @classmethod def main ( cls , cli_args = None ): \"\"\" Component main function, it is not recommended to override this method. It parses arguments and executes run() with the right arguments. Args: cli_args (List[str], optional): list of args to feed script, useful for debugging. Defaults to None. \"\"\" cls . initialize_root_logger () args , unknown_args = cls . parse_class_arguments ( cli_args ) # create script instance, initialize mlflow script_instance = cls () script_instance . initialize_run ( args ) # catch run function exceptions to properly finalize run (kill/join threads) try : # run the class run method (passthrough) script_instance . _main_run_hook ( args , unknown_args ) except BaseException as e : logging . critical ( f \"Exception occured during run(): \\n { traceback . format_exc () } \" ) script_instance . finalize_run ( args ) raise e # close mlflow script_instance . finalize_run ( args ) # return for unit tests return script_instance","title":"RunnableScript"},{"location":"references/common/components/#src.common.components.RunnableScript.__init__","text":"Generic initialization for all script classes. Parameters: Name Type Description Default task str name of task in the pipeline/benchmark (ex: train, score) required framework str name of ML framework required framework_version str a version of this framework required metrics_prefix str any prefix to add to this scripts metrics None Source code in src/common/components.py def __init__ ( self , task , framework , framework_version , metrics_prefix = None ): \"\"\" Generic initialization for all script classes. Args: task (str): name of task in the pipeline/benchmark (ex: train, score) framework (str): name of ML framework framework_version (str): a version of this framework metrics_prefix (str): any prefix to add to this scripts metrics \"\"\" self . task = task self . framework = framework self . framework_version = framework_version self . metrics_prefix = metrics_prefix self . logger = logging . getLogger ( f \" { framework } . { task } \" ) # default metrics logger is just stdout print self . metrics_logger = MetricsLogger ( f \" { framework } . { task } \" , metrics_prefix = self . metrics_prefix ) self . perf_report_collector = None","title":"__init__()"},{"location":"references/common/components/#src.common.components.RunnableScript.get_arg_parser","text":"Adds component/module arguments to a given argument parser. Parameters: Name Type Description Default parser argparse . ArgumentParser an argument parser instance None Returns: Name Type Description ArgumentParser the argument parser instance Notes if parser is None, creates a new parser instance Source code in src/common/components.py @classmethod def get_arg_parser ( cls , parser = None ): \"\"\"Adds component/module arguments to a given argument parser. Args: parser (argparse.ArgumentParser): an argument parser instance Returns: ArgumentParser: the argument parser instance Notes: if parser is None, creates a new parser instance \"\"\" # add arguments that are specific to the module if parser is None : parser = argparse . ArgumentParser () # add generic arguments here group_general = parser . add_argument_group ( f \"General parameters [ { __name__ } : { cls . __name__ } ]\" ) group_general . add_argument ( \"--verbose\" , required = False , default = False , type = strtobool , # use this for bool args, do not use action_store=True help = \"set True to show DEBUG logs\" , ) group_general . add_argument ( \"--custom_properties\" , required = False , default = None , type = str , help = \"provide custom properties as json dict\" , ) group_general . add_argument ( \"--disable_perf_metrics\" , required = False , default = False , type = strtobool , help = \"disable performance metrics (default: False)\" , ) group_general . add_argument ( \"--metrics_driver\" , required = False , default = \"mlflow\" , choices = [ 'mlflow' , 'azureml' ], type = str , help = \"which class to use to report metrics mlflow or azureml\" , ) return parser","title":"get_arg_parser()"},{"location":"references/common/components/#src.common.components.RunnableScript.initialize_run","text":"Initialize the component run, opens/setups what needs to be Source code in src/common/components.py def initialize_run ( self , args ): \"\"\"Initialize the component run, opens/setups what needs to be\"\"\" self . logger . info ( \"Initializing script run...\" ) # initializes reporting of metrics if args . metrics_driver == 'mlflow' : self . metrics_logger = MLFlowMetricsLogger ( f \" { self . framework } . { self . task } \" , metrics_prefix = self . metrics_prefix ) elif args . metrics_driver == 'azureml' : self . metrics_logger = AzureMLRunMetricsLogger ( f \" { self . framework } . { self . task } \" , metrics_prefix = self . metrics_prefix ) else : # use default metrics_logger (stdout print) pass # open mlflow self . metrics_logger . open () # record properties of the run self . metrics_logger . set_properties ( task = self . task , framework = self . framework , framework_version = self . framework_version ) # if provided some custom_properties by the outside orchestrator if args . custom_properties : self . metrics_logger . set_properties_from_json ( args . custom_properties ) # add properties about environment of this script self . metrics_logger . set_platform_properties () # enable perf reporting if not args . disable_perf_metrics : self . perf_report_collector = PerformanceMetricsCollector () self . perf_report_collector . start ()","title":"initialize_run()"},{"location":"references/common/components/#src.common.components.RunnableScript.run","text":"The run function of your script. You are required to override this method with your own implementation. Parameters: Name Type Description Default args argparse . namespace command line arguments provided to script required logger logging . logger a logger initialized for this script required metrics_logger common . metrics . MetricLogger to report metrics for this script, already initialized for MLFlow required unknown_args list [ str ] list of arguments not recognized during argparse required Source code in src/common/components.py def run ( self , args , logger , metrics_logger , unknown_args ): \"\"\"The run function of your script. You are required to override this method with your own implementation. Args: args (argparse.namespace): command line arguments provided to script logger (logging.logger): a logger initialized for this script metrics_logger (common.metrics.MetricLogger): to report metrics for this script, already initialized for MLFlow unknown_args (list[str]): list of arguments not recognized during argparse \"\"\" raise NotImplementedError ( f \"run() method from class { self . __class__ . __name__ } hasn't actually been implemented.\" )","title":"run()"},{"location":"references/common/components/#src.common.components.RunnableScript.finalize_run","text":"Finalize the run, close what needs to be Source code in src/common/components.py def finalize_run ( self , args ): \"\"\"Finalize the run, close what needs to be\"\"\" self . logger . info ( \"Finalizing script run...\" ) if self . perf_report_collector : self . perf_report_collector . finalize () plotter = PerfReportPlotter ( self . metrics_logger ) plotter . add_perf_reports ( self . perf_report_collector . perf_reports , node = 0 ) plotter . report_nodes_perf () # write perf record as artifact self . metrics_logger . log_artifact ( plotter . save_to ()) # close mlflow self . metrics_logger . close ()","title":"finalize_run()"},{"location":"references/common/components/#src.common.components.RunnableScript.main","text":"Component main function, it is not recommended to override this method. It parses arguments and executes run() with the right arguments. Parameters: Name Type Description Default cli_args List [ str ] list of args to feed script, useful for debugging. Defaults to None. None Source code in src/common/components.py @classmethod def main ( cls , cli_args = None ): \"\"\" Component main function, it is not recommended to override this method. It parses arguments and executes run() with the right arguments. Args: cli_args (List[str], optional): list of args to feed script, useful for debugging. Defaults to None. \"\"\" cls . initialize_root_logger () args , unknown_args = cls . parse_class_arguments ( cli_args ) # create script instance, initialize mlflow script_instance = cls () script_instance . initialize_run ( args ) # catch run function exceptions to properly finalize run (kill/join threads) try : # run the class run method (passthrough) script_instance . _main_run_hook ( args , unknown_args ) except BaseException as e : logging . critical ( f \"Exception occured during run(): \\n { traceback . format_exc () } \" ) script_instance . finalize_run ( args ) raise e # close mlflow script_instance . finalize_run ( args ) # return for unit tests return script_instance","title":"main()"},{"location":"references/common/io/","text":"This contains helper functions to handle inputs and outputs arguments in the benchmark scripts. It also provides some automation routine to handle data. PartitioningEngine This class handles partitioning data files into chunks with various strategies. Source code in src/common/io.py class PartitioningEngine (): \"\"\"This class handles partitioning data files into chunks with various strategies. \"\"\" PARTITION_MODES = [ 'chunk' , 'roundrobin' , 'append' ] def __init__ ( self , mode , number , header = False , logger = None ): \"\"\"Constructs and setup of the engine Args: mode (str): which partition mode (in PartitioningEngine.PARTITION_MODE list) number (int): parameter, behavior depends on mode header (bool): are there header in the input files? logger (logging.logger): a custom logger, if needed, for this engine to log \"\"\" self . mode = mode self . number = number self . header = header self . logger = logger or logging . getLogger ( __name__ ) def split_by_append ( self , input_files , output_path , file_count_target ): \"\"\"Just appends N++ files in N groups. Args: input_files (List[str]): list of file paths output_path (str): directory path, where to write the partitions file_count_target (int): how many partitions we want \"\"\" if len ( input_files ) < file_count_target : raise Exception ( f \"To use mode=append, the number of input files ( { len ( input_files ) } ) needs to be higher than requested number of output files ( { file_count_target } )\" ) # each partition starts as an empty list partitions = [ [] for i in range ( file_count_target ) ] # loop on all files, and put them in one partition for index , input_file in enumerate ( input_files ): partitions [ index % file_count_target ] . append ( input_file ) self . logger . info ( f \"Shuffled { len ( input_files ) } files into { file_count_target } partitions.\" ) # then write each partition by appending content for current_partition_index , partition in enumerate ( partitions ): self . logger . info ( f \"Writing partition { current_partition_index } ...\" ) with open ( os . path . join ( output_path , \"part_ {:06d} \" . format ( current_partition_index )), 'a' , encoding = \"utf-8\" ) as output_handler : for input_file in partition : self . logger . info ( f \"Reading input file { input_file } ...\" ) with open ( input_file , 'r' ) as input_handler : output_handler . write ( input_handler . read ()) self . logger . info ( f \"Created { current_partition_index + 1 } partitions\" ) def split_by_size ( self , input_files , output_path , partition_size ): \"\"\"Splits input files into a variable number of partitions by chunking a fixed number of lines from inputs into each output file. Args: input_files (List[str]): list of file paths output_path (str): directory path, where to write the partitions partition_size (int): how many lines per partition \"\"\" current_partition_size = 0 current_partition_index = 0 self . logger . info ( f \"Creating partition { current_partition_index } \" ) header_line = None # there can be only on header line for input_file in input_files : self . logger . info ( f \"Opening input file { input_file } \" ) with open ( input_file , \"r\" , encoding = \"utf-8\" ) as input_handler : for line in input_handler : if self . header and header_line is None : # if first line of first input file # write that line in every partition header_line = line if partition_size > 0 and current_partition_size >= partition_size : current_partition_index += 1 current_partition_size = 0 self . logger . info ( f \"Creating partition { current_partition_index } \" ) with open ( os . path . join ( output_path , \"part_ {:06d} \" . format ( current_partition_index )), 'a' , encoding = \"utf-8\" ) as output_handler : if self . header and current_partition_size == 0 : # put header before anything else output_handler . write ( header_line ) output_handler . write ( line ) current_partition_size += 1 self . logger . info ( f \"Created { current_partition_index + 1 } partitions\" ) def split_by_count ( self , input_files , output_path , partition_count ): \"\"\"Splits input files into a fixed number of partitions by round-robin shuffling of the lines of input files. Args: input_files (List[str]): list of file paths output_path (str): directory path, where to write the partitions partition_count (int): how many lines per partition \"\"\" self . logger . info ( f \"Creating { partition_count } partitions using round robin.\" ) partition_files = [ open ( os . path . join ( output_path , \"part_ {:06d} \" . format ( i )), \"w\" , encoding = \"utf-8\" ) for i in range ( partition_count )] current_index = 0 header_line = None # there can be only on header line for input_file in input_files : self . logger . info ( f \"Opening input file { input_file } \" ) with open ( input_file , \"r\" , encoding = \"utf-8\" ) as input_handler : for line_index , line in enumerate ( input_handler ): if self . header and header_line is None : # if first line of first input file # write that line in every partition header_line = line for partition_file in partition_files : partition_file . write ( header_line ) continue elif self . header and line_index == 0 : # if first line of 2nd... input file, just pass continue partition_files [ current_index % partition_count ] . write ( line ) current_index += 1 for handler in partition_files : handler . close () self . logger . info ( f \"Created { partition_count } partitions\" ) def run ( self , input_path , output_path ): \"\"\"Runs the partition based on provided arguments. Args: input_path (str): path to input file(s) output_path (str): path to store output partitions \"\"\" # Retrieve all input files if os . path . isfile ( input_path ): self . logger . info ( \"Input is one unique file\" ) file_names = [ os . path . basename ( input_path )] input_files = [ input_path ] else : self . logger . info ( \"Input is a directory, listing all of them for processing\" ) file_names = os . listdir ( input_path ) input_files = [ os . path . join ( input_path , file ) for file in file_names ] self . logger . info ( \"Found {} files in {} \" . format ( len ( input_files ), input_path )) if self . mode == \"chunk\" : self . split_by_size ( input_files , output_path , self . number ) elif self . mode == \"roundrobin\" : self . split_by_count ( input_files , output_path , self . number ) elif self . mode == \"append\" : self . split_by_append ( input_files , output_path , self . number ) else : raise NotImplementedError ( f \"Mode { self . mode } not implemented.\" ) __init__ ( mode , number , header = False , logger = None ) Constructs and setup of the engine Parameters: Name Type Description Default mode str which partition mode (in PartitioningEngine.PARTITION_MODE list) required number int parameter, behavior depends on mode required header bool are there header in the input files? False logger logging . logger a custom logger, if needed, for this engine to log None Source code in src/common/io.py def __init__ ( self , mode , number , header = False , logger = None ): \"\"\"Constructs and setup of the engine Args: mode (str): which partition mode (in PartitioningEngine.PARTITION_MODE list) number (int): parameter, behavior depends on mode header (bool): are there header in the input files? logger (logging.logger): a custom logger, if needed, for this engine to log \"\"\" self . mode = mode self . number = number self . header = header self . logger = logger or logging . getLogger ( __name__ ) split_by_append ( input_files , output_path , file_count_target ) Just appends N++ files in N groups. Parameters: Name Type Description Default input_files List [ str ] list of file paths required output_path str directory path, where to write the partitions required file_count_target int how many partitions we want required Source code in src/common/io.py def split_by_append ( self , input_files , output_path , file_count_target ): \"\"\"Just appends N++ files in N groups. Args: input_files (List[str]): list of file paths output_path (str): directory path, where to write the partitions file_count_target (int): how many partitions we want \"\"\" if len ( input_files ) < file_count_target : raise Exception ( f \"To use mode=append, the number of input files ( { len ( input_files ) } ) needs to be higher than requested number of output files ( { file_count_target } )\" ) # each partition starts as an empty list partitions = [ [] for i in range ( file_count_target ) ] # loop on all files, and put them in one partition for index , input_file in enumerate ( input_files ): partitions [ index % file_count_target ] . append ( input_file ) self . logger . info ( f \"Shuffled { len ( input_files ) } files into { file_count_target } partitions.\" ) # then write each partition by appending content for current_partition_index , partition in enumerate ( partitions ): self . logger . info ( f \"Writing partition { current_partition_index } ...\" ) with open ( os . path . join ( output_path , \"part_ {:06d} \" . format ( current_partition_index )), 'a' , encoding = \"utf-8\" ) as output_handler : for input_file in partition : self . logger . info ( f \"Reading input file { input_file } ...\" ) with open ( input_file , 'r' ) as input_handler : output_handler . write ( input_handler . read ()) self . logger . info ( f \"Created { current_partition_index + 1 } partitions\" ) split_by_size ( input_files , output_path , partition_size ) Splits input files into a variable number of partitions by chunking a fixed number of lines from inputs into each output file. Parameters: Name Type Description Default input_files List [ str ] list of file paths required output_path str directory path, where to write the partitions required partition_size int how many lines per partition required Source code in src/common/io.py def split_by_size ( self , input_files , output_path , partition_size ): \"\"\"Splits input files into a variable number of partitions by chunking a fixed number of lines from inputs into each output file. Args: input_files (List[str]): list of file paths output_path (str): directory path, where to write the partitions partition_size (int): how many lines per partition \"\"\" current_partition_size = 0 current_partition_index = 0 self . logger . info ( f \"Creating partition { current_partition_index } \" ) header_line = None # there can be only on header line for input_file in input_files : self . logger . info ( f \"Opening input file { input_file } \" ) with open ( input_file , \"r\" , encoding = \"utf-8\" ) as input_handler : for line in input_handler : if self . header and header_line is None : # if first line of first input file # write that line in every partition header_line = line if partition_size > 0 and current_partition_size >= partition_size : current_partition_index += 1 current_partition_size = 0 self . logger . info ( f \"Creating partition { current_partition_index } \" ) with open ( os . path . join ( output_path , \"part_ {:06d} \" . format ( current_partition_index )), 'a' , encoding = \"utf-8\" ) as output_handler : if self . header and current_partition_size == 0 : # put header before anything else output_handler . write ( header_line ) output_handler . write ( line ) current_partition_size += 1 self . logger . info ( f \"Created { current_partition_index + 1 } partitions\" ) split_by_count ( input_files , output_path , partition_count ) Splits input files into a fixed number of partitions by round-robin shuffling of the lines of input files. Parameters: Name Type Description Default input_files List [ str ] list of file paths required output_path str directory path, where to write the partitions required partition_count int how many lines per partition required Source code in src/common/io.py def split_by_count ( self , input_files , output_path , partition_count ): \"\"\"Splits input files into a fixed number of partitions by round-robin shuffling of the lines of input files. Args: input_files (List[str]): list of file paths output_path (str): directory path, where to write the partitions partition_count (int): how many lines per partition \"\"\" self . logger . info ( f \"Creating { partition_count } partitions using round robin.\" ) partition_files = [ open ( os . path . join ( output_path , \"part_ {:06d} \" . format ( i )), \"w\" , encoding = \"utf-8\" ) for i in range ( partition_count )] current_index = 0 header_line = None # there can be only on header line for input_file in input_files : self . logger . info ( f \"Opening input file { input_file } \" ) with open ( input_file , \"r\" , encoding = \"utf-8\" ) as input_handler : for line_index , line in enumerate ( input_handler ): if self . header and header_line is None : # if first line of first input file # write that line in every partition header_line = line for partition_file in partition_files : partition_file . write ( header_line ) continue elif self . header and line_index == 0 : # if first line of 2nd... input file, just pass continue partition_files [ current_index % partition_count ] . write ( line ) current_index += 1 for handler in partition_files : handler . close () self . logger . info ( f \"Created { partition_count } partitions\" ) run ( input_path , output_path ) Runs the partition based on provided arguments. Parameters: Name Type Description Default input_path str path to input file(s) required output_path str path to store output partitions required Source code in src/common/io.py def run ( self , input_path , output_path ): \"\"\"Runs the partition based on provided arguments. Args: input_path (str): path to input file(s) output_path (str): path to store output partitions \"\"\" # Retrieve all input files if os . path . isfile ( input_path ): self . logger . info ( \"Input is one unique file\" ) file_names = [ os . path . basename ( input_path )] input_files = [ input_path ] else : self . logger . info ( \"Input is a directory, listing all of them for processing\" ) file_names = os . listdir ( input_path ) input_files = [ os . path . join ( input_path , file ) for file in file_names ] self . logger . info ( \"Found {} files in {} \" . format ( len ( input_files ), input_path )) if self . mode == \"chunk\" : self . split_by_size ( input_files , output_path , self . number ) elif self . mode == \"roundrobin\" : self . split_by_count ( input_files , output_path , self . number ) elif self . mode == \"append\" : self . split_by_append ( input_files , output_path , self . number ) else : raise NotImplementedError ( f \"Mode { self . mode } not implemented.\" ) input_file_path ( path ) Argparse type to resolve input path as single file from directory. Given input path can be either a file, or a directory. If it's a directory, this returns the path to the unique file it contains. Parameters: Name Type Description Default path str either file or directory path required Returns: Name Type Description str path to file, or to unique file in directory Source code in src/common/io.py def input_file_path ( path ): \"\"\" Argparse type to resolve input path as single file from directory. Given input path can be either a file, or a directory. If it's a directory, this returns the path to the unique file it contains. Args: path (str): either file or directory path Returns: str: path to file, or to unique file in directory \"\"\" if os . path . isfile ( path ): logging . getLogger ( __name__ ) . info ( f \"Found INPUT file { path } \" ) return path if os . path . isdir ( path ): all_files = os . listdir ( path ) if not all_files : raise Exception ( f \"Could not find any file in specified input directory { path } \" ) if len ( all_files ) > 1 : raise Exception ( f \"Found multiple files in input file path { path } , use input_directory_path type instead.\" ) logging . getLogger ( __name__ ) . info ( f \"Found INPUT directory { path } , selecting unique file { all_files [ 0 ] } \" ) return os . path . join ( path , all_files [ 0 ]) logging . getLogger ( __name__ ) . critical ( f \"Provided INPUT path { path } is neither a directory or a file???\" ) return path get_all_files ( path , fail_on_unknown_type = False ) Scans some input path and returns a list of files. Parameters: Name Type Description Default path str either a file, or directory path required fail_on_unknown_type bool fails if path is neither a file or a dir? False Returns: Type Description List[str]: list of paths contained in path Source code in src/common/io.py def get_all_files ( path , fail_on_unknown_type = False ): \"\"\" Scans some input path and returns a list of files. Args: path (str): either a file, or directory path fail_on_unknown_type (bool): fails if path is neither a file or a dir? Returns: List[str]: list of paths contained in path \"\"\" # check the existence of the path if exists ( path ) == False : raise Exception ( f \"The specified path { path } does not exist.\" ) # if input path is already a file, return as list if os . path . isfile ( path ): logging . getLogger ( __name__ ) . info ( f \"Found INPUT file { path } \" ) return [ path ] # if input path is a directory, list all files and return if os . path . isdir ( path ): all_files = [ os . path . join ( path , entry ) for entry in os . listdir ( path ) ] if not all_files : raise Exception ( f \"Could not find any file in specified input directory { path } \" ) return all_files if fail_on_unknown_type : raise FileNotFoundError ( f \"Provided INPUT path { path } is neither a directory or a file???\" ) else : logging . getLogger ( __name__ ) . critical ( f \"Provided INPUT path { path } is neither a directory or a file???\" ) return path","title":"io.py"},{"location":"references/common/io/#src.common.io.PartitioningEngine","text":"This class handles partitioning data files into chunks with various strategies. Source code in src/common/io.py class PartitioningEngine (): \"\"\"This class handles partitioning data files into chunks with various strategies. \"\"\" PARTITION_MODES = [ 'chunk' , 'roundrobin' , 'append' ] def __init__ ( self , mode , number , header = False , logger = None ): \"\"\"Constructs and setup of the engine Args: mode (str): which partition mode (in PartitioningEngine.PARTITION_MODE list) number (int): parameter, behavior depends on mode header (bool): are there header in the input files? logger (logging.logger): a custom logger, if needed, for this engine to log \"\"\" self . mode = mode self . number = number self . header = header self . logger = logger or logging . getLogger ( __name__ ) def split_by_append ( self , input_files , output_path , file_count_target ): \"\"\"Just appends N++ files in N groups. Args: input_files (List[str]): list of file paths output_path (str): directory path, where to write the partitions file_count_target (int): how many partitions we want \"\"\" if len ( input_files ) < file_count_target : raise Exception ( f \"To use mode=append, the number of input files ( { len ( input_files ) } ) needs to be higher than requested number of output files ( { file_count_target } )\" ) # each partition starts as an empty list partitions = [ [] for i in range ( file_count_target ) ] # loop on all files, and put them in one partition for index , input_file in enumerate ( input_files ): partitions [ index % file_count_target ] . append ( input_file ) self . logger . info ( f \"Shuffled { len ( input_files ) } files into { file_count_target } partitions.\" ) # then write each partition by appending content for current_partition_index , partition in enumerate ( partitions ): self . logger . info ( f \"Writing partition { current_partition_index } ...\" ) with open ( os . path . join ( output_path , \"part_ {:06d} \" . format ( current_partition_index )), 'a' , encoding = \"utf-8\" ) as output_handler : for input_file in partition : self . logger . info ( f \"Reading input file { input_file } ...\" ) with open ( input_file , 'r' ) as input_handler : output_handler . write ( input_handler . read ()) self . logger . info ( f \"Created { current_partition_index + 1 } partitions\" ) def split_by_size ( self , input_files , output_path , partition_size ): \"\"\"Splits input files into a variable number of partitions by chunking a fixed number of lines from inputs into each output file. Args: input_files (List[str]): list of file paths output_path (str): directory path, where to write the partitions partition_size (int): how many lines per partition \"\"\" current_partition_size = 0 current_partition_index = 0 self . logger . info ( f \"Creating partition { current_partition_index } \" ) header_line = None # there can be only on header line for input_file in input_files : self . logger . info ( f \"Opening input file { input_file } \" ) with open ( input_file , \"r\" , encoding = \"utf-8\" ) as input_handler : for line in input_handler : if self . header and header_line is None : # if first line of first input file # write that line in every partition header_line = line if partition_size > 0 and current_partition_size >= partition_size : current_partition_index += 1 current_partition_size = 0 self . logger . info ( f \"Creating partition { current_partition_index } \" ) with open ( os . path . join ( output_path , \"part_ {:06d} \" . format ( current_partition_index )), 'a' , encoding = \"utf-8\" ) as output_handler : if self . header and current_partition_size == 0 : # put header before anything else output_handler . write ( header_line ) output_handler . write ( line ) current_partition_size += 1 self . logger . info ( f \"Created { current_partition_index + 1 } partitions\" ) def split_by_count ( self , input_files , output_path , partition_count ): \"\"\"Splits input files into a fixed number of partitions by round-robin shuffling of the lines of input files. Args: input_files (List[str]): list of file paths output_path (str): directory path, where to write the partitions partition_count (int): how many lines per partition \"\"\" self . logger . info ( f \"Creating { partition_count } partitions using round robin.\" ) partition_files = [ open ( os . path . join ( output_path , \"part_ {:06d} \" . format ( i )), \"w\" , encoding = \"utf-8\" ) for i in range ( partition_count )] current_index = 0 header_line = None # there can be only on header line for input_file in input_files : self . logger . info ( f \"Opening input file { input_file } \" ) with open ( input_file , \"r\" , encoding = \"utf-8\" ) as input_handler : for line_index , line in enumerate ( input_handler ): if self . header and header_line is None : # if first line of first input file # write that line in every partition header_line = line for partition_file in partition_files : partition_file . write ( header_line ) continue elif self . header and line_index == 0 : # if first line of 2nd... input file, just pass continue partition_files [ current_index % partition_count ] . write ( line ) current_index += 1 for handler in partition_files : handler . close () self . logger . info ( f \"Created { partition_count } partitions\" ) def run ( self , input_path , output_path ): \"\"\"Runs the partition based on provided arguments. Args: input_path (str): path to input file(s) output_path (str): path to store output partitions \"\"\" # Retrieve all input files if os . path . isfile ( input_path ): self . logger . info ( \"Input is one unique file\" ) file_names = [ os . path . basename ( input_path )] input_files = [ input_path ] else : self . logger . info ( \"Input is a directory, listing all of them for processing\" ) file_names = os . listdir ( input_path ) input_files = [ os . path . join ( input_path , file ) for file in file_names ] self . logger . info ( \"Found {} files in {} \" . format ( len ( input_files ), input_path )) if self . mode == \"chunk\" : self . split_by_size ( input_files , output_path , self . number ) elif self . mode == \"roundrobin\" : self . split_by_count ( input_files , output_path , self . number ) elif self . mode == \"append\" : self . split_by_append ( input_files , output_path , self . number ) else : raise NotImplementedError ( f \"Mode { self . mode } not implemented.\" )","title":"PartitioningEngine"},{"location":"references/common/io/#src.common.io.PartitioningEngine.__init__","text":"Constructs and setup of the engine Parameters: Name Type Description Default mode str which partition mode (in PartitioningEngine.PARTITION_MODE list) required number int parameter, behavior depends on mode required header bool are there header in the input files? False logger logging . logger a custom logger, if needed, for this engine to log None Source code in src/common/io.py def __init__ ( self , mode , number , header = False , logger = None ): \"\"\"Constructs and setup of the engine Args: mode (str): which partition mode (in PartitioningEngine.PARTITION_MODE list) number (int): parameter, behavior depends on mode header (bool): are there header in the input files? logger (logging.logger): a custom logger, if needed, for this engine to log \"\"\" self . mode = mode self . number = number self . header = header self . logger = logger or logging . getLogger ( __name__ )","title":"__init__()"},{"location":"references/common/io/#src.common.io.PartitioningEngine.split_by_append","text":"Just appends N++ files in N groups. Parameters: Name Type Description Default input_files List [ str ] list of file paths required output_path str directory path, where to write the partitions required file_count_target int how many partitions we want required Source code in src/common/io.py def split_by_append ( self , input_files , output_path , file_count_target ): \"\"\"Just appends N++ files in N groups. Args: input_files (List[str]): list of file paths output_path (str): directory path, where to write the partitions file_count_target (int): how many partitions we want \"\"\" if len ( input_files ) < file_count_target : raise Exception ( f \"To use mode=append, the number of input files ( { len ( input_files ) } ) needs to be higher than requested number of output files ( { file_count_target } )\" ) # each partition starts as an empty list partitions = [ [] for i in range ( file_count_target ) ] # loop on all files, and put them in one partition for index , input_file in enumerate ( input_files ): partitions [ index % file_count_target ] . append ( input_file ) self . logger . info ( f \"Shuffled { len ( input_files ) } files into { file_count_target } partitions.\" ) # then write each partition by appending content for current_partition_index , partition in enumerate ( partitions ): self . logger . info ( f \"Writing partition { current_partition_index } ...\" ) with open ( os . path . join ( output_path , \"part_ {:06d} \" . format ( current_partition_index )), 'a' , encoding = \"utf-8\" ) as output_handler : for input_file in partition : self . logger . info ( f \"Reading input file { input_file } ...\" ) with open ( input_file , 'r' ) as input_handler : output_handler . write ( input_handler . read ()) self . logger . info ( f \"Created { current_partition_index + 1 } partitions\" )","title":"split_by_append()"},{"location":"references/common/io/#src.common.io.PartitioningEngine.split_by_size","text":"Splits input files into a variable number of partitions by chunking a fixed number of lines from inputs into each output file. Parameters: Name Type Description Default input_files List [ str ] list of file paths required output_path str directory path, where to write the partitions required partition_size int how many lines per partition required Source code in src/common/io.py def split_by_size ( self , input_files , output_path , partition_size ): \"\"\"Splits input files into a variable number of partitions by chunking a fixed number of lines from inputs into each output file. Args: input_files (List[str]): list of file paths output_path (str): directory path, where to write the partitions partition_size (int): how many lines per partition \"\"\" current_partition_size = 0 current_partition_index = 0 self . logger . info ( f \"Creating partition { current_partition_index } \" ) header_line = None # there can be only on header line for input_file in input_files : self . logger . info ( f \"Opening input file { input_file } \" ) with open ( input_file , \"r\" , encoding = \"utf-8\" ) as input_handler : for line in input_handler : if self . header and header_line is None : # if first line of first input file # write that line in every partition header_line = line if partition_size > 0 and current_partition_size >= partition_size : current_partition_index += 1 current_partition_size = 0 self . logger . info ( f \"Creating partition { current_partition_index } \" ) with open ( os . path . join ( output_path , \"part_ {:06d} \" . format ( current_partition_index )), 'a' , encoding = \"utf-8\" ) as output_handler : if self . header and current_partition_size == 0 : # put header before anything else output_handler . write ( header_line ) output_handler . write ( line ) current_partition_size += 1 self . logger . info ( f \"Created { current_partition_index + 1 } partitions\" )","title":"split_by_size()"},{"location":"references/common/io/#src.common.io.PartitioningEngine.split_by_count","text":"Splits input files into a fixed number of partitions by round-robin shuffling of the lines of input files. Parameters: Name Type Description Default input_files List [ str ] list of file paths required output_path str directory path, where to write the partitions required partition_count int how many lines per partition required Source code in src/common/io.py def split_by_count ( self , input_files , output_path , partition_count ): \"\"\"Splits input files into a fixed number of partitions by round-robin shuffling of the lines of input files. Args: input_files (List[str]): list of file paths output_path (str): directory path, where to write the partitions partition_count (int): how many lines per partition \"\"\" self . logger . info ( f \"Creating { partition_count } partitions using round robin.\" ) partition_files = [ open ( os . path . join ( output_path , \"part_ {:06d} \" . format ( i )), \"w\" , encoding = \"utf-8\" ) for i in range ( partition_count )] current_index = 0 header_line = None # there can be only on header line for input_file in input_files : self . logger . info ( f \"Opening input file { input_file } \" ) with open ( input_file , \"r\" , encoding = \"utf-8\" ) as input_handler : for line_index , line in enumerate ( input_handler ): if self . header and header_line is None : # if first line of first input file # write that line in every partition header_line = line for partition_file in partition_files : partition_file . write ( header_line ) continue elif self . header and line_index == 0 : # if first line of 2nd... input file, just pass continue partition_files [ current_index % partition_count ] . write ( line ) current_index += 1 for handler in partition_files : handler . close () self . logger . info ( f \"Created { partition_count } partitions\" )","title":"split_by_count()"},{"location":"references/common/io/#src.common.io.PartitioningEngine.run","text":"Runs the partition based on provided arguments. Parameters: Name Type Description Default input_path str path to input file(s) required output_path str path to store output partitions required Source code in src/common/io.py def run ( self , input_path , output_path ): \"\"\"Runs the partition based on provided arguments. Args: input_path (str): path to input file(s) output_path (str): path to store output partitions \"\"\" # Retrieve all input files if os . path . isfile ( input_path ): self . logger . info ( \"Input is one unique file\" ) file_names = [ os . path . basename ( input_path )] input_files = [ input_path ] else : self . logger . info ( \"Input is a directory, listing all of them for processing\" ) file_names = os . listdir ( input_path ) input_files = [ os . path . join ( input_path , file ) for file in file_names ] self . logger . info ( \"Found {} files in {} \" . format ( len ( input_files ), input_path )) if self . mode == \"chunk\" : self . split_by_size ( input_files , output_path , self . number ) elif self . mode == \"roundrobin\" : self . split_by_count ( input_files , output_path , self . number ) elif self . mode == \"append\" : self . split_by_append ( input_files , output_path , self . number ) else : raise NotImplementedError ( f \"Mode { self . mode } not implemented.\" )","title":"run()"},{"location":"references/common/io/#src.common.io.input_file_path","text":"Argparse type to resolve input path as single file from directory. Given input path can be either a file, or a directory. If it's a directory, this returns the path to the unique file it contains. Parameters: Name Type Description Default path str either file or directory path required Returns: Name Type Description str path to file, or to unique file in directory Source code in src/common/io.py def input_file_path ( path ): \"\"\" Argparse type to resolve input path as single file from directory. Given input path can be either a file, or a directory. If it's a directory, this returns the path to the unique file it contains. Args: path (str): either file or directory path Returns: str: path to file, or to unique file in directory \"\"\" if os . path . isfile ( path ): logging . getLogger ( __name__ ) . info ( f \"Found INPUT file { path } \" ) return path if os . path . isdir ( path ): all_files = os . listdir ( path ) if not all_files : raise Exception ( f \"Could not find any file in specified input directory { path } \" ) if len ( all_files ) > 1 : raise Exception ( f \"Found multiple files in input file path { path } , use input_directory_path type instead.\" ) logging . getLogger ( __name__ ) . info ( f \"Found INPUT directory { path } , selecting unique file { all_files [ 0 ] } \" ) return os . path . join ( path , all_files [ 0 ]) logging . getLogger ( __name__ ) . critical ( f \"Provided INPUT path { path } is neither a directory or a file???\" ) return path","title":"input_file_path()"},{"location":"references/common/io/#src.common.io.get_all_files","text":"Scans some input path and returns a list of files. Parameters: Name Type Description Default path str either a file, or directory path required fail_on_unknown_type bool fails if path is neither a file or a dir? False Returns: Type Description List[str]: list of paths contained in path Source code in src/common/io.py def get_all_files ( path , fail_on_unknown_type = False ): \"\"\" Scans some input path and returns a list of files. Args: path (str): either a file, or directory path fail_on_unknown_type (bool): fails if path is neither a file or a dir? Returns: List[str]: list of paths contained in path \"\"\" # check the existence of the path if exists ( path ) == False : raise Exception ( f \"The specified path { path } does not exist.\" ) # if input path is already a file, return as list if os . path . isfile ( path ): logging . getLogger ( __name__ ) . info ( f \"Found INPUT file { path } \" ) return [ path ] # if input path is a directory, list all files and return if os . path . isdir ( path ): all_files = [ os . path . join ( path , entry ) for entry in os . listdir ( path ) ] if not all_files : raise Exception ( f \"Could not find any file in specified input directory { path } \" ) return all_files if fail_on_unknown_type : raise FileNotFoundError ( f \"Provided INPUT path { path } is neither a directory or a file???\" ) else : logging . getLogger ( __name__ ) . critical ( f \"Provided INPUT path { path } is neither a directory or a file???\" ) return path","title":"get_all_files()"},{"location":"references/common/lightgbm_utils/","text":"This classes provide help to integrate lightgbm LightGBMCallbackHandler This class handles LightGBM callbacks for recording metrics. Source code in src/common/lightgbm_utils.py class LightGBMCallbackHandler (): \"\"\" This class handles LightGBM callbacks for recording metrics. \"\"\" def __init__ ( self , metrics_logger , metrics_prefix = None , metrics_suffix = None ): \"\"\" Args: metrics_logger (common.metrics.MetricsLogger): class to log metrics using MLFlow \"\"\" self . metrics = {} self . metrics_logger = metrics_logger self . metrics_prefix = metrics_prefix self . metrics_suffix = metrics_suffix self . logger = logging . getLogger ( __name__ ) def _format_metric_key ( self , data_name , eval_name ): \"\"\"Builds a metric key with prefix and suffix\"\"\" key = f \" { data_name } . { eval_name } \" if self . metrics_prefix : key = self . metrics_prefix + key if self . metrics_suffix : key = key + self . metrics_suffix return key def callback ( self , env : lightgbm . callback . CallbackEnv ) -> None : \"\"\"Callback method to collect metrics produced by LightGBM. See https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html \"\"\" # let's record in the object for future use self . metrics [ env . iteration ] = env . evaluation_result_list # loop on all the evaluation results tuples for data_name , eval_name , result , _ in env . evaluation_result_list : # log each as a distinct metric self . metrics_logger . log_metric ( key = self . _format_metric_key ( data_name , eval_name ), value = result , step = env . iteration # provide iteration as step in mlflow ) __init__ ( metrics_logger , metrics_prefix = None , metrics_suffix = None ) Parameters: Name Type Description Default metrics_logger common . metrics . MetricsLogger class to log metrics using MLFlow required Source code in src/common/lightgbm_utils.py def __init__ ( self , metrics_logger , metrics_prefix = None , metrics_suffix = None ): \"\"\" Args: metrics_logger (common.metrics.MetricsLogger): class to log metrics using MLFlow \"\"\" self . metrics = {} self . metrics_logger = metrics_logger self . metrics_prefix = metrics_prefix self . metrics_suffix = metrics_suffix self . logger = logging . getLogger ( __name__ ) callback ( env ) Callback method to collect metrics produced by LightGBM. See https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html Source code in src/common/lightgbm_utils.py def callback ( self , env : lightgbm . callback . CallbackEnv ) -> None : \"\"\"Callback method to collect metrics produced by LightGBM. See https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html \"\"\" # let's record in the object for future use self . metrics [ env . iteration ] = env . evaluation_result_list # loop on all the evaluation results tuples for data_name , eval_name , result , _ in env . evaluation_result_list : # log each as a distinct metric self . metrics_logger . log_metric ( key = self . _format_metric_key ( data_name , eval_name ), value = result , step = env . iteration # provide iteration as step in mlflow )","title":"lightgbm_utils.py"},{"location":"references/common/lightgbm_utils/#src.common.lightgbm_utils.LightGBMCallbackHandler","text":"This class handles LightGBM callbacks for recording metrics. Source code in src/common/lightgbm_utils.py class LightGBMCallbackHandler (): \"\"\" This class handles LightGBM callbacks for recording metrics. \"\"\" def __init__ ( self , metrics_logger , metrics_prefix = None , metrics_suffix = None ): \"\"\" Args: metrics_logger (common.metrics.MetricsLogger): class to log metrics using MLFlow \"\"\" self . metrics = {} self . metrics_logger = metrics_logger self . metrics_prefix = metrics_prefix self . metrics_suffix = metrics_suffix self . logger = logging . getLogger ( __name__ ) def _format_metric_key ( self , data_name , eval_name ): \"\"\"Builds a metric key with prefix and suffix\"\"\" key = f \" { data_name } . { eval_name } \" if self . metrics_prefix : key = self . metrics_prefix + key if self . metrics_suffix : key = key + self . metrics_suffix return key def callback ( self , env : lightgbm . callback . CallbackEnv ) -> None : \"\"\"Callback method to collect metrics produced by LightGBM. See https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html \"\"\" # let's record in the object for future use self . metrics [ env . iteration ] = env . evaluation_result_list # loop on all the evaluation results tuples for data_name , eval_name , result , _ in env . evaluation_result_list : # log each as a distinct metric self . metrics_logger . log_metric ( key = self . _format_metric_key ( data_name , eval_name ), value = result , step = env . iteration # provide iteration as step in mlflow )","title":"LightGBMCallbackHandler"},{"location":"references/common/lightgbm_utils/#src.common.lightgbm_utils.LightGBMCallbackHandler.__init__","text":"Parameters: Name Type Description Default metrics_logger common . metrics . MetricsLogger class to log metrics using MLFlow required Source code in src/common/lightgbm_utils.py def __init__ ( self , metrics_logger , metrics_prefix = None , metrics_suffix = None ): \"\"\" Args: metrics_logger (common.metrics.MetricsLogger): class to log metrics using MLFlow \"\"\" self . metrics = {} self . metrics_logger = metrics_logger self . metrics_prefix = metrics_prefix self . metrics_suffix = metrics_suffix self . logger = logging . getLogger ( __name__ )","title":"__init__()"},{"location":"references/common/lightgbm_utils/#src.common.lightgbm_utils.LightGBMCallbackHandler.callback","text":"Callback method to collect metrics produced by LightGBM. See https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html Source code in src/common/lightgbm_utils.py def callback ( self , env : lightgbm . callback . CallbackEnv ) -> None : \"\"\"Callback method to collect metrics produced by LightGBM. See https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html \"\"\" # let's record in the object for future use self . metrics [ env . iteration ] = env . evaluation_result_list # loop on all the evaluation results tuples for data_name , eval_name , result , _ in env . evaluation_result_list : # log each as a distinct metric self . metrics_logger . log_metric ( key = self . _format_metric_key ( data_name , eval_name ), value = result , step = env . iteration # provide iteration as step in mlflow )","title":"callback()"},{"location":"references/common/metrics/","text":"These classes provide some tools to automate wall time compute and logging. MetricsLogger Class for handling metrics logging in MLFlow. Source code in src/common/metrics.py class MetricsLogger (): \"\"\" Class for handling metrics logging in MLFlow. \"\"\" def __init__ ( self , session_name = None , metrics_prefix = None ): self . _metrics_prefix = metrics_prefix self . _session_name = session_name self . _logger = logging . getLogger ( __name__ ) ############################### ### DRIVER SPECIFIC METHODS ### ############################### def open ( self ): self . _logger . info ( f \"Initializing { self . __class__ . __name__ } [session=' { self . _session_name } ', metrics_prefix= { self . _metrics_prefix } ]\" ) def close ( self ): self . _logger . info ( f \"Finalizing { self . __class__ . __name__ } [session=' { self . _session_name } ', metrics_prefix= { self . _metrics_prefix } ]\" ) def log_metric ( self , key , value , step = None , metric_type = MetricType . ONETIME_METRIC ): self . _logger . info ( f \" { self . __class__ . __name__ } .log_metric( { key } , { value } , step= { step } ) [session= { self . _session_name } ]\" ) def log_figure ( self , figure , artifact_file ): self . _logger . info ( f \" { self . __class__ . __name__ } .log_figure(*figure*, { artifact_file } ) [session= { self . _session_name } ]\" ) def log_artifact ( self , local_path , artifact_path = None ): self . _logger . info ( f \" { self . __class__ . __name__ } .log_artifact( { local_path } , { artifact_path } ) [session= { self . _session_name } ]\" ) def log_artifacts ( self , local_dir , artifact_path = None ): self . _logger . info ( f \" { self . __class__ . __name__ } .log_artifacts( { local_dir } , { artifact_path } ) [session= { self . _session_name } ]\" ) def set_properties ( self , ** kwargs ): self . _logger . info ( f \" { self . __class__ . __name__ } .set_properties( { kwargs } ) [session= { self . _session_name } ]\" ) def log_parameters ( self , ** kwargs ): self . _logger . info ( f \" { self . __class__ . __name__ } .log_parameters( { kwargs } ) [session= { self . _session_name } ]\" ) ############################### ### GENERIC UTILITY METHODS ### ############################### @classmethod def _remove_non_allowed_chars ( cls , name_string ): \"\"\" Removes chars not allowed for metric keys \"\"\" return re . sub ( r '[^a-zA-Z0-9_\\-\\.\\ \\/]' , '' , name_string ) def set_platform_properties ( self ): \"\"\" Capture platform sysinfo and record as properties. \"\"\" self . set_properties ( machine = platform . machine (), processor = platform . processor (), architecture = \"-\" . join ( platform . architecture ()), platform = platform . platform (), system = platform . system (), system_version = platform . version (), cpu_count = os . cpu_count (), cpu_frequency = round ( psutil . cpu_freq () . current ), system_memory = round (( psutil . virtual_memory () . total ) / ( 1024 * 1024 * 1024 )) ) def set_properties_from_json ( self , json_string ): \"\"\" Set properties/tags for the session from a json_string. Args: json_string (str): a string parsable as json, contains a dict. \"\"\" try : json_dict = json . loads ( json_string ) except : raise ValueError ( f \"During parsing of JSON properties ' { json_string } ', an exception occured: { traceback . format_exc () } \" ) if not isinstance ( json_dict , dict ): raise ValueError ( f \"Provided JSON properties should be a dict, instead it was { str ( type ( json_dict )) } : { json_string } \" ) properties_dict = dict ( [ ( k , str ( v )) # transform whatever as a string for k , v in json_dict . items () ] ) self . set_properties ( ** properties_dict ) def log_time_block ( self , metric_name , step = None ): \"\"\" [Proxy] Use in a `with` statement to measure execution time of a code block. Uses LogTimeBlock. Example ------- ```python with LogTimeBlock(\"my_perf_metric_name\"): print(\"(((sleeping for 1 second)))\") time.sleep(1) ``` \"\"\" # see class below with proper __enter__ and __exit__ return LogTimeBlock ( metric_name , step = step , metrics_logger = self ) def log_inferencing_latencies ( self , time_per_batch , batch_length = 1 , factor_to_usecs = 1000000.0 ): \"\"\"Logs prediction latencies (for inferencing) with lots of fancy metrics and plots. Args: time_per_batch_list (List[float]): time per inferencing batch batch_lengths (Union[List[int],int]): length of each batch (List or constant) factor_to_usecs (float): factor to apply to time_per_batch to convert to microseconds \"\"\" if isinstance ( batch_length , list ): sum_batch_lengths = sum ( batch_length ) else : sum_batch_lengths = batch_length * len ( time_per_batch ) # log metadata self . log_metric ( \"prediction_batches\" , len ( time_per_batch )) self . log_metric ( \"prediction_queries\" , sum_batch_lengths ) if len ( time_per_batch ) > 0 : self . log_metric ( \"prediction_latency_avg\" , ( sum ( time_per_batch ) * factor_to_usecs ) / sum_batch_lengths ) # usecs # if there's more than 1 batch, compute percentiles if len ( time_per_batch ) > 1 : import numpy as np import matplotlib.pyplot as plt plt . switch_backend ( 'agg' ) # latency per batch batch_run_times = np . array ( time_per_batch ) * factor_to_usecs self . log_metric ( \"batch_latency_p50_usecs\" , np . percentile ( batch_run_times , 50 )) self . log_metric ( \"batch_latency_p75_usecs\" , np . percentile ( batch_run_times , 75 )) self . log_metric ( \"batch_latency_p90_usecs\" , np . percentile ( batch_run_times , 90 )) self . log_metric ( \"batch_latency_p95_usecs\" , np . percentile ( batch_run_times , 95 )) self . log_metric ( \"batch_latency_p99_usecs\" , np . percentile ( batch_run_times , 99 )) # show the distribution prediction latencies fig , ax = plt . subplots ( 1 ) ax . hist ( batch_run_times , bins = 100 ) ax . set_title ( \"Latency-per-batch histogram (log scale)\" ) plt . xlabel ( \"usecs\" ) plt . ylabel ( \"occurence\" ) plt . yscale ( 'log' ) # record in mlflow self . log_figure ( fig , \"batch_latency_log_histogram.png\" ) # latency per query if isinstance ( batch_length , list ): prediction_latencies = np . array ( time_per_batch ) * factor_to_usecs / np . array ( batch_length ) else : prediction_latencies = np . array ( time_per_batch ) * factor_to_usecs / batch_length self . log_metric ( \"prediction_latency_p50_usecs\" , np . percentile ( prediction_latencies , 50 )) self . log_metric ( \"prediction_latency_p75_usecs\" , np . percentile ( prediction_latencies , 75 )) self . log_metric ( \"prediction_latency_p90_usecs\" , np . percentile ( prediction_latencies , 90 )) self . log_metric ( \"prediction_latency_p95_usecs\" , np . percentile ( prediction_latencies , 95 )) self . log_metric ( \"prediction_latency_p99_usecs\" , np . percentile ( prediction_latencies , 99 )) # show the distribution prediction latencies fig , ax = plt . subplots ( 1 ) ax . hist ( prediction_latencies , bins = 100 ) ax . set_title ( \"Latency-per-prediction histogram (log scale)\" ) plt . xlabel ( \"usecs\" ) plt . ylabel ( \"occurence\" ) plt . yscale ( 'log' ) # record in mlflow self . log_figure ( fig , \"prediction_latency_log_histogram.png\" ) set_platform_properties () Capture platform sysinfo and record as properties. Source code in src/common/metrics.py def set_platform_properties ( self ): \"\"\" Capture platform sysinfo and record as properties. \"\"\" self . set_properties ( machine = platform . machine (), processor = platform . processor (), architecture = \"-\" . join ( platform . architecture ()), platform = platform . platform (), system = platform . system (), system_version = platform . version (), cpu_count = os . cpu_count (), cpu_frequency = round ( psutil . cpu_freq () . current ), system_memory = round (( psutil . virtual_memory () . total ) / ( 1024 * 1024 * 1024 )) ) set_properties_from_json ( json_string ) Set properties/tags for the session from a json_string. Parameters: Name Type Description Default json_string str a string parsable as json, contains a dict. required Source code in src/common/metrics.py def set_properties_from_json ( self , json_string ): \"\"\" Set properties/tags for the session from a json_string. Args: json_string (str): a string parsable as json, contains a dict. \"\"\" try : json_dict = json . loads ( json_string ) except : raise ValueError ( f \"During parsing of JSON properties ' { json_string } ', an exception occured: { traceback . format_exc () } \" ) if not isinstance ( json_dict , dict ): raise ValueError ( f \"Provided JSON properties should be a dict, instead it was { str ( type ( json_dict )) } : { json_string } \" ) properties_dict = dict ( [ ( k , str ( v )) # transform whatever as a string for k , v in json_dict . items () ] ) self . set_properties ( ** properties_dict ) log_time_block ( metric_name , step = None ) [Proxy] Use in a with statement to measure execution time of a code block. Uses LogTimeBlock. Example with LogTimeBlock ( \"my_perf_metric_name\" ): print ( \"(((sleeping for 1 second)))\" ) time . sleep ( 1 ) Source code in src/common/metrics.py def log_time_block ( self , metric_name , step = None ): \"\"\" [Proxy] Use in a `with` statement to measure execution time of a code block. Uses LogTimeBlock. Example ------- ```python with LogTimeBlock(\"my_perf_metric_name\"): print(\"(((sleeping for 1 second)))\") time.sleep(1) ``` \"\"\" # see class below with proper __enter__ and __exit__ return LogTimeBlock ( metric_name , step = step , metrics_logger = self ) log_inferencing_latencies ( time_per_batch , batch_length = 1 , factor_to_usecs = 1000000.0 ) Logs prediction latencies (for inferencing) with lots of fancy metrics and plots. Parameters: Name Type Description Default time_per_batch_list List [ float ] time per inferencing batch required batch_lengths Union [ List [ int ], int ] length of each batch (List or constant) required factor_to_usecs float factor to apply to time_per_batch to convert to microseconds 1000000.0 Source code in src/common/metrics.py def log_inferencing_latencies ( self , time_per_batch , batch_length = 1 , factor_to_usecs = 1000000.0 ): \"\"\"Logs prediction latencies (for inferencing) with lots of fancy metrics and plots. Args: time_per_batch_list (List[float]): time per inferencing batch batch_lengths (Union[List[int],int]): length of each batch (List or constant) factor_to_usecs (float): factor to apply to time_per_batch to convert to microseconds \"\"\" if isinstance ( batch_length , list ): sum_batch_lengths = sum ( batch_length ) else : sum_batch_lengths = batch_length * len ( time_per_batch ) # log metadata self . log_metric ( \"prediction_batches\" , len ( time_per_batch )) self . log_metric ( \"prediction_queries\" , sum_batch_lengths ) if len ( time_per_batch ) > 0 : self . log_metric ( \"prediction_latency_avg\" , ( sum ( time_per_batch ) * factor_to_usecs ) / sum_batch_lengths ) # usecs # if there's more than 1 batch, compute percentiles if len ( time_per_batch ) > 1 : import numpy as np import matplotlib.pyplot as plt plt . switch_backend ( 'agg' ) # latency per batch batch_run_times = np . array ( time_per_batch ) * factor_to_usecs self . log_metric ( \"batch_latency_p50_usecs\" , np . percentile ( batch_run_times , 50 )) self . log_metric ( \"batch_latency_p75_usecs\" , np . percentile ( batch_run_times , 75 )) self . log_metric ( \"batch_latency_p90_usecs\" , np . percentile ( batch_run_times , 90 )) self . log_metric ( \"batch_latency_p95_usecs\" , np . percentile ( batch_run_times , 95 )) self . log_metric ( \"batch_latency_p99_usecs\" , np . percentile ( batch_run_times , 99 )) # show the distribution prediction latencies fig , ax = plt . subplots ( 1 ) ax . hist ( batch_run_times , bins = 100 ) ax . set_title ( \"Latency-per-batch histogram (log scale)\" ) plt . xlabel ( \"usecs\" ) plt . ylabel ( \"occurence\" ) plt . yscale ( 'log' ) # record in mlflow self . log_figure ( fig , \"batch_latency_log_histogram.png\" ) # latency per query if isinstance ( batch_length , list ): prediction_latencies = np . array ( time_per_batch ) * factor_to_usecs / np . array ( batch_length ) else : prediction_latencies = np . array ( time_per_batch ) * factor_to_usecs / batch_length self . log_metric ( \"prediction_latency_p50_usecs\" , np . percentile ( prediction_latencies , 50 )) self . log_metric ( \"prediction_latency_p75_usecs\" , np . percentile ( prediction_latencies , 75 )) self . log_metric ( \"prediction_latency_p90_usecs\" , np . percentile ( prediction_latencies , 90 )) self . log_metric ( \"prediction_latency_p95_usecs\" , np . percentile ( prediction_latencies , 95 )) self . log_metric ( \"prediction_latency_p99_usecs\" , np . percentile ( prediction_latencies , 99 )) # show the distribution prediction latencies fig , ax = plt . subplots ( 1 ) ax . hist ( prediction_latencies , bins = 100 ) ax . set_title ( \"Latency-per-prediction histogram (log scale)\" ) plt . xlabel ( \"usecs\" ) plt . ylabel ( \"occurence\" ) plt . yscale ( 'log' ) # record in mlflow self . log_figure ( fig , \"prediction_latency_log_histogram.png\" ) MLFlowMetricsLogger Bases: MetricsLogger Class for handling metrics logging in MLFlow. Source code in src/common/metrics.py class MLFlowMetricsLogger ( MetricsLogger ): \"\"\" Class for handling metrics logging in MLFlow. \"\"\" ############################### ### MLFLOW SPECIFIC METHODS ### ############################### _initialized = False def open ( self ): \"\"\"Opens the MLFlow session.\"\"\" if not MLFlowMetricsLogger . _initialized : super () . open () mlflow . start_run () MLFlowMetricsLogger . _initialized = True def close ( self ): \"\"\"Close the MLFlow session.\"\"\" if MLFlowMetricsLogger . _initialized : super () . close () mlflow . end_run () MLFlowMetricsLogger . _initialized = False else : self . _logger . warning ( f \"Call to finalize MLFLOW [session=' { self . _session_name } '] that was never initialized.\" ) def log_metric ( self , key , value , step = None , metric_type = MetricType . ONETIME_METRIC ): \"\"\"Logs a metric key/value pair. Args: key (str): metric key value (str): metric value step (int): which step to log this metric? (see mlflow.log_metric()) metric_type (int): type of the metric \"\"\" super () . log_metric ( key , value , step = step , metric_type = metric_type ) if self . _metrics_prefix : key = self . _metrics_prefix + key key = self . _remove_non_allowed_chars ( key ) # NOTE: there's a limit to the name of a metric if len ( key ) > 50 : key = key [: 50 ] if type == MetricType . PERF_INTERVAL_METRIC : pass # for now, do not process those else : try : mlflow . log_metric ( key , value , step = step ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log metric using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) def log_figure ( self , figure , artifact_file ): \"\"\"Logs a figure using mlflow Args: figure (Union[matplotlib.figure.Figure, plotly.graph_objects.Figure]): figure to log artifact_file (str): name of file to record \"\"\" super () . log_figure ( figure , artifact_file ) try : mlflow . log_figure ( figure , artifact_file ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log figure using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) def log_artifact ( self , local_path , artifact_path = None ): \"\"\"Logs an artifact Args: local_path (str): Path to the file to write. artifact_path (str): If provided, the directory in artifact_uri to write to. \"\"\" super () . log_artifact ( local_path , artifact_path = artifact_path ) try : mlflow . log_artifact ( local_path , artifact_path = artifact_path ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log artifact using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) def log_artifacts ( self , local_dir , artifact_path = None ): \"\"\"Logs an artifact Args: local_dir (str): Path to the directory of files to write. artifact_path (str): If provided, the directory in artifact_uri to write to. \"\"\" super () . log_artifacts ( local_dir , artifact_path = artifact_path ) try : mlflow . log_artifacts ( local_dir , artifact_path = artifact_path ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log artifacts using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) def set_properties ( self , ** kwargs ): \"\"\"Set properties/tags for the session. Args: kwargs (dict): any keyword argument will be passed as tags to MLFLow \"\"\" super () . set_properties ( ** kwargs ) try : mlflow . set_tags ( kwargs ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not set properties using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) def log_parameters ( self , ** kwargs ): \"\"\" Logs parameters to MLFlow. Args: kwargs (dict): any keyword arguments will be passed as parameters to MLFlow \"\"\" super () . log_parameters ( ** kwargs ) # NOTE: to avoid mlflow exception when value length is too long (ex: label_gain) for key , value in kwargs . items (): if isinstance ( value , str ) and len ( value ) > 255 : self . _logger . warning ( f \"parameter { key } (str) could not be logged, value length { len ( value ) } > 255\" ) else : try : mlflow . log_param ( key , value ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log parameter using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) open () Opens the MLFlow session. Source code in src/common/metrics.py def open ( self ): \"\"\"Opens the MLFlow session.\"\"\" if not MLFlowMetricsLogger . _initialized : super () . open () mlflow . start_run () MLFlowMetricsLogger . _initialized = True close () Close the MLFlow session. Source code in src/common/metrics.py def close ( self ): \"\"\"Close the MLFlow session.\"\"\" if MLFlowMetricsLogger . _initialized : super () . close () mlflow . end_run () MLFlowMetricsLogger . _initialized = False else : self . _logger . warning ( f \"Call to finalize MLFLOW [session=' { self . _session_name } '] that was never initialized.\" ) log_metric ( key , value , step = None , metric_type = MetricType . ONETIME_METRIC ) Logs a metric key/value pair. Parameters: Name Type Description Default key str metric key required value str metric value required step int which step to log this metric? (see mlflow.log_metric()) None metric_type int type of the metric MetricType.ONETIME_METRIC Source code in src/common/metrics.py def log_metric ( self , key , value , step = None , metric_type = MetricType . ONETIME_METRIC ): \"\"\"Logs a metric key/value pair. Args: key (str): metric key value (str): metric value step (int): which step to log this metric? (see mlflow.log_metric()) metric_type (int): type of the metric \"\"\" super () . log_metric ( key , value , step = step , metric_type = metric_type ) if self . _metrics_prefix : key = self . _metrics_prefix + key key = self . _remove_non_allowed_chars ( key ) # NOTE: there's a limit to the name of a metric if len ( key ) > 50 : key = key [: 50 ] if type == MetricType . PERF_INTERVAL_METRIC : pass # for now, do not process those else : try : mlflow . log_metric ( key , value , step = step ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log metric using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) log_figure ( figure , artifact_file ) Logs a figure using mlflow Parameters: Name Type Description Default figure Union [ matplotlib . figure . Figure , plotly . graph_objects . Figure ] figure to log required artifact_file str name of file to record required Source code in src/common/metrics.py def log_figure ( self , figure , artifact_file ): \"\"\"Logs a figure using mlflow Args: figure (Union[matplotlib.figure.Figure, plotly.graph_objects.Figure]): figure to log artifact_file (str): name of file to record \"\"\" super () . log_figure ( figure , artifact_file ) try : mlflow . log_figure ( figure , artifact_file ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log figure using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) log_artifact ( local_path , artifact_path = None ) Logs an artifact Parameters: Name Type Description Default local_path str Path to the file to write. required artifact_path str If provided, the directory in artifact_uri to write to. None Source code in src/common/metrics.py def log_artifact ( self , local_path , artifact_path = None ): \"\"\"Logs an artifact Args: local_path (str): Path to the file to write. artifact_path (str): If provided, the directory in artifact_uri to write to. \"\"\" super () . log_artifact ( local_path , artifact_path = artifact_path ) try : mlflow . log_artifact ( local_path , artifact_path = artifact_path ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log artifact using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) log_artifacts ( local_dir , artifact_path = None ) Logs an artifact Parameters: Name Type Description Default local_dir str Path to the directory of files to write. required artifact_path str If provided, the directory in artifact_uri to write to. None Source code in src/common/metrics.py def log_artifacts ( self , local_dir , artifact_path = None ): \"\"\"Logs an artifact Args: local_dir (str): Path to the directory of files to write. artifact_path (str): If provided, the directory in artifact_uri to write to. \"\"\" super () . log_artifacts ( local_dir , artifact_path = artifact_path ) try : mlflow . log_artifacts ( local_dir , artifact_path = artifact_path ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log artifacts using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) set_properties ( kwargs ) Set properties/tags for the session. Parameters: Name Type Description Default kwargs dict any keyword argument will be passed as tags to MLFLow {} Source code in src/common/metrics.py def set_properties ( self , ** kwargs ): \"\"\"Set properties/tags for the session. Args: kwargs (dict): any keyword argument will be passed as tags to MLFLow \"\"\" super () . set_properties ( ** kwargs ) try : mlflow . set_tags ( kwargs ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not set properties using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) log_parameters ( kwargs ) Logs parameters to MLFlow. Parameters: Name Type Description Default kwargs dict any keyword arguments will be passed as parameters to MLFlow {} Source code in src/common/metrics.py def log_parameters ( self , ** kwargs ): \"\"\" Logs parameters to MLFlow. Args: kwargs (dict): any keyword arguments will be passed as parameters to MLFlow \"\"\" super () . log_parameters ( ** kwargs ) # NOTE: to avoid mlflow exception when value length is too long (ex: label_gain) for key , value in kwargs . items (): if isinstance ( value , str ) and len ( value ) > 255 : self . _logger . warning ( f \"parameter { key } (str) could not be logged, value length { len ( value ) } > 255\" ) else : try : mlflow . log_param ( key , value ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log parameter using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) AzureMLRunMetricsLogger Bases: MetricsLogger Class for handling metrics logging using AzureML Run Source code in src/common/metrics.py class AzureMLRunMetricsLogger ( MetricsLogger ): \"\"\" Class for handling metrics logging using AzureML Run \"\"\" def __init__ ( self , session_name = None , metrics_prefix = None ): super () . __init__ ( session_name = session_name , metrics_prefix = metrics_prefix ) self . _aml_run = None def open ( self ): \"\"\"Opens the AzureML run session.\"\"\" super () . open () try : from azureml.core.run import Run self . _aml_run = Run . get_context () if \"_OfflineRun\" in str ( type ( self . _aml_run )): self . _logger . warning ( f \"Running offline, will not report any AzureML metrics\" ) self . _aml_run = None except BaseException as e : self . _logger . warning ( f \"Run get_context() failed due to exception: { traceback . format_exc () } \" . replace ( \" \\n \" , \"--\" )) self . _aml_run = None def close ( self ): \"\"\"Close the AzureML session.\"\"\" super () . close () if self . _aml_run : self . _aml_run . flush () else : self . _logger . warning ( f \"Call to finalize AzureML Run [session=' { self . _session_name } '] that was never initialized.\" ) def log_metric ( self , key , value , step = None , metric_type = MetricType . ONETIME_METRIC ): \"\"\"Logs a metric key/value pair. Args: key (str): metric key value (str): metric value step (int): which step to log this metric? (see mlflow.log_metric()) metric_type (int): type of the metric \"\"\" super () . log_metric ( key , value , step = step , metric_type = metric_type ) if self . _metrics_prefix : key = self . _metrics_prefix + key key = self . _remove_non_allowed_chars ( key ) # NOTE: there's a limit to the name of a metric if len ( key ) > 50 : key = key [: 50 ] if type == MetricType . PERF_INTERVAL_METRIC : pass # for now, do not process those else : if self . _aml_run : self . _aml_run . log_row ( key , key = value , step = step ) def set_properties ( self , ** kwargs ): \"\"\"Set properties/tags for the session. Args: kwargs (dict): any keyword argument will be passed as tags to MLFLow \"\"\" super () . set_properties ( ** kwargs ) if self . _aml_run : self . _aml_run . add_properties ( kwargs ) def log_parameters ( self , ** kwargs ): \"\"\" Logs parameters to MLFlow. Args: kwargs (dict): any keyword arguments will be passed as parameters to MLFlow \"\"\" super () . log_parameters ( ** kwargs ) # NOTE: to avoid mlflow exception when value length is too long (ex: label_gain) for key , value in kwargs . items (): if isinstance ( value , str ) and len ( value ) > 255 : self . _logger . warning ( f \"parameter { key } (str) could not be logged, value length { len ( value ) } > 255\" ) else : if self . _aml_run : self . _aml_run . set_tags ({ key : value }) open () Opens the AzureML run session. Source code in src/common/metrics.py def open ( self ): \"\"\"Opens the AzureML run session.\"\"\" super () . open () try : from azureml.core.run import Run self . _aml_run = Run . get_context () if \"_OfflineRun\" in str ( type ( self . _aml_run )): self . _logger . warning ( f \"Running offline, will not report any AzureML metrics\" ) self . _aml_run = None except BaseException as e : self . _logger . warning ( f \"Run get_context() failed due to exception: { traceback . format_exc () } \" . replace ( \" \\n \" , \"--\" )) self . _aml_run = None close () Close the AzureML session. Source code in src/common/metrics.py def close ( self ): \"\"\"Close the AzureML session.\"\"\" super () . close () if self . _aml_run : self . _aml_run . flush () else : self . _logger . warning ( f \"Call to finalize AzureML Run [session=' { self . _session_name } '] that was never initialized.\" ) log_metric ( key , value , step = None , metric_type = MetricType . ONETIME_METRIC ) Logs a metric key/value pair. Parameters: Name Type Description Default key str metric key required value str metric value required step int which step to log this metric? (see mlflow.log_metric()) None metric_type int type of the metric MetricType.ONETIME_METRIC Source code in src/common/metrics.py def log_metric ( self , key , value , step = None , metric_type = MetricType . ONETIME_METRIC ): \"\"\"Logs a metric key/value pair. Args: key (str): metric key value (str): metric value step (int): which step to log this metric? (see mlflow.log_metric()) metric_type (int): type of the metric \"\"\" super () . log_metric ( key , value , step = step , metric_type = metric_type ) if self . _metrics_prefix : key = self . _metrics_prefix + key key = self . _remove_non_allowed_chars ( key ) # NOTE: there's a limit to the name of a metric if len ( key ) > 50 : key = key [: 50 ] if type == MetricType . PERF_INTERVAL_METRIC : pass # for now, do not process those else : if self . _aml_run : self . _aml_run . log_row ( key , key = value , step = step ) set_properties ( kwargs ) Set properties/tags for the session. Parameters: Name Type Description Default kwargs dict any keyword argument will be passed as tags to MLFLow {} Source code in src/common/metrics.py def set_properties ( self , ** kwargs ): \"\"\"Set properties/tags for the session. Args: kwargs (dict): any keyword argument will be passed as tags to MLFLow \"\"\" super () . set_properties ( ** kwargs ) if self . _aml_run : self . _aml_run . add_properties ( kwargs ) log_parameters ( kwargs ) Logs parameters to MLFlow. Parameters: Name Type Description Default kwargs dict any keyword arguments will be passed as parameters to MLFlow {} Source code in src/common/metrics.py def log_parameters ( self , ** kwargs ): \"\"\" Logs parameters to MLFlow. Args: kwargs (dict): any keyword arguments will be passed as parameters to MLFlow \"\"\" super () . log_parameters ( ** kwargs ) # NOTE: to avoid mlflow exception when value length is too long (ex: label_gain) for key , value in kwargs . items (): if isinstance ( value , str ) and len ( value ) > 255 : self . _logger . warning ( f \"parameter { key } (str) could not be logged, value length { len ( value ) } > 255\" ) else : if self . _aml_run : self . _aml_run . set_tags ({ key : value }) LogTimeBlock Bases: object This class should be used to time a code block. The time diff is computed from enter to exit . Example with LogTimeBlock ( \"my_perf_metric_name\" ): print ( \"(((sleeping for 1 second)))\" ) time . sleep ( 1 ) Source code in src/common/metrics.py class LogTimeBlock ( object ): \"\"\" This class should be used to time a code block. The time diff is computed from __enter__ to __exit__. Example ------- ```python with LogTimeBlock(\"my_perf_metric_name\"): print(\"(((sleeping for 1 second)))\") time.sleep(1) ``` \"\"\" def __init__ ( self , name , ** kwargs ): \"\"\" Constructs the LogTimeBlock. Args: name (str): key for the time difference (for storing as metric) kwargs (dict): any keyword will be added as properties to metrics for logging (work in progress) \"\"\" # kwargs self . tags = kwargs . get ( 'tags' , None ) self . step = kwargs . get ( 'step' , None ) self . metrics_logger = kwargs . get ( 'metrics_logger' , None ) # internal variables self . name = name self . start_time = None self . _logger = logging . getLogger ( __name__ ) def __enter__ ( self ): \"\"\" Starts the timer, gets triggered at beginning of code block \"\"\" self . start_time = time . time () # starts \"timer\" def __exit__ ( self , exc_type , value , traceback ): \"\"\" Stops the timer and stores accordingly gets triggered at beginning of code block. Note: arguments are by design for with statements. \"\"\" run_time = time . time () - self . start_time # stops \"timer\" self . _logger . info ( f \"--- time elapsed: { self . name } = { run_time : 2f } s\" + ( f \" [tags: { self . tags } ]\" if self . tags else \"\" )) if self . metrics_logger : self . metrics_logger . log_metric ( self . name , run_time , step = self . step ) else : MetricsLogger () . log_metric ( self . name , run_time , step = self . step ) __init__ ( name , kwargs ) Constructs the LogTimeBlock. name (str): key for the time difference (for storing as metric) kwargs (dict): any keyword will be added as properties to metrics for logging (work in progress) Source code in src/common/metrics.py def __init__ ( self , name , ** kwargs ): \"\"\" Constructs the LogTimeBlock. Args: name (str): key for the time difference (for storing as metric) kwargs (dict): any keyword will be added as properties to metrics for logging (work in progress) \"\"\" # kwargs self . tags = kwargs . get ( 'tags' , None ) self . step = kwargs . get ( 'step' , None ) self . metrics_logger = kwargs . get ( 'metrics_logger' , None ) # internal variables self . name = name self . start_time = None self . _logger = logging . getLogger ( __name__ ) __enter__ () Starts the timer, gets triggered at beginning of code block Source code in src/common/metrics.py def __enter__ ( self ): \"\"\" Starts the timer, gets triggered at beginning of code block \"\"\" self . start_time = time . time () # starts \"timer\" __exit__ ( exc_type , value , traceback ) Stops the timer and stores accordingly gets triggered at beginning of code block. Note arguments are by design for with statements. Source code in src/common/metrics.py def __exit__ ( self , exc_type , value , traceback ): \"\"\" Stops the timer and stores accordingly gets triggered at beginning of code block. Note: arguments are by design for with statements. \"\"\" run_time = time . time () - self . start_time # stops \"timer\" self . _logger . info ( f \"--- time elapsed: { self . name } = { run_time : 2f } s\" + ( f \" [tags: { self . tags } ]\" if self . tags else \"\" )) if self . metrics_logger : self . metrics_logger . log_metric ( self . name , run_time , step = self . step ) else : MetricsLogger () . log_metric ( self . name , run_time , step = self . step )","title":"metrics.py"},{"location":"references/common/metrics/#src.common.metrics.MetricsLogger","text":"Class for handling metrics logging in MLFlow. Source code in src/common/metrics.py class MetricsLogger (): \"\"\" Class for handling metrics logging in MLFlow. \"\"\" def __init__ ( self , session_name = None , metrics_prefix = None ): self . _metrics_prefix = metrics_prefix self . _session_name = session_name self . _logger = logging . getLogger ( __name__ ) ############################### ### DRIVER SPECIFIC METHODS ### ############################### def open ( self ): self . _logger . info ( f \"Initializing { self . __class__ . __name__ } [session=' { self . _session_name } ', metrics_prefix= { self . _metrics_prefix } ]\" ) def close ( self ): self . _logger . info ( f \"Finalizing { self . __class__ . __name__ } [session=' { self . _session_name } ', metrics_prefix= { self . _metrics_prefix } ]\" ) def log_metric ( self , key , value , step = None , metric_type = MetricType . ONETIME_METRIC ): self . _logger . info ( f \" { self . __class__ . __name__ } .log_metric( { key } , { value } , step= { step } ) [session= { self . _session_name } ]\" ) def log_figure ( self , figure , artifact_file ): self . _logger . info ( f \" { self . __class__ . __name__ } .log_figure(*figure*, { artifact_file } ) [session= { self . _session_name } ]\" ) def log_artifact ( self , local_path , artifact_path = None ): self . _logger . info ( f \" { self . __class__ . __name__ } .log_artifact( { local_path } , { artifact_path } ) [session= { self . _session_name } ]\" ) def log_artifacts ( self , local_dir , artifact_path = None ): self . _logger . info ( f \" { self . __class__ . __name__ } .log_artifacts( { local_dir } , { artifact_path } ) [session= { self . _session_name } ]\" ) def set_properties ( self , ** kwargs ): self . _logger . info ( f \" { self . __class__ . __name__ } .set_properties( { kwargs } ) [session= { self . _session_name } ]\" ) def log_parameters ( self , ** kwargs ): self . _logger . info ( f \" { self . __class__ . __name__ } .log_parameters( { kwargs } ) [session= { self . _session_name } ]\" ) ############################### ### GENERIC UTILITY METHODS ### ############################### @classmethod def _remove_non_allowed_chars ( cls , name_string ): \"\"\" Removes chars not allowed for metric keys \"\"\" return re . sub ( r '[^a-zA-Z0-9_\\-\\.\\ \\/]' , '' , name_string ) def set_platform_properties ( self ): \"\"\" Capture platform sysinfo and record as properties. \"\"\" self . set_properties ( machine = platform . machine (), processor = platform . processor (), architecture = \"-\" . join ( platform . architecture ()), platform = platform . platform (), system = platform . system (), system_version = platform . version (), cpu_count = os . cpu_count (), cpu_frequency = round ( psutil . cpu_freq () . current ), system_memory = round (( psutil . virtual_memory () . total ) / ( 1024 * 1024 * 1024 )) ) def set_properties_from_json ( self , json_string ): \"\"\" Set properties/tags for the session from a json_string. Args: json_string (str): a string parsable as json, contains a dict. \"\"\" try : json_dict = json . loads ( json_string ) except : raise ValueError ( f \"During parsing of JSON properties ' { json_string } ', an exception occured: { traceback . format_exc () } \" ) if not isinstance ( json_dict , dict ): raise ValueError ( f \"Provided JSON properties should be a dict, instead it was { str ( type ( json_dict )) } : { json_string } \" ) properties_dict = dict ( [ ( k , str ( v )) # transform whatever as a string for k , v in json_dict . items () ] ) self . set_properties ( ** properties_dict ) def log_time_block ( self , metric_name , step = None ): \"\"\" [Proxy] Use in a `with` statement to measure execution time of a code block. Uses LogTimeBlock. Example ------- ```python with LogTimeBlock(\"my_perf_metric_name\"): print(\"(((sleeping for 1 second)))\") time.sleep(1) ``` \"\"\" # see class below with proper __enter__ and __exit__ return LogTimeBlock ( metric_name , step = step , metrics_logger = self ) def log_inferencing_latencies ( self , time_per_batch , batch_length = 1 , factor_to_usecs = 1000000.0 ): \"\"\"Logs prediction latencies (for inferencing) with lots of fancy metrics and plots. Args: time_per_batch_list (List[float]): time per inferencing batch batch_lengths (Union[List[int],int]): length of each batch (List or constant) factor_to_usecs (float): factor to apply to time_per_batch to convert to microseconds \"\"\" if isinstance ( batch_length , list ): sum_batch_lengths = sum ( batch_length ) else : sum_batch_lengths = batch_length * len ( time_per_batch ) # log metadata self . log_metric ( \"prediction_batches\" , len ( time_per_batch )) self . log_metric ( \"prediction_queries\" , sum_batch_lengths ) if len ( time_per_batch ) > 0 : self . log_metric ( \"prediction_latency_avg\" , ( sum ( time_per_batch ) * factor_to_usecs ) / sum_batch_lengths ) # usecs # if there's more than 1 batch, compute percentiles if len ( time_per_batch ) > 1 : import numpy as np import matplotlib.pyplot as plt plt . switch_backend ( 'agg' ) # latency per batch batch_run_times = np . array ( time_per_batch ) * factor_to_usecs self . log_metric ( \"batch_latency_p50_usecs\" , np . percentile ( batch_run_times , 50 )) self . log_metric ( \"batch_latency_p75_usecs\" , np . percentile ( batch_run_times , 75 )) self . log_metric ( \"batch_latency_p90_usecs\" , np . percentile ( batch_run_times , 90 )) self . log_metric ( \"batch_latency_p95_usecs\" , np . percentile ( batch_run_times , 95 )) self . log_metric ( \"batch_latency_p99_usecs\" , np . percentile ( batch_run_times , 99 )) # show the distribution prediction latencies fig , ax = plt . subplots ( 1 ) ax . hist ( batch_run_times , bins = 100 ) ax . set_title ( \"Latency-per-batch histogram (log scale)\" ) plt . xlabel ( \"usecs\" ) plt . ylabel ( \"occurence\" ) plt . yscale ( 'log' ) # record in mlflow self . log_figure ( fig , \"batch_latency_log_histogram.png\" ) # latency per query if isinstance ( batch_length , list ): prediction_latencies = np . array ( time_per_batch ) * factor_to_usecs / np . array ( batch_length ) else : prediction_latencies = np . array ( time_per_batch ) * factor_to_usecs / batch_length self . log_metric ( \"prediction_latency_p50_usecs\" , np . percentile ( prediction_latencies , 50 )) self . log_metric ( \"prediction_latency_p75_usecs\" , np . percentile ( prediction_latencies , 75 )) self . log_metric ( \"prediction_latency_p90_usecs\" , np . percentile ( prediction_latencies , 90 )) self . log_metric ( \"prediction_latency_p95_usecs\" , np . percentile ( prediction_latencies , 95 )) self . log_metric ( \"prediction_latency_p99_usecs\" , np . percentile ( prediction_latencies , 99 )) # show the distribution prediction latencies fig , ax = plt . subplots ( 1 ) ax . hist ( prediction_latencies , bins = 100 ) ax . set_title ( \"Latency-per-prediction histogram (log scale)\" ) plt . xlabel ( \"usecs\" ) plt . ylabel ( \"occurence\" ) plt . yscale ( 'log' ) # record in mlflow self . log_figure ( fig , \"prediction_latency_log_histogram.png\" )","title":"MetricsLogger"},{"location":"references/common/metrics/#src.common.metrics.MetricsLogger.set_platform_properties","text":"Capture platform sysinfo and record as properties. Source code in src/common/metrics.py def set_platform_properties ( self ): \"\"\" Capture platform sysinfo and record as properties. \"\"\" self . set_properties ( machine = platform . machine (), processor = platform . processor (), architecture = \"-\" . join ( platform . architecture ()), platform = platform . platform (), system = platform . system (), system_version = platform . version (), cpu_count = os . cpu_count (), cpu_frequency = round ( psutil . cpu_freq () . current ), system_memory = round (( psutil . virtual_memory () . total ) / ( 1024 * 1024 * 1024 )) )","title":"set_platform_properties()"},{"location":"references/common/metrics/#src.common.metrics.MetricsLogger.set_properties_from_json","text":"Set properties/tags for the session from a json_string. Parameters: Name Type Description Default json_string str a string parsable as json, contains a dict. required Source code in src/common/metrics.py def set_properties_from_json ( self , json_string ): \"\"\" Set properties/tags for the session from a json_string. Args: json_string (str): a string parsable as json, contains a dict. \"\"\" try : json_dict = json . loads ( json_string ) except : raise ValueError ( f \"During parsing of JSON properties ' { json_string } ', an exception occured: { traceback . format_exc () } \" ) if not isinstance ( json_dict , dict ): raise ValueError ( f \"Provided JSON properties should be a dict, instead it was { str ( type ( json_dict )) } : { json_string } \" ) properties_dict = dict ( [ ( k , str ( v )) # transform whatever as a string for k , v in json_dict . items () ] ) self . set_properties ( ** properties_dict )","title":"set_properties_from_json()"},{"location":"references/common/metrics/#src.common.metrics.MetricsLogger.log_time_block","text":"[Proxy] Use in a with statement to measure execution time of a code block. Uses LogTimeBlock.","title":"log_time_block()"},{"location":"references/common/metrics/#src.common.metrics.MetricsLogger.log_time_block--example","text":"with LogTimeBlock ( \"my_perf_metric_name\" ): print ( \"(((sleeping for 1 second)))\" ) time . sleep ( 1 ) Source code in src/common/metrics.py def log_time_block ( self , metric_name , step = None ): \"\"\" [Proxy] Use in a `with` statement to measure execution time of a code block. Uses LogTimeBlock. Example ------- ```python with LogTimeBlock(\"my_perf_metric_name\"): print(\"(((sleeping for 1 second)))\") time.sleep(1) ``` \"\"\" # see class below with proper __enter__ and __exit__ return LogTimeBlock ( metric_name , step = step , metrics_logger = self )","title":"Example"},{"location":"references/common/metrics/#src.common.metrics.MetricsLogger.log_inferencing_latencies","text":"Logs prediction latencies (for inferencing) with lots of fancy metrics and plots. Parameters: Name Type Description Default time_per_batch_list List [ float ] time per inferencing batch required batch_lengths Union [ List [ int ], int ] length of each batch (List or constant) required factor_to_usecs float factor to apply to time_per_batch to convert to microseconds 1000000.0 Source code in src/common/metrics.py def log_inferencing_latencies ( self , time_per_batch , batch_length = 1 , factor_to_usecs = 1000000.0 ): \"\"\"Logs prediction latencies (for inferencing) with lots of fancy metrics and plots. Args: time_per_batch_list (List[float]): time per inferencing batch batch_lengths (Union[List[int],int]): length of each batch (List or constant) factor_to_usecs (float): factor to apply to time_per_batch to convert to microseconds \"\"\" if isinstance ( batch_length , list ): sum_batch_lengths = sum ( batch_length ) else : sum_batch_lengths = batch_length * len ( time_per_batch ) # log metadata self . log_metric ( \"prediction_batches\" , len ( time_per_batch )) self . log_metric ( \"prediction_queries\" , sum_batch_lengths ) if len ( time_per_batch ) > 0 : self . log_metric ( \"prediction_latency_avg\" , ( sum ( time_per_batch ) * factor_to_usecs ) / sum_batch_lengths ) # usecs # if there's more than 1 batch, compute percentiles if len ( time_per_batch ) > 1 : import numpy as np import matplotlib.pyplot as plt plt . switch_backend ( 'agg' ) # latency per batch batch_run_times = np . array ( time_per_batch ) * factor_to_usecs self . log_metric ( \"batch_latency_p50_usecs\" , np . percentile ( batch_run_times , 50 )) self . log_metric ( \"batch_latency_p75_usecs\" , np . percentile ( batch_run_times , 75 )) self . log_metric ( \"batch_latency_p90_usecs\" , np . percentile ( batch_run_times , 90 )) self . log_metric ( \"batch_latency_p95_usecs\" , np . percentile ( batch_run_times , 95 )) self . log_metric ( \"batch_latency_p99_usecs\" , np . percentile ( batch_run_times , 99 )) # show the distribution prediction latencies fig , ax = plt . subplots ( 1 ) ax . hist ( batch_run_times , bins = 100 ) ax . set_title ( \"Latency-per-batch histogram (log scale)\" ) plt . xlabel ( \"usecs\" ) plt . ylabel ( \"occurence\" ) plt . yscale ( 'log' ) # record in mlflow self . log_figure ( fig , \"batch_latency_log_histogram.png\" ) # latency per query if isinstance ( batch_length , list ): prediction_latencies = np . array ( time_per_batch ) * factor_to_usecs / np . array ( batch_length ) else : prediction_latencies = np . array ( time_per_batch ) * factor_to_usecs / batch_length self . log_metric ( \"prediction_latency_p50_usecs\" , np . percentile ( prediction_latencies , 50 )) self . log_metric ( \"prediction_latency_p75_usecs\" , np . percentile ( prediction_latencies , 75 )) self . log_metric ( \"prediction_latency_p90_usecs\" , np . percentile ( prediction_latencies , 90 )) self . log_metric ( \"prediction_latency_p95_usecs\" , np . percentile ( prediction_latencies , 95 )) self . log_metric ( \"prediction_latency_p99_usecs\" , np . percentile ( prediction_latencies , 99 )) # show the distribution prediction latencies fig , ax = plt . subplots ( 1 ) ax . hist ( prediction_latencies , bins = 100 ) ax . set_title ( \"Latency-per-prediction histogram (log scale)\" ) plt . xlabel ( \"usecs\" ) plt . ylabel ( \"occurence\" ) plt . yscale ( 'log' ) # record in mlflow self . log_figure ( fig , \"prediction_latency_log_histogram.png\" )","title":"log_inferencing_latencies()"},{"location":"references/common/metrics/#src.common.metrics.MLFlowMetricsLogger","text":"Bases: MetricsLogger Class for handling metrics logging in MLFlow. Source code in src/common/metrics.py class MLFlowMetricsLogger ( MetricsLogger ): \"\"\" Class for handling metrics logging in MLFlow. \"\"\" ############################### ### MLFLOW SPECIFIC METHODS ### ############################### _initialized = False def open ( self ): \"\"\"Opens the MLFlow session.\"\"\" if not MLFlowMetricsLogger . _initialized : super () . open () mlflow . start_run () MLFlowMetricsLogger . _initialized = True def close ( self ): \"\"\"Close the MLFlow session.\"\"\" if MLFlowMetricsLogger . _initialized : super () . close () mlflow . end_run () MLFlowMetricsLogger . _initialized = False else : self . _logger . warning ( f \"Call to finalize MLFLOW [session=' { self . _session_name } '] that was never initialized.\" ) def log_metric ( self , key , value , step = None , metric_type = MetricType . ONETIME_METRIC ): \"\"\"Logs a metric key/value pair. Args: key (str): metric key value (str): metric value step (int): which step to log this metric? (see mlflow.log_metric()) metric_type (int): type of the metric \"\"\" super () . log_metric ( key , value , step = step , metric_type = metric_type ) if self . _metrics_prefix : key = self . _metrics_prefix + key key = self . _remove_non_allowed_chars ( key ) # NOTE: there's a limit to the name of a metric if len ( key ) > 50 : key = key [: 50 ] if type == MetricType . PERF_INTERVAL_METRIC : pass # for now, do not process those else : try : mlflow . log_metric ( key , value , step = step ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log metric using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) def log_figure ( self , figure , artifact_file ): \"\"\"Logs a figure using mlflow Args: figure (Union[matplotlib.figure.Figure, plotly.graph_objects.Figure]): figure to log artifact_file (str): name of file to record \"\"\" super () . log_figure ( figure , artifact_file ) try : mlflow . log_figure ( figure , artifact_file ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log figure using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) def log_artifact ( self , local_path , artifact_path = None ): \"\"\"Logs an artifact Args: local_path (str): Path to the file to write. artifact_path (str): If provided, the directory in artifact_uri to write to. \"\"\" super () . log_artifact ( local_path , artifact_path = artifact_path ) try : mlflow . log_artifact ( local_path , artifact_path = artifact_path ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log artifact using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) def log_artifacts ( self , local_dir , artifact_path = None ): \"\"\"Logs an artifact Args: local_dir (str): Path to the directory of files to write. artifact_path (str): If provided, the directory in artifact_uri to write to. \"\"\" super () . log_artifacts ( local_dir , artifact_path = artifact_path ) try : mlflow . log_artifacts ( local_dir , artifact_path = artifact_path ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log artifacts using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) def set_properties ( self , ** kwargs ): \"\"\"Set properties/tags for the session. Args: kwargs (dict): any keyword argument will be passed as tags to MLFLow \"\"\" super () . set_properties ( ** kwargs ) try : mlflow . set_tags ( kwargs ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not set properties using MLFLOW due to exception: \\n { traceback . format_exc () } \" ) def log_parameters ( self , ** kwargs ): \"\"\" Logs parameters to MLFlow. Args: kwargs (dict): any keyword arguments will be passed as parameters to MLFlow \"\"\" super () . log_parameters ( ** kwargs ) # NOTE: to avoid mlflow exception when value length is too long (ex: label_gain) for key , value in kwargs . items (): if isinstance ( value , str ) and len ( value ) > 255 : self . _logger . warning ( f \"parameter { key } (str) could not be logged, value length { len ( value ) } > 255\" ) else : try : mlflow . log_param ( key , value ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log parameter using MLFLOW due to exception: \\n { traceback . format_exc () } \" )","title":"MLFlowMetricsLogger"},{"location":"references/common/metrics/#src.common.metrics.MLFlowMetricsLogger.open","text":"Opens the MLFlow session. Source code in src/common/metrics.py def open ( self ): \"\"\"Opens the MLFlow session.\"\"\" if not MLFlowMetricsLogger . _initialized : super () . open () mlflow . start_run () MLFlowMetricsLogger . _initialized = True","title":"open()"},{"location":"references/common/metrics/#src.common.metrics.MLFlowMetricsLogger.close","text":"Close the MLFlow session. Source code in src/common/metrics.py def close ( self ): \"\"\"Close the MLFlow session.\"\"\" if MLFlowMetricsLogger . _initialized : super () . close () mlflow . end_run () MLFlowMetricsLogger . _initialized = False else : self . _logger . warning ( f \"Call to finalize MLFLOW [session=' { self . _session_name } '] that was never initialized.\" )","title":"close()"},{"location":"references/common/metrics/#src.common.metrics.MLFlowMetricsLogger.log_metric","text":"Logs a metric key/value pair. Parameters: Name Type Description Default key str metric key required value str metric value required step int which step to log this metric? (see mlflow.log_metric()) None metric_type int type of the metric MetricType.ONETIME_METRIC Source code in src/common/metrics.py def log_metric ( self , key , value , step = None , metric_type = MetricType . ONETIME_METRIC ): \"\"\"Logs a metric key/value pair. Args: key (str): metric key value (str): metric value step (int): which step to log this metric? (see mlflow.log_metric()) metric_type (int): type of the metric \"\"\" super () . log_metric ( key , value , step = step , metric_type = metric_type ) if self . _metrics_prefix : key = self . _metrics_prefix + key key = self . _remove_non_allowed_chars ( key ) # NOTE: there's a limit to the name of a metric if len ( key ) > 50 : key = key [: 50 ] if type == MetricType . PERF_INTERVAL_METRIC : pass # for now, do not process those else : try : mlflow . log_metric ( key , value , step = step ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log metric using MLFLOW due to exception: \\n { traceback . format_exc () } \" )","title":"log_metric()"},{"location":"references/common/metrics/#src.common.metrics.MLFlowMetricsLogger.log_figure","text":"Logs a figure using mlflow Parameters: Name Type Description Default figure Union [ matplotlib . figure . Figure , plotly . graph_objects . Figure ] figure to log required artifact_file str name of file to record required Source code in src/common/metrics.py def log_figure ( self , figure , artifact_file ): \"\"\"Logs a figure using mlflow Args: figure (Union[matplotlib.figure.Figure, plotly.graph_objects.Figure]): figure to log artifact_file (str): name of file to record \"\"\" super () . log_figure ( figure , artifact_file ) try : mlflow . log_figure ( figure , artifact_file ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log figure using MLFLOW due to exception: \\n { traceback . format_exc () } \" )","title":"log_figure()"},{"location":"references/common/metrics/#src.common.metrics.MLFlowMetricsLogger.log_artifact","text":"Logs an artifact Parameters: Name Type Description Default local_path str Path to the file to write. required artifact_path str If provided, the directory in artifact_uri to write to. None Source code in src/common/metrics.py def log_artifact ( self , local_path , artifact_path = None ): \"\"\"Logs an artifact Args: local_path (str): Path to the file to write. artifact_path (str): If provided, the directory in artifact_uri to write to. \"\"\" super () . log_artifact ( local_path , artifact_path = artifact_path ) try : mlflow . log_artifact ( local_path , artifact_path = artifact_path ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log artifact using MLFLOW due to exception: \\n { traceback . format_exc () } \" )","title":"log_artifact()"},{"location":"references/common/metrics/#src.common.metrics.MLFlowMetricsLogger.log_artifacts","text":"Logs an artifact Parameters: Name Type Description Default local_dir str Path to the directory of files to write. required artifact_path str If provided, the directory in artifact_uri to write to. None Source code in src/common/metrics.py def log_artifacts ( self , local_dir , artifact_path = None ): \"\"\"Logs an artifact Args: local_dir (str): Path to the directory of files to write. artifact_path (str): If provided, the directory in artifact_uri to write to. \"\"\" super () . log_artifacts ( local_dir , artifact_path = artifact_path ) try : mlflow . log_artifacts ( local_dir , artifact_path = artifact_path ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log artifacts using MLFLOW due to exception: \\n { traceback . format_exc () } \" )","title":"log_artifacts()"},{"location":"references/common/metrics/#src.common.metrics.MLFlowMetricsLogger.set_properties","text":"Set properties/tags for the session. Parameters: Name Type Description Default kwargs dict any keyword argument will be passed as tags to MLFLow {} Source code in src/common/metrics.py def set_properties ( self , ** kwargs ): \"\"\"Set properties/tags for the session. Args: kwargs (dict): any keyword argument will be passed as tags to MLFLow \"\"\" super () . set_properties ( ** kwargs ) try : mlflow . set_tags ( kwargs ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not set properties using MLFLOW due to exception: \\n { traceback . format_exc () } \" )","title":"set_properties()"},{"location":"references/common/metrics/#src.common.metrics.MLFlowMetricsLogger.log_parameters","text":"Logs parameters to MLFlow. Parameters: Name Type Description Default kwargs dict any keyword arguments will be passed as parameters to MLFlow {} Source code in src/common/metrics.py def log_parameters ( self , ** kwargs ): \"\"\" Logs parameters to MLFlow. Args: kwargs (dict): any keyword arguments will be passed as parameters to MLFlow \"\"\" super () . log_parameters ( ** kwargs ) # NOTE: to avoid mlflow exception when value length is too long (ex: label_gain) for key , value in kwargs . items (): if isinstance ( value , str ) and len ( value ) > 255 : self . _logger . warning ( f \"parameter { key } (str) could not be logged, value length { len ( value ) } > 255\" ) else : try : mlflow . log_param ( key , value ) except mlflow . exceptions . MlflowException : self . _logger . critical ( f \"Could not log parameter using MLFLOW due to exception: \\n { traceback . format_exc () } \" )","title":"log_parameters()"},{"location":"references/common/metrics/#src.common.metrics.AzureMLRunMetricsLogger","text":"Bases: MetricsLogger Class for handling metrics logging using AzureML Run Source code in src/common/metrics.py class AzureMLRunMetricsLogger ( MetricsLogger ): \"\"\" Class for handling metrics logging using AzureML Run \"\"\" def __init__ ( self , session_name = None , metrics_prefix = None ): super () . __init__ ( session_name = session_name , metrics_prefix = metrics_prefix ) self . _aml_run = None def open ( self ): \"\"\"Opens the AzureML run session.\"\"\" super () . open () try : from azureml.core.run import Run self . _aml_run = Run . get_context () if \"_OfflineRun\" in str ( type ( self . _aml_run )): self . _logger . warning ( f \"Running offline, will not report any AzureML metrics\" ) self . _aml_run = None except BaseException as e : self . _logger . warning ( f \"Run get_context() failed due to exception: { traceback . format_exc () } \" . replace ( \" \\n \" , \"--\" )) self . _aml_run = None def close ( self ): \"\"\"Close the AzureML session.\"\"\" super () . close () if self . _aml_run : self . _aml_run . flush () else : self . _logger . warning ( f \"Call to finalize AzureML Run [session=' { self . _session_name } '] that was never initialized.\" ) def log_metric ( self , key , value , step = None , metric_type = MetricType . ONETIME_METRIC ): \"\"\"Logs a metric key/value pair. Args: key (str): metric key value (str): metric value step (int): which step to log this metric? (see mlflow.log_metric()) metric_type (int): type of the metric \"\"\" super () . log_metric ( key , value , step = step , metric_type = metric_type ) if self . _metrics_prefix : key = self . _metrics_prefix + key key = self . _remove_non_allowed_chars ( key ) # NOTE: there's a limit to the name of a metric if len ( key ) > 50 : key = key [: 50 ] if type == MetricType . PERF_INTERVAL_METRIC : pass # for now, do not process those else : if self . _aml_run : self . _aml_run . log_row ( key , key = value , step = step ) def set_properties ( self , ** kwargs ): \"\"\"Set properties/tags for the session. Args: kwargs (dict): any keyword argument will be passed as tags to MLFLow \"\"\" super () . set_properties ( ** kwargs ) if self . _aml_run : self . _aml_run . add_properties ( kwargs ) def log_parameters ( self , ** kwargs ): \"\"\" Logs parameters to MLFlow. Args: kwargs (dict): any keyword arguments will be passed as parameters to MLFlow \"\"\" super () . log_parameters ( ** kwargs ) # NOTE: to avoid mlflow exception when value length is too long (ex: label_gain) for key , value in kwargs . items (): if isinstance ( value , str ) and len ( value ) > 255 : self . _logger . warning ( f \"parameter { key } (str) could not be logged, value length { len ( value ) } > 255\" ) else : if self . _aml_run : self . _aml_run . set_tags ({ key : value })","title":"AzureMLRunMetricsLogger"},{"location":"references/common/metrics/#src.common.metrics.AzureMLRunMetricsLogger.open","text":"Opens the AzureML run session. Source code in src/common/metrics.py def open ( self ): \"\"\"Opens the AzureML run session.\"\"\" super () . open () try : from azureml.core.run import Run self . _aml_run = Run . get_context () if \"_OfflineRun\" in str ( type ( self . _aml_run )): self . _logger . warning ( f \"Running offline, will not report any AzureML metrics\" ) self . _aml_run = None except BaseException as e : self . _logger . warning ( f \"Run get_context() failed due to exception: { traceback . format_exc () } \" . replace ( \" \\n \" , \"--\" )) self . _aml_run = None","title":"open()"},{"location":"references/common/metrics/#src.common.metrics.AzureMLRunMetricsLogger.close","text":"Close the AzureML session. Source code in src/common/metrics.py def close ( self ): \"\"\"Close the AzureML session.\"\"\" super () . close () if self . _aml_run : self . _aml_run . flush () else : self . _logger . warning ( f \"Call to finalize AzureML Run [session=' { self . _session_name } '] that was never initialized.\" )","title":"close()"},{"location":"references/common/metrics/#src.common.metrics.AzureMLRunMetricsLogger.log_metric","text":"Logs a metric key/value pair. Parameters: Name Type Description Default key str metric key required value str metric value required step int which step to log this metric? (see mlflow.log_metric()) None metric_type int type of the metric MetricType.ONETIME_METRIC Source code in src/common/metrics.py def log_metric ( self , key , value , step = None , metric_type = MetricType . ONETIME_METRIC ): \"\"\"Logs a metric key/value pair. Args: key (str): metric key value (str): metric value step (int): which step to log this metric? (see mlflow.log_metric()) metric_type (int): type of the metric \"\"\" super () . log_metric ( key , value , step = step , metric_type = metric_type ) if self . _metrics_prefix : key = self . _metrics_prefix + key key = self . _remove_non_allowed_chars ( key ) # NOTE: there's a limit to the name of a metric if len ( key ) > 50 : key = key [: 50 ] if type == MetricType . PERF_INTERVAL_METRIC : pass # for now, do not process those else : if self . _aml_run : self . _aml_run . log_row ( key , key = value , step = step )","title":"log_metric()"},{"location":"references/common/metrics/#src.common.metrics.AzureMLRunMetricsLogger.set_properties","text":"Set properties/tags for the session. Parameters: Name Type Description Default kwargs dict any keyword argument will be passed as tags to MLFLow {} Source code in src/common/metrics.py def set_properties ( self , ** kwargs ): \"\"\"Set properties/tags for the session. Args: kwargs (dict): any keyword argument will be passed as tags to MLFLow \"\"\" super () . set_properties ( ** kwargs ) if self . _aml_run : self . _aml_run . add_properties ( kwargs )","title":"set_properties()"},{"location":"references/common/metrics/#src.common.metrics.AzureMLRunMetricsLogger.log_parameters","text":"Logs parameters to MLFlow. Parameters: Name Type Description Default kwargs dict any keyword arguments will be passed as parameters to MLFlow {} Source code in src/common/metrics.py def log_parameters ( self , ** kwargs ): \"\"\" Logs parameters to MLFlow. Args: kwargs (dict): any keyword arguments will be passed as parameters to MLFlow \"\"\" super () . log_parameters ( ** kwargs ) # NOTE: to avoid mlflow exception when value length is too long (ex: label_gain) for key , value in kwargs . items (): if isinstance ( value , str ) and len ( value ) > 255 : self . _logger . warning ( f \"parameter { key } (str) could not be logged, value length { len ( value ) } > 255\" ) else : if self . _aml_run : self . _aml_run . set_tags ({ key : value })","title":"log_parameters()"},{"location":"references/common/metrics/#src.common.metrics.LogTimeBlock","text":"Bases: object This class should be used to time a code block. The time diff is computed from enter to exit .","title":"LogTimeBlock"},{"location":"references/common/metrics/#src.common.metrics.LogTimeBlock--example","text":"with LogTimeBlock ( \"my_perf_metric_name\" ): print ( \"(((sleeping for 1 second)))\" ) time . sleep ( 1 ) Source code in src/common/metrics.py class LogTimeBlock ( object ): \"\"\" This class should be used to time a code block. The time diff is computed from __enter__ to __exit__. Example ------- ```python with LogTimeBlock(\"my_perf_metric_name\"): print(\"(((sleeping for 1 second)))\") time.sleep(1) ``` \"\"\" def __init__ ( self , name , ** kwargs ): \"\"\" Constructs the LogTimeBlock. Args: name (str): key for the time difference (for storing as metric) kwargs (dict): any keyword will be added as properties to metrics for logging (work in progress) \"\"\" # kwargs self . tags = kwargs . get ( 'tags' , None ) self . step = kwargs . get ( 'step' , None ) self . metrics_logger = kwargs . get ( 'metrics_logger' , None ) # internal variables self . name = name self . start_time = None self . _logger = logging . getLogger ( __name__ ) def __enter__ ( self ): \"\"\" Starts the timer, gets triggered at beginning of code block \"\"\" self . start_time = time . time () # starts \"timer\" def __exit__ ( self , exc_type , value , traceback ): \"\"\" Stops the timer and stores accordingly gets triggered at beginning of code block. Note: arguments are by design for with statements. \"\"\" run_time = time . time () - self . start_time # stops \"timer\" self . _logger . info ( f \"--- time elapsed: { self . name } = { run_time : 2f } s\" + ( f \" [tags: { self . tags } ]\" if self . tags else \"\" )) if self . metrics_logger : self . metrics_logger . log_metric ( self . name , run_time , step = self . step ) else : MetricsLogger () . log_metric ( self . name , run_time , step = self . step )","title":"Example"},{"location":"references/common/metrics/#src.common.metrics.LogTimeBlock.__init__","text":"Constructs the LogTimeBlock. name (str): key for the time difference (for storing as metric) kwargs (dict): any keyword will be added as properties to metrics for logging (work in progress) Source code in src/common/metrics.py def __init__ ( self , name , ** kwargs ): \"\"\" Constructs the LogTimeBlock. Args: name (str): key for the time difference (for storing as metric) kwargs (dict): any keyword will be added as properties to metrics for logging (work in progress) \"\"\" # kwargs self . tags = kwargs . get ( 'tags' , None ) self . step = kwargs . get ( 'step' , None ) self . metrics_logger = kwargs . get ( 'metrics_logger' , None ) # internal variables self . name = name self . start_time = None self . _logger = logging . getLogger ( __name__ )","title":"__init__()"},{"location":"references/common/metrics/#src.common.metrics.LogTimeBlock.__enter__","text":"Starts the timer, gets triggered at beginning of code block Source code in src/common/metrics.py def __enter__ ( self ): \"\"\" Starts the timer, gets triggered at beginning of code block \"\"\" self . start_time = time . time () # starts \"timer\"","title":"__enter__()"},{"location":"references/common/metrics/#src.common.metrics.LogTimeBlock.__exit__","text":"Stops the timer and stores accordingly gets triggered at beginning of code block. Note arguments are by design for with statements. Source code in src/common/metrics.py def __exit__ ( self , exc_type , value , traceback ): \"\"\" Stops the timer and stores accordingly gets triggered at beginning of code block. Note: arguments are by design for with statements. \"\"\" run_time = time . time () - self . start_time # stops \"timer\" self . _logger . info ( f \"--- time elapsed: { self . name } = { run_time : 2f } s\" + ( f \" [tags: { self . tags } ]\" if self . tags else \"\" )) if self . metrics_logger : self . metrics_logger . log_metric ( self . name , run_time , step = self . step ) else : MetricsLogger () . log_metric ( self . name , run_time , step = self . step )","title":"__exit__()"},{"location":"references/common/perf/","text":"Each component in this repository is reporting performance metrics in MLFlow. The list of available metrics is detailed below: Metric Level Description cpu_avg_utilization_over20_pct One value per node How much time every cpu of that node are utilized more than 20%, over the total job time. cpu_avg_utilization_over40_pct One value per node How much time are all cpus are utilized more than 40%, over the total job time. cpu_avg_utilization_over80_pct One value per node ow much time are all cpus are utilized more than 80%, over the total job time. cpu_avg_utilization_at100_pct One value per node How much time are all cpus fully utilized at 100%, over the total job time. cpu_avg_utilization_pct One value per node How much are every cpu utilized on average during the entire job. max_t_cpu_pct_per_cpu_avg One value per node Maximum of average cpu utilization over the entire job time. max_t_cpu_pct_per_cpu_max One value per node Maximum of maximum cpu utilization over the entire job time. max_t_cpu_pct_per_cpu_min One value per node Maximum of minimum cpu utilization over the entire job time. node_cpu_hours One value per node time * #cpus node_unused_cpu_hours One value per node time * #cpus * (1 - cpu_avg_utilization_pct) max_t_mem_percent One value per node Maximum of memory utilization over the entire job time. max_t_disk_usage_percent One value per node Maximum of disk usage over the entire job time. total_disk_io_read_mb One value per node Total disk data read in MB (max value at the end of job). total_disk_io_write_mb One value per node Total disk data write in MB (max value at the end of job). total_net_io_lo_sent_mb One value per node Total net data sent on loopback device (max value at the end of job). total_net_io_ext_sent_mb One value per node Total net data sent on external device (max value at the end of job). total_net_io_lo_recv_mb One value per node Total net data received on loopback device (max value at the end of job). total_net_io_ext_recv_mb One value per node Total net data received on external device (max value at the end of job). Helps with reporting performance metrics (cpu/mem utilization). Needs to be implemented in the rest of the code. PerformanceReportingThread Bases: threading . Thread Thread to report performance (cpu/mem/net) Source code in src/common/perf.py class PerformanceReportingThread ( threading . Thread ): \"\"\"Thread to report performance (cpu/mem/net)\"\"\" def __init__ ( self , initial_time_increment = 1.0 , cpu_interval = 1.0 , callback_on_loop = None , callback_on_exit = None ): \"\"\"Constructor Args: initial_time_increment (float): how much time to sleep between perf readings cpu_interval (float): interval to capture cpu utilization callback_on_loop (func): function to call when a perf reading is issued callback_on_exit (func): function to call when thread is finalized \"\"\" threading . Thread . __init__ ( self ) self . killed = False # flag, set to True to kill from the inside self . logger = logging . getLogger ( __name__ ) # time between perf reports self . time_increment = initial_time_increment self . cpu_interval = cpu_interval # set callbacks self . callback_on_loop = callback_on_loop self . callback_on_exit = callback_on_exit ##################### ### RUN FUNCTIONS ### ##################### def run ( self ): \"\"\"Run function of the thread, while(True)\"\"\" while not ( self . killed ): if self . time_increment >= self . cpu_interval : # cpu_percent.interval already consumes 1sec time . sleep ( self . time_increment - self . cpu_interval ) # will double every time report_store_max_length is reached self . _run_loop () if self . callback_on_exit : self . callback_on_exit () def _run_loop ( self ): \"\"\"What to run within the while(not(killed))\"\"\" perf_report = {} # CPU UTILIZATION cpu_utilization = psutil . cpu_percent ( interval = self . cpu_interval , percpu = True ) # will take 1 sec to return perf_report [ \"cpu_pct_per_cpu_avg\" ] = sum ( cpu_utilization ) / len ( cpu_utilization ) perf_report [ \"cpu_pct_per_cpu_min\" ] = min ( cpu_utilization ) perf_report [ \"cpu_pct_per_cpu_max\" ] = max ( cpu_utilization ) # MEM UTILIZATION perf_report [ \"mem_percent\" ] = psutil . virtual_memory () . percent # DISK UTILIZAITON perf_report [ \"disk_usage_percent\" ] = psutil . disk_usage ( '/' ) . percent perf_report [ \"disk_io_read_mb\" ] = ( psutil . disk_io_counters ( perdisk = False ) . read_bytes / ( 1024 * 1024 )) perf_report [ \"disk_io_write_mb\" ] = ( psutil . disk_io_counters ( perdisk = False ) . write_bytes / ( 1024 * 1024 )) # NET I/O SEND/RECV net_io_counters = psutil . net_io_counters ( pernic = True ) net_io_lo_identifiers = [] net_io_ext_identifiers = [] for key in net_io_counters : if 'loopback' in key . lower (): net_io_lo_identifiers . append ( key ) elif key . lower () == 'lo' : net_io_lo_identifiers . append ( key ) else : net_io_ext_identifiers . append ( key ) lo_sent_mb = sum ( [ net_io_counters . get ( key ) . bytes_sent for key in net_io_lo_identifiers ] ) / ( 1024 * 1024 ) ext_sent_mb = sum ( [ net_io_counters . get ( key ) . bytes_sent for key in net_io_ext_identifiers ] ) / ( 1024 * 1024 ) lo_recv_mb = sum ( [ net_io_counters . get ( key ) . bytes_recv for key in net_io_lo_identifiers ] ) / ( 1024 * 1024 ) ext_recv_mb = sum ( [ net_io_counters . get ( key ) . bytes_recv for key in net_io_ext_identifiers ] ) / ( 1024 * 1024 ) perf_report [ \"net_io_lo_sent_mb\" ] = lo_sent_mb perf_report [ \"net_io_ext_sent_mb\" ] = ext_sent_mb perf_report [ \"net_io_lo_recv_mb\" ] = lo_recv_mb perf_report [ \"net_io_ext_recv_mb\" ] = ext_recv_mb # add a timestamp perf_report [ \"timestamp\" ] = time . time () # END OF REPORT if self . callback_on_loop : self . callback_on_loop ( perf_report ) def finalize ( self ): \"\"\"Ask the thread to finalize (clean)\"\"\" self . killed = True self . join () __init__ ( initial_time_increment = 1.0 , cpu_interval = 1.0 , callback_on_loop = None , callback_on_exit = None ) Constructor Parameters: Name Type Description Default initial_time_increment float how much time to sleep between perf readings 1.0 cpu_interval float interval to capture cpu utilization 1.0 callback_on_loop func function to call when a perf reading is issued None callback_on_exit func function to call when thread is finalized None Source code in src/common/perf.py def __init__ ( self , initial_time_increment = 1.0 , cpu_interval = 1.0 , callback_on_loop = None , callback_on_exit = None ): \"\"\"Constructor Args: initial_time_increment (float): how much time to sleep between perf readings cpu_interval (float): interval to capture cpu utilization callback_on_loop (func): function to call when a perf reading is issued callback_on_exit (func): function to call when thread is finalized \"\"\" threading . Thread . __init__ ( self ) self . killed = False # flag, set to True to kill from the inside self . logger = logging . getLogger ( __name__ ) # time between perf reports self . time_increment = initial_time_increment self . cpu_interval = cpu_interval # set callbacks self . callback_on_loop = callback_on_loop self . callback_on_exit = callback_on_exit run () Run function of the thread, while(True) Source code in src/common/perf.py def run ( self ): \"\"\"Run function of the thread, while(True)\"\"\" while not ( self . killed ): if self . time_increment >= self . cpu_interval : # cpu_percent.interval already consumes 1sec time . sleep ( self . time_increment - self . cpu_interval ) # will double every time report_store_max_length is reached self . _run_loop () if self . callback_on_exit : self . callback_on_exit () finalize () Ask the thread to finalize (clean) Source code in src/common/perf.py def finalize ( self ): \"\"\"Ask the thread to finalize (clean)\"\"\" self . killed = True self . join () PerformanceMetricsCollector Collects performance metrics from PerformanceReportingThread Limits all values to a maximum length Source code in src/common/perf.py class PerformanceMetricsCollector (): \"\"\"Collects performance metrics from PerformanceReportingThread Limits all values to a maximum length\"\"\" def __init__ ( self , max_length = 1000 ): \"\"\"Constructor Args: max_length (int): maximum number of perf reports to keep \"\"\" self . logger = logging . getLogger ( __name__ ) # create a thread to generate reports regularly self . report_thread = PerformanceReportingThread ( initial_time_increment = 1.0 , cpu_interval = 1.0 , callback_on_loop = self . append_perf_metrics ) self . perf_reports = [] # internal storage self . perf_reports_freqs = 1 # frequency to skip reports from thread self . perf_reports_counter = 0 # how many reports we had so far self . max_length = ( max_length // 2 + max_length % 2 ) * 2 # has to be dividable by 2 def start ( self ): \"\"\"Start collector perf metrics (start internal thread)\"\"\" self . logger . info ( f \"Starting perf metric collector (max_length= { self . max_length } )\" ) self . report_thread . start () def finalize ( self ): \"\"\"Stop collector perf metrics (stop internal thread)\"\"\" self . logger . info ( f \"Finalizing perf metric collector (length= { len ( self . perf_reports ) } )\" ) self . report_thread . finalize () def append_perf_metrics ( self , perf_metrics ): \"\"\"Add a perf metric report to the internal storage\"\"\" self . perf_reports_counter += 1 if ( self . perf_reports_counter % self . perf_reports_freqs ): # if we've decided to skip this one return self . perf_reports . append ( perf_metrics ) if len ( self . perf_reports ) > self . max_length : # trim the report by half self . perf_reports = [ self . perf_reports [ i ] for i in range ( 0 , self . max_length , 2 ) ] self . perf_reports_freqs *= 2 # we'll start accepting reports only 1 out of 2 self . logger . warning ( f \"Perf report store reached max, increasing freq to { self . perf_reports_freqs } \" ) __init__ ( max_length = 1000 ) Constructor Parameters: Name Type Description Default max_length int maximum number of perf reports to keep 1000 Source code in src/common/perf.py def __init__ ( self , max_length = 1000 ): \"\"\"Constructor Args: max_length (int): maximum number of perf reports to keep \"\"\" self . logger = logging . getLogger ( __name__ ) # create a thread to generate reports regularly self . report_thread = PerformanceReportingThread ( initial_time_increment = 1.0 , cpu_interval = 1.0 , callback_on_loop = self . append_perf_metrics ) self . perf_reports = [] # internal storage self . perf_reports_freqs = 1 # frequency to skip reports from thread self . perf_reports_counter = 0 # how many reports we had so far self . max_length = ( max_length // 2 + max_length % 2 ) * 2 # has to be dividable by 2 start () Start collector perf metrics (start internal thread) Source code in src/common/perf.py def start ( self ): \"\"\"Start collector perf metrics (start internal thread)\"\"\" self . logger . info ( f \"Starting perf metric collector (max_length= { self . max_length } )\" ) self . report_thread . start () finalize () Stop collector perf metrics (stop internal thread) Source code in src/common/perf.py def finalize ( self ): \"\"\"Stop collector perf metrics (stop internal thread)\"\"\" self . logger . info ( f \"Finalizing perf metric collector (length= { len ( self . perf_reports ) } )\" ) self . report_thread . finalize () append_perf_metrics ( perf_metrics ) Add a perf metric report to the internal storage Source code in src/common/perf.py def append_perf_metrics ( self , perf_metrics ): \"\"\"Add a perf metric report to the internal storage\"\"\" self . perf_reports_counter += 1 if ( self . perf_reports_counter % self . perf_reports_freqs ): # if we've decided to skip this one return self . perf_reports . append ( perf_metrics ) if len ( self . perf_reports ) > self . max_length : # trim the report by half self . perf_reports = [ self . perf_reports [ i ] for i in range ( 0 , self . max_length , 2 ) ] self . perf_reports_freqs *= 2 # we'll start accepting reports only 1 out of 2 self . logger . warning ( f \"Perf report store reached max, increasing freq to { self . perf_reports_freqs } \" ) PerfReportPlotter Once collected all perf reports from all nodes Source code in src/common/perf.py class PerfReportPlotter (): \"\"\"Once collected all perf reports from all nodes\"\"\" def __init__ ( self , metrics_logger ): self . all_reports = {} self . metrics_logger = metrics_logger def save_to ( self , perf_report_file_path = None ): \"\"\"Saves all reports into a json file\"\"\" # if no file path provided, create a temp file if perf_report_file_path is None : perf_report_file_path = tempfile . NamedTemporaryFile ( suffix = \".json\" ) . name with open ( perf_report_file_path , \"w\" ) as out_file : out_file . write ( json . dumps ( self . all_reports , indent = \" \" )) return perf_report_file_path def add_perf_reports ( self , perf_reports , node ): \"\"\"Add a set of reports from a given node\"\"\" self . all_reports [ node ] = perf_reports def report_nodes_perf ( self ): # Currently reporting one metric per node for node in self . all_reports : # CPU UTILIZATION cpu_avg_utilization = [ report [ \"cpu_pct_per_cpu_avg\" ] for report in self . all_reports [ node ] ] self . metrics_logger . log_metric ( \"max_t_(cpu_pct_per_cpu_avg)\" , max ( cpu_avg_utilization ), step = node ) self . metrics_logger . log_metric ( \"cpu_avg_utilization_pct\" , sum ( cpu_avg_utilization ) / len ( cpu_avg_utilization ), step = node ) self . metrics_logger . log_metric ( \"cpu_avg_utilization_at100_pct\" , sum ( [ utilization >= 100.0 for utilization in cpu_avg_utilization ]) / len ( cpu_avg_utilization ) * 100.0 , step = node ) self . metrics_logger . log_metric ( \"cpu_avg_utilization_over80_pct\" , sum ( [ utilization >= 80.0 for utilization in cpu_avg_utilization ]) / len ( cpu_avg_utilization ) * 100.0 , step = node ) self . metrics_logger . log_metric ( \"cpu_avg_utilization_over40_pct\" , sum ( [ utilization >= 40.0 for utilization in cpu_avg_utilization ]) / len ( cpu_avg_utilization ) * 100.0 , step = node ) self . metrics_logger . log_metric ( \"cpu_avg_utilization_over20_pct\" , sum ( [ utilization >= 20.0 for utilization in cpu_avg_utilization ]) / len ( cpu_avg_utilization ) * 100.0 , step = node ) self . metrics_logger . log_metric ( \"max_t_(cpu_pct_per_cpu_min)\" , max ([ report [ \"cpu_pct_per_cpu_min\" ] for report in self . all_reports [ node ] ]), step = node ) self . metrics_logger . log_metric ( \"max_t_(cpu_pct_per_cpu_max)\" , max ([ report [ \"cpu_pct_per_cpu_max\" ] for report in self . all_reports [ node ] ]), step = node ) # \"CPU HOURS\" job_internal_cpu_hours = ( time . time () - self . all_reports [ node ][ 0 ][ \"timestamp\" ]) * psutil . cpu_count () / 60 / 60 self . metrics_logger . log_metric ( \"node_cpu_hours\" , job_internal_cpu_hours , step = node ) self . metrics_logger . log_metric ( \"node_unused_cpu_hours\" , job_internal_cpu_hours * ( 100.0 - sum ( cpu_avg_utilization ) / len ( cpu_avg_utilization )) / 100.0 , step = node ) # MEM self . metrics_logger . log_metric ( \"max_t_(mem_percent)\" , max ([ report [ \"mem_percent\" ] for report in self . all_reports [ node ] ]), step = node ) # DISK self . metrics_logger . log_metric ( \"max_t_disk_usage_percent\" , max ([ report [ \"disk_usage_percent\" ] for report in self . all_reports [ node ] ]), step = node ) self . metrics_logger . log_metric ( \"total_disk_io_read_mb\" , max ([ report [ \"disk_io_read_mb\" ] for report in self . all_reports [ node ] ]), step = node ) self . metrics_logger . log_metric ( \"total_disk_io_write_mb\" , max ([ report [ \"disk_io_write_mb\" ] for report in self . all_reports [ node ] ]), step = node ) # NET I/O self . metrics_logger . log_metric ( \"total_net_io_lo_sent_mb\" , max ([ report [ \"net_io_lo_sent_mb\" ] for report in self . all_reports [ node ] ]), step = node ) self . metrics_logger . log_metric ( \"total_net_io_ext_sent_mb\" , max ([ report [ \"net_io_ext_sent_mb\" ] for report in self . all_reports [ node ] ]), step = node ) self . metrics_logger . log_metric ( \"total_net_io_lo_recv_mb\" , max ([ report [ \"net_io_lo_recv_mb\" ] for report in self . all_reports [ node ] ]), step = node ) self . metrics_logger . log_metric ( \"total_net_io_ext_recv_mb\" , max ([ report [ \"net_io_ext_recv_mb\" ] for report in self . all_reports [ node ] ]), step = node ) save_to ( perf_report_file_path = None ) Saves all reports into a json file Source code in src/common/perf.py def save_to ( self , perf_report_file_path = None ): \"\"\"Saves all reports into a json file\"\"\" # if no file path provided, create a temp file if perf_report_file_path is None : perf_report_file_path = tempfile . NamedTemporaryFile ( suffix = \".json\" ) . name with open ( perf_report_file_path , \"w\" ) as out_file : out_file . write ( json . dumps ( self . all_reports , indent = \" \" )) return perf_report_file_path add_perf_reports ( perf_reports , node ) Add a set of reports from a given node Source code in src/common/perf.py def add_perf_reports ( self , perf_reports , node ): \"\"\"Add a set of reports from a given node\"\"\" self . all_reports [ node ] = perf_reports","title":"perf.py"},{"location":"references/common/perf/#src.common.perf.PerformanceReportingThread","text":"Bases: threading . Thread Thread to report performance (cpu/mem/net) Source code in src/common/perf.py class PerformanceReportingThread ( threading . Thread ): \"\"\"Thread to report performance (cpu/mem/net)\"\"\" def __init__ ( self , initial_time_increment = 1.0 , cpu_interval = 1.0 , callback_on_loop = None , callback_on_exit = None ): \"\"\"Constructor Args: initial_time_increment (float): how much time to sleep between perf readings cpu_interval (float): interval to capture cpu utilization callback_on_loop (func): function to call when a perf reading is issued callback_on_exit (func): function to call when thread is finalized \"\"\" threading . Thread . __init__ ( self ) self . killed = False # flag, set to True to kill from the inside self . logger = logging . getLogger ( __name__ ) # time between perf reports self . time_increment = initial_time_increment self . cpu_interval = cpu_interval # set callbacks self . callback_on_loop = callback_on_loop self . callback_on_exit = callback_on_exit ##################### ### RUN FUNCTIONS ### ##################### def run ( self ): \"\"\"Run function of the thread, while(True)\"\"\" while not ( self . killed ): if self . time_increment >= self . cpu_interval : # cpu_percent.interval already consumes 1sec time . sleep ( self . time_increment - self . cpu_interval ) # will double every time report_store_max_length is reached self . _run_loop () if self . callback_on_exit : self . callback_on_exit () def _run_loop ( self ): \"\"\"What to run within the while(not(killed))\"\"\" perf_report = {} # CPU UTILIZATION cpu_utilization = psutil . cpu_percent ( interval = self . cpu_interval , percpu = True ) # will take 1 sec to return perf_report [ \"cpu_pct_per_cpu_avg\" ] = sum ( cpu_utilization ) / len ( cpu_utilization ) perf_report [ \"cpu_pct_per_cpu_min\" ] = min ( cpu_utilization ) perf_report [ \"cpu_pct_per_cpu_max\" ] = max ( cpu_utilization ) # MEM UTILIZATION perf_report [ \"mem_percent\" ] = psutil . virtual_memory () . percent # DISK UTILIZAITON perf_report [ \"disk_usage_percent\" ] = psutil . disk_usage ( '/' ) . percent perf_report [ \"disk_io_read_mb\" ] = ( psutil . disk_io_counters ( perdisk = False ) . read_bytes / ( 1024 * 1024 )) perf_report [ \"disk_io_write_mb\" ] = ( psutil . disk_io_counters ( perdisk = False ) . write_bytes / ( 1024 * 1024 )) # NET I/O SEND/RECV net_io_counters = psutil . net_io_counters ( pernic = True ) net_io_lo_identifiers = [] net_io_ext_identifiers = [] for key in net_io_counters : if 'loopback' in key . lower (): net_io_lo_identifiers . append ( key ) elif key . lower () == 'lo' : net_io_lo_identifiers . append ( key ) else : net_io_ext_identifiers . append ( key ) lo_sent_mb = sum ( [ net_io_counters . get ( key ) . bytes_sent for key in net_io_lo_identifiers ] ) / ( 1024 * 1024 ) ext_sent_mb = sum ( [ net_io_counters . get ( key ) . bytes_sent for key in net_io_ext_identifiers ] ) / ( 1024 * 1024 ) lo_recv_mb = sum ( [ net_io_counters . get ( key ) . bytes_recv for key in net_io_lo_identifiers ] ) / ( 1024 * 1024 ) ext_recv_mb = sum ( [ net_io_counters . get ( key ) . bytes_recv for key in net_io_ext_identifiers ] ) / ( 1024 * 1024 ) perf_report [ \"net_io_lo_sent_mb\" ] = lo_sent_mb perf_report [ \"net_io_ext_sent_mb\" ] = ext_sent_mb perf_report [ \"net_io_lo_recv_mb\" ] = lo_recv_mb perf_report [ \"net_io_ext_recv_mb\" ] = ext_recv_mb # add a timestamp perf_report [ \"timestamp\" ] = time . time () # END OF REPORT if self . callback_on_loop : self . callback_on_loop ( perf_report ) def finalize ( self ): \"\"\"Ask the thread to finalize (clean)\"\"\" self . killed = True self . join ()","title":"PerformanceReportingThread"},{"location":"references/common/perf/#src.common.perf.PerformanceReportingThread.__init__","text":"Constructor Parameters: Name Type Description Default initial_time_increment float how much time to sleep between perf readings 1.0 cpu_interval float interval to capture cpu utilization 1.0 callback_on_loop func function to call when a perf reading is issued None callback_on_exit func function to call when thread is finalized None Source code in src/common/perf.py def __init__ ( self , initial_time_increment = 1.0 , cpu_interval = 1.0 , callback_on_loop = None , callback_on_exit = None ): \"\"\"Constructor Args: initial_time_increment (float): how much time to sleep between perf readings cpu_interval (float): interval to capture cpu utilization callback_on_loop (func): function to call when a perf reading is issued callback_on_exit (func): function to call when thread is finalized \"\"\" threading . Thread . __init__ ( self ) self . killed = False # flag, set to True to kill from the inside self . logger = logging . getLogger ( __name__ ) # time between perf reports self . time_increment = initial_time_increment self . cpu_interval = cpu_interval # set callbacks self . callback_on_loop = callback_on_loop self . callback_on_exit = callback_on_exit","title":"__init__()"},{"location":"references/common/perf/#src.common.perf.PerformanceReportingThread.run","text":"Run function of the thread, while(True) Source code in src/common/perf.py def run ( self ): \"\"\"Run function of the thread, while(True)\"\"\" while not ( self . killed ): if self . time_increment >= self . cpu_interval : # cpu_percent.interval already consumes 1sec time . sleep ( self . time_increment - self . cpu_interval ) # will double every time report_store_max_length is reached self . _run_loop () if self . callback_on_exit : self . callback_on_exit ()","title":"run()"},{"location":"references/common/perf/#src.common.perf.PerformanceReportingThread.finalize","text":"Ask the thread to finalize (clean) Source code in src/common/perf.py def finalize ( self ): \"\"\"Ask the thread to finalize (clean)\"\"\" self . killed = True self . join ()","title":"finalize()"},{"location":"references/common/perf/#src.common.perf.PerformanceMetricsCollector","text":"Collects performance metrics from PerformanceReportingThread Limits all values to a maximum length Source code in src/common/perf.py class PerformanceMetricsCollector (): \"\"\"Collects performance metrics from PerformanceReportingThread Limits all values to a maximum length\"\"\" def __init__ ( self , max_length = 1000 ): \"\"\"Constructor Args: max_length (int): maximum number of perf reports to keep \"\"\" self . logger = logging . getLogger ( __name__ ) # create a thread to generate reports regularly self . report_thread = PerformanceReportingThread ( initial_time_increment = 1.0 , cpu_interval = 1.0 , callback_on_loop = self . append_perf_metrics ) self . perf_reports = [] # internal storage self . perf_reports_freqs = 1 # frequency to skip reports from thread self . perf_reports_counter = 0 # how many reports we had so far self . max_length = ( max_length // 2 + max_length % 2 ) * 2 # has to be dividable by 2 def start ( self ): \"\"\"Start collector perf metrics (start internal thread)\"\"\" self . logger . info ( f \"Starting perf metric collector (max_length= { self . max_length } )\" ) self . report_thread . start () def finalize ( self ): \"\"\"Stop collector perf metrics (stop internal thread)\"\"\" self . logger . info ( f \"Finalizing perf metric collector (length= { len ( self . perf_reports ) } )\" ) self . report_thread . finalize () def append_perf_metrics ( self , perf_metrics ): \"\"\"Add a perf metric report to the internal storage\"\"\" self . perf_reports_counter += 1 if ( self . perf_reports_counter % self . perf_reports_freqs ): # if we've decided to skip this one return self . perf_reports . append ( perf_metrics ) if len ( self . perf_reports ) > self . max_length : # trim the report by half self . perf_reports = [ self . perf_reports [ i ] for i in range ( 0 , self . max_length , 2 ) ] self . perf_reports_freqs *= 2 # we'll start accepting reports only 1 out of 2 self . logger . warning ( f \"Perf report store reached max, increasing freq to { self . perf_reports_freqs } \" )","title":"PerformanceMetricsCollector"},{"location":"references/common/perf/#src.common.perf.PerformanceMetricsCollector.__init__","text":"Constructor Parameters: Name Type Description Default max_length int maximum number of perf reports to keep 1000 Source code in src/common/perf.py def __init__ ( self , max_length = 1000 ): \"\"\"Constructor Args: max_length (int): maximum number of perf reports to keep \"\"\" self . logger = logging . getLogger ( __name__ ) # create a thread to generate reports regularly self . report_thread = PerformanceReportingThread ( initial_time_increment = 1.0 , cpu_interval = 1.0 , callback_on_loop = self . append_perf_metrics ) self . perf_reports = [] # internal storage self . perf_reports_freqs = 1 # frequency to skip reports from thread self . perf_reports_counter = 0 # how many reports we had so far self . max_length = ( max_length // 2 + max_length % 2 ) * 2 # has to be dividable by 2","title":"__init__()"},{"location":"references/common/perf/#src.common.perf.PerformanceMetricsCollector.start","text":"Start collector perf metrics (start internal thread) Source code in src/common/perf.py def start ( self ): \"\"\"Start collector perf metrics (start internal thread)\"\"\" self . logger . info ( f \"Starting perf metric collector (max_length= { self . max_length } )\" ) self . report_thread . start ()","title":"start()"},{"location":"references/common/perf/#src.common.perf.PerformanceMetricsCollector.finalize","text":"Stop collector perf metrics (stop internal thread) Source code in src/common/perf.py def finalize ( self ): \"\"\"Stop collector perf metrics (stop internal thread)\"\"\" self . logger . info ( f \"Finalizing perf metric collector (length= { len ( self . perf_reports ) } )\" ) self . report_thread . finalize ()","title":"finalize()"},{"location":"references/common/perf/#src.common.perf.PerformanceMetricsCollector.append_perf_metrics","text":"Add a perf metric report to the internal storage Source code in src/common/perf.py def append_perf_metrics ( self , perf_metrics ): \"\"\"Add a perf metric report to the internal storage\"\"\" self . perf_reports_counter += 1 if ( self . perf_reports_counter % self . perf_reports_freqs ): # if we've decided to skip this one return self . perf_reports . append ( perf_metrics ) if len ( self . perf_reports ) > self . max_length : # trim the report by half self . perf_reports = [ self . perf_reports [ i ] for i in range ( 0 , self . max_length , 2 ) ] self . perf_reports_freqs *= 2 # we'll start accepting reports only 1 out of 2 self . logger . warning ( f \"Perf report store reached max, increasing freq to { self . perf_reports_freqs } \" )","title":"append_perf_metrics()"},{"location":"references/common/perf/#src.common.perf.PerfReportPlotter","text":"Once collected all perf reports from all nodes Source code in src/common/perf.py class PerfReportPlotter (): \"\"\"Once collected all perf reports from all nodes\"\"\" def __init__ ( self , metrics_logger ): self . all_reports = {} self . metrics_logger = metrics_logger def save_to ( self , perf_report_file_path = None ): \"\"\"Saves all reports into a json file\"\"\" # if no file path provided, create a temp file if perf_report_file_path is None : perf_report_file_path = tempfile . NamedTemporaryFile ( suffix = \".json\" ) . name with open ( perf_report_file_path , \"w\" ) as out_file : out_file . write ( json . dumps ( self . all_reports , indent = \" \" )) return perf_report_file_path def add_perf_reports ( self , perf_reports , node ): \"\"\"Add a set of reports from a given node\"\"\" self . all_reports [ node ] = perf_reports def report_nodes_perf ( self ): # Currently reporting one metric per node for node in self . all_reports : # CPU UTILIZATION cpu_avg_utilization = [ report [ \"cpu_pct_per_cpu_avg\" ] for report in self . all_reports [ node ] ] self . metrics_logger . log_metric ( \"max_t_(cpu_pct_per_cpu_avg)\" , max ( cpu_avg_utilization ), step = node ) self . metrics_logger . log_metric ( \"cpu_avg_utilization_pct\" , sum ( cpu_avg_utilization ) / len ( cpu_avg_utilization ), step = node ) self . metrics_logger . log_metric ( \"cpu_avg_utilization_at100_pct\" , sum ( [ utilization >= 100.0 for utilization in cpu_avg_utilization ]) / len ( cpu_avg_utilization ) * 100.0 , step = node ) self . metrics_logger . log_metric ( \"cpu_avg_utilization_over80_pct\" , sum ( [ utilization >= 80.0 for utilization in cpu_avg_utilization ]) / len ( cpu_avg_utilization ) * 100.0 , step = node ) self . metrics_logger . log_metric ( \"cpu_avg_utilization_over40_pct\" , sum ( [ utilization >= 40.0 for utilization in cpu_avg_utilization ]) / len ( cpu_avg_utilization ) * 100.0 , step = node ) self . metrics_logger . log_metric ( \"cpu_avg_utilization_over20_pct\" , sum ( [ utilization >= 20.0 for utilization in cpu_avg_utilization ]) / len ( cpu_avg_utilization ) * 100.0 , step = node ) self . metrics_logger . log_metric ( \"max_t_(cpu_pct_per_cpu_min)\" , max ([ report [ \"cpu_pct_per_cpu_min\" ] for report in self . all_reports [ node ] ]), step = node ) self . metrics_logger . log_metric ( \"max_t_(cpu_pct_per_cpu_max)\" , max ([ report [ \"cpu_pct_per_cpu_max\" ] for report in self . all_reports [ node ] ]), step = node ) # \"CPU HOURS\" job_internal_cpu_hours = ( time . time () - self . all_reports [ node ][ 0 ][ \"timestamp\" ]) * psutil . cpu_count () / 60 / 60 self . metrics_logger . log_metric ( \"node_cpu_hours\" , job_internal_cpu_hours , step = node ) self . metrics_logger . log_metric ( \"node_unused_cpu_hours\" , job_internal_cpu_hours * ( 100.0 - sum ( cpu_avg_utilization ) / len ( cpu_avg_utilization )) / 100.0 , step = node ) # MEM self . metrics_logger . log_metric ( \"max_t_(mem_percent)\" , max ([ report [ \"mem_percent\" ] for report in self . all_reports [ node ] ]), step = node ) # DISK self . metrics_logger . log_metric ( \"max_t_disk_usage_percent\" , max ([ report [ \"disk_usage_percent\" ] for report in self . all_reports [ node ] ]), step = node ) self . metrics_logger . log_metric ( \"total_disk_io_read_mb\" , max ([ report [ \"disk_io_read_mb\" ] for report in self . all_reports [ node ] ]), step = node ) self . metrics_logger . log_metric ( \"total_disk_io_write_mb\" , max ([ report [ \"disk_io_write_mb\" ] for report in self . all_reports [ node ] ]), step = node ) # NET I/O self . metrics_logger . log_metric ( \"total_net_io_lo_sent_mb\" , max ([ report [ \"net_io_lo_sent_mb\" ] for report in self . all_reports [ node ] ]), step = node ) self . metrics_logger . log_metric ( \"total_net_io_ext_sent_mb\" , max ([ report [ \"net_io_ext_sent_mb\" ] for report in self . all_reports [ node ] ]), step = node ) self . metrics_logger . log_metric ( \"total_net_io_lo_recv_mb\" , max ([ report [ \"net_io_lo_recv_mb\" ] for report in self . all_reports [ node ] ]), step = node ) self . metrics_logger . log_metric ( \"total_net_io_ext_recv_mb\" , max ([ report [ \"net_io_ext_recv_mb\" ] for report in self . all_reports [ node ] ]), step = node )","title":"PerfReportPlotter"},{"location":"references/common/perf/#src.common.perf.PerfReportPlotter.save_to","text":"Saves all reports into a json file Source code in src/common/perf.py def save_to ( self , perf_report_file_path = None ): \"\"\"Saves all reports into a json file\"\"\" # if no file path provided, create a temp file if perf_report_file_path is None : perf_report_file_path = tempfile . NamedTemporaryFile ( suffix = \".json\" ) . name with open ( perf_report_file_path , \"w\" ) as out_file : out_file . write ( json . dumps ( self . all_reports , indent = \" \" )) return perf_report_file_path","title":"save_to()"},{"location":"references/common/perf/#src.common.perf.PerfReportPlotter.add_perf_reports","text":"Add a set of reports from a given node Source code in src/common/perf.py def add_perf_reports ( self , perf_reports , node ): \"\"\"Add a set of reports from a given node\"\"\" self . all_reports [ node ] = perf_reports","title":"add_perf_reports()"},{"location":"references/common/tasks/","text":"This contains all the configuration dataclasses needed to configure AzureML pipelines. from dataclasses import dataclass from omegaconf import MISSING from typing import Any , Optional @dataclass class data_input_spec : # NOTE: Union is not supported in Hydra/OmegaConf # specify either by dataset name and version name : Optional [ str ] = None version : Optional [ str ] = None # or by uuid (non-registered) uuid : Optional [ str ] = None # or by datastore+path datastore : Optional [ str ] = None path : Optional [ str ] = None validate : bool = True @dataclass class inferencing_task : data : data_input_spec = MISSING model : data_input_spec = MISSING task_key : Optional [ str ] = None predict_disable_shape_check : bool = False @dataclass class inferencing_variants : framework : str = MISSING build : Optional [ str ] = None @dataclass class data_generation_task : task : str = MISSING task_key : Optional [ str ] = None train_samples : int = MISSING train_partitions : int = 1 test_samples : int = MISSING test_partitions : int = 1 inferencing_samples : int = MISSING inferencing_partitions : int = 1 n_features : int = MISSING n_informative : Optional [ int ] = None n_label_classes : Optional [ int ] = None docs_per_query : Optional [ int ] = None delimiter : str = \"comma\" header : bool = False @dataclass class training_task : train : data_input_spec = MISSING test : data_input_spec = MISSING # provide a key for internal tagging + reporting task_key : Optional [ str ] = None @dataclass class sweep_early_termination_settings : policy_type : str = 'default' # truncation_selection | median_stopping | bandit evaluation_interval : Optional [ int ] = None delay_evaluation : Optional [ int ] = None # truncation settings truncation_percentage : Optional [ int ] = None # for truncation_selection # bandit settings slack_factor : Optional [ float ] = None @dataclass class sweep_limits_settings : max_total_trials : int = MISSING max_concurrent_trials : Optional [ int ] = None # must be between 1 and 100 timeout_minutes : Optional [ int ] = None @dataclass class sweep_settings : # TODO: add all parameters from shrike https://github.com/Azure/shrike/blob/387fadb47d69e46bd7e5ac6f243250dc6044afaa/shrike/pipeline/pipeline_helper.py#L809 # goal settings primary_metric : Optional [ str ] = None goal : Optional [ str ] = None algorithm : str = \"random\" early_termination : Optional [ sweep_early_termination_settings ] = None limits : Optional [ sweep_limits_settings ] = None @dataclass class lightgbm_training_variant_parameters : # fixed training parameters objective : str = MISSING metric : str = MISSING boosting : str = MISSING tree_learner : str = MISSING # sweepable training parameters # NOTE: need to be str so they can be parsed (ex: 'choice(100,200)') num_iterations : str = MISSING num_leaves : str = MISSING min_data_in_leaf : str = MISSING learning_rate : str = MISSING max_bin : str = MISSING feature_fraction : str = MISSING label_gain : Optional [ str ] = None custom_params : Optional [ Any ] = None # COMPUTE device_type : str = \"cpu\" multinode_driver : str = \"socket\" verbose : bool = False @dataclass class lightgbm_training_data_variant_parameters : # FILE OPTIONS auto_partitioning : bool = True pre_convert_to_binary : bool = False # doesn't work with partitioned data (yet) # input parameters header : bool = False label_column : Optional [ str ] = \"0\" group_column : Optional [ str ] = None construct : bool = True # data formats train_data_format : Optional [ str ] = None test_data_format : Optional [ str ] = None @dataclass class lightgbm_training_environment_variant_parameters : # COMPUTE nodes : int = 1 processes : int = 1 target : Optional [ str ] = None build : Optional [ str ] = None @dataclass class lightgbm_training_output_variant_parameters : register_model : bool = False # \"{register_model_prefix}-{task_key}-{num_iterations}trees-{num_leaves}leaves-{register_model_suffix}\" register_model_prefix : Optional [ str ] = None register_model_suffix : Optional [ str ] = None @dataclass class training_variant : # below are mandatory sections of the variant config framework : str = MISSING data : lightgbm_training_data_variant_parameters = MISSING training : lightgbm_training_variant_parameters = MISSING runtime : lightgbm_training_environment_variant_parameters = MISSING # below are optional raytune : Optional [ Any ] = None sweep : Optional [ sweep_settings ] = None output : Optional [ lightgbm_training_output_variant_parameters ] = None","title":"tasks.py"},{"location":"references/scripts/sample/sample/","text":"Usage This script is a tutorial sample script to explain how all the benchmark scripts are structured and standardized using the RunnableScript helper class. We've numbered the steps you need to modify and adapt this sample script to your own needs. Follow each STEP below, and their associated TODO. SampleScript Bases: RunnableScript STEP 1 : Package your script as a class. This class inherits from RunnableScript, that factors duplicate code to achieve usual routines of every script in the lightgbm-benchmark repo. It has a standard main() function (see below) that you should not need to modify except for edge cases. See src/common/components.py for details on that class. TODO: name your class specifically for this script Source code in src/scripts/sample/sample.py class SampleScript ( RunnableScript ): \"\"\" STEP 1 : Package your script as a class. This class inherits from RunnableScript, that factors duplicate code to achieve usual routines of every script in the lightgbm-benchmark repo. It has a standard main() function (see below) that you should not need to modify except for edge cases. See `src/common/components.py` for details on that class. TODO: name your class specifically for this script \"\"\" def __init__ ( self ): \"\"\" STEP 3 : Define your benchmark \"task\" in the constructor. This Specific constructor for this SampleScript class. It has no arguments, as it will be called from the helper `main()` method. In your custom script class, you need to call the super constructor with the parameters below. TODO: pick your task (score, train, generate, compile, ...) TODO: name your framework and version \"\"\" super () . __init__ ( task = \"sample_task\" , # str framework = \"sample_framework\" , # str framework_version = \"0.0.1\" # str ) @classmethod def get_arg_parser ( cls , parser = None ): \"\"\" STEP 4 : Define your arguments This method will be called by the main() function to add your script custom arguments to argparse, on top of standard arguments of the benchmark. TODO: align this section with your requirements. Args: parser (argparse.ArgumentParser): an existing argument parser instance Returns: ArgumentParser: the argument parser instance \"\"\" # IMPORTANT: call this to add generic benchmark arguments parser = RunnableScript . get_arg_parser ( parser ) # add arguments that are specific to your script # here's a couple examples group_i = parser . add_argument_group ( f \"I/O Arguments [ { __name__ } : { cls . __name__ } ]\" ) group_i . add_argument ( \"--data\" , required = True , type = input_file_path , # use this helper type for a directory containing a single file help = \"Some input location (directory containing a unique file)\" , ) group_i . add_argument ( \"--model\" , required = True , type = input_file_path , # use this helper type for a directory containing a single file help = \"Some input location (directory containing a unique file)\" , ) group_i . add_argument ( \"--output\" , required = True , default = None , type = str , help = \"Some output location (directory)\" , ) # make sure to return parser return parser def run ( self , args , logger , metrics_logger , unknown_args ): \"\"\" STEP 5 : Define your run function. This is the core function of your script. You are required to override this method with your own implementation. Args: args (argparse.namespace): command line arguments provided to script logger (logging.logger): a logger initialized for this script metrics_logger (common.metrics.MetricLogger): to report metrics for this script, already initialized for MLFlow unknown_args (list[str]): list of arguments not recognized during argparse \"\"\" # make sure the output argument exists os . makedirs ( args . output , exist_ok = True ) # and create your own file inside the output args . output = os . path . join ( args . output , \"predictions.txt\" ) # CUSTOM CODE STARTS HERE # below this line is user code logger . info ( f \"Loading model from { args . model } \" ) booster = lightgbm . Booster ( model_file = args . model ) # to log executing time of a code block, use log_time_block() logger . info ( f \"Loading data for inferencing\" ) with metrics_logger . log_time_block ( metric_name = \"time_data_loading\" ): inference_data = lightgbm . Dataset ( args . data , free_raw_data = False ) . construct () inference_raw_data = inference_data . get_data () # optional: add data shape as property metrics_logger . set_properties ( inference_data_length = inference_data . num_data (), inference_data_width = inference_data . num_feature (), ) # to log executing time of a code block, use log_time_block() logger . info ( f \"Running .predict()\" ) with metrics_logger . log_time_block ( metric_name = \"time_inferencing\" ): booster . predict ( data = inference_raw_data ) __init__ () STEP 3 : Define your benchmark \"task\" in the constructor. This Specific constructor for this SampleScript class. It has no arguments, as it will be called from the helper main() method. In your custom script class, you need to call the super constructor with the parameters below. TODO: pick your task (score, train, generate, compile, ...) TODO: name your framework and version Source code in src/scripts/sample/sample.py def __init__ ( self ): \"\"\" STEP 3 : Define your benchmark \"task\" in the constructor. This Specific constructor for this SampleScript class. It has no arguments, as it will be called from the helper `main()` method. In your custom script class, you need to call the super constructor with the parameters below. TODO: pick your task (score, train, generate, compile, ...) TODO: name your framework and version \"\"\" super () . __init__ ( task = \"sample_task\" , # str framework = \"sample_framework\" , # str framework_version = \"0.0.1\" # str ) get_arg_parser ( parser = None ) classmethod STEP 4 : Define your arguments This method will be called by the main() function to add your script custom arguments to argparse, on top of standard arguments of the benchmark. TODO: align this section with your requirements. Parameters: Name Type Description Default parser argparse . ArgumentParser an existing argument parser instance None Returns: Name Type Description ArgumentParser the argument parser instance Source code in src/scripts/sample/sample.py @classmethod def get_arg_parser ( cls , parser = None ): \"\"\" STEP 4 : Define your arguments This method will be called by the main() function to add your script custom arguments to argparse, on top of standard arguments of the benchmark. TODO: align this section with your requirements. Args: parser (argparse.ArgumentParser): an existing argument parser instance Returns: ArgumentParser: the argument parser instance \"\"\" # IMPORTANT: call this to add generic benchmark arguments parser = RunnableScript . get_arg_parser ( parser ) # add arguments that are specific to your script # here's a couple examples group_i = parser . add_argument_group ( f \"I/O Arguments [ { __name__ } : { cls . __name__ } ]\" ) group_i . add_argument ( \"--data\" , required = True , type = input_file_path , # use this helper type for a directory containing a single file help = \"Some input location (directory containing a unique file)\" , ) group_i . add_argument ( \"--model\" , required = True , type = input_file_path , # use this helper type for a directory containing a single file help = \"Some input location (directory containing a unique file)\" , ) group_i . add_argument ( \"--output\" , required = True , default = None , type = str , help = \"Some output location (directory)\" , ) # make sure to return parser return parser run ( args , logger , metrics_logger , unknown_args ) STEP 5 : Define your run function. This is the core function of your script. You are required to override this method with your own implementation. Parameters: Name Type Description Default args argparse . namespace command line arguments provided to script required logger logging . logger a logger initialized for this script required metrics_logger common . metrics . MetricLogger to report metrics for this script, already initialized for MLFlow required unknown_args list [ str ] list of arguments not recognized during argparse required Source code in src/scripts/sample/sample.py def run ( self , args , logger , metrics_logger , unknown_args ): \"\"\" STEP 5 : Define your run function. This is the core function of your script. You are required to override this method with your own implementation. Args: args (argparse.namespace): command line arguments provided to script logger (logging.logger): a logger initialized for this script metrics_logger (common.metrics.MetricLogger): to report metrics for this script, already initialized for MLFlow unknown_args (list[str]): list of arguments not recognized during argparse \"\"\" # make sure the output argument exists os . makedirs ( args . output , exist_ok = True ) # and create your own file inside the output args . output = os . path . join ( args . output , \"predictions.txt\" ) # CUSTOM CODE STARTS HERE # below this line is user code logger . info ( f \"Loading model from { args . model } \" ) booster = lightgbm . Booster ( model_file = args . model ) # to log executing time of a code block, use log_time_block() logger . info ( f \"Loading data for inferencing\" ) with metrics_logger . log_time_block ( metric_name = \"time_data_loading\" ): inference_data = lightgbm . Dataset ( args . data , free_raw_data = False ) . construct () inference_raw_data = inference_data . get_data () # optional: add data shape as property metrics_logger . set_properties ( inference_data_length = inference_data . num_data (), inference_data_width = inference_data . num_feature (), ) # to log executing time of a code block, use log_time_block() logger . info ( f \"Running .predict()\" ) with metrics_logger . log_time_block ( metric_name = \"time_inferencing\" ): booster . predict ( data = inference_raw_data ) get_arg_parser ( parser = None ) STEP 2: main function block The section below (get_arg_parser(), main() and main block) should go unchanged, except for the name of your class. Those are standard functions we enforce in order to get some unit tests on the module (arguments parsing mainly). To ensure compatibility with shrike unit tests TODO: just replace SampleScript to the name of your class Source code in src/scripts/sample/sample.py def get_arg_parser ( parser = None ): \"\"\" STEP 2: main function block The section below (get_arg_parser(), main() and main block) should go unchanged, except for the name of your class. Those are standard functions we enforce in order to get some unit tests on the module (arguments parsing mainly). To ensure compatibility with shrike unit tests TODO: just replace SampleScript to the name of your class \"\"\" return SampleScript . get_arg_parser ( parser ) main ( cli_args = None ) To ensure compatibility with shrike unit tests Source code in src/scripts/sample/sample.py def main ( cli_args = None ): \"\"\" To ensure compatibility with shrike unit tests \"\"\" SampleScript . main ( cli_args )","title":"sample/sample.py"},{"location":"references/scripts/sample/sample/#usage","text":"This script is a tutorial sample script to explain how all the benchmark scripts are structured and standardized using the RunnableScript helper class. We've numbered the steps you need to modify and adapt this sample script to your own needs. Follow each STEP below, and their associated TODO.","title":"Usage"},{"location":"references/scripts/sample/sample/#src.scripts.sample.sample.SampleScript","text":"Bases: RunnableScript STEP 1 : Package your script as a class. This class inherits from RunnableScript, that factors duplicate code to achieve usual routines of every script in the lightgbm-benchmark repo. It has a standard main() function (see below) that you should not need to modify except for edge cases. See src/common/components.py for details on that class. TODO: name your class specifically for this script Source code in src/scripts/sample/sample.py class SampleScript ( RunnableScript ): \"\"\" STEP 1 : Package your script as a class. This class inherits from RunnableScript, that factors duplicate code to achieve usual routines of every script in the lightgbm-benchmark repo. It has a standard main() function (see below) that you should not need to modify except for edge cases. See `src/common/components.py` for details on that class. TODO: name your class specifically for this script \"\"\" def __init__ ( self ): \"\"\" STEP 3 : Define your benchmark \"task\" in the constructor. This Specific constructor for this SampleScript class. It has no arguments, as it will be called from the helper `main()` method. In your custom script class, you need to call the super constructor with the parameters below. TODO: pick your task (score, train, generate, compile, ...) TODO: name your framework and version \"\"\" super () . __init__ ( task = \"sample_task\" , # str framework = \"sample_framework\" , # str framework_version = \"0.0.1\" # str ) @classmethod def get_arg_parser ( cls , parser = None ): \"\"\" STEP 4 : Define your arguments This method will be called by the main() function to add your script custom arguments to argparse, on top of standard arguments of the benchmark. TODO: align this section with your requirements. Args: parser (argparse.ArgumentParser): an existing argument parser instance Returns: ArgumentParser: the argument parser instance \"\"\" # IMPORTANT: call this to add generic benchmark arguments parser = RunnableScript . get_arg_parser ( parser ) # add arguments that are specific to your script # here's a couple examples group_i = parser . add_argument_group ( f \"I/O Arguments [ { __name__ } : { cls . __name__ } ]\" ) group_i . add_argument ( \"--data\" , required = True , type = input_file_path , # use this helper type for a directory containing a single file help = \"Some input location (directory containing a unique file)\" , ) group_i . add_argument ( \"--model\" , required = True , type = input_file_path , # use this helper type for a directory containing a single file help = \"Some input location (directory containing a unique file)\" , ) group_i . add_argument ( \"--output\" , required = True , default = None , type = str , help = \"Some output location (directory)\" , ) # make sure to return parser return parser def run ( self , args , logger , metrics_logger , unknown_args ): \"\"\" STEP 5 : Define your run function. This is the core function of your script. You are required to override this method with your own implementation. Args: args (argparse.namespace): command line arguments provided to script logger (logging.logger): a logger initialized for this script metrics_logger (common.metrics.MetricLogger): to report metrics for this script, already initialized for MLFlow unknown_args (list[str]): list of arguments not recognized during argparse \"\"\" # make sure the output argument exists os . makedirs ( args . output , exist_ok = True ) # and create your own file inside the output args . output = os . path . join ( args . output , \"predictions.txt\" ) # CUSTOM CODE STARTS HERE # below this line is user code logger . info ( f \"Loading model from { args . model } \" ) booster = lightgbm . Booster ( model_file = args . model ) # to log executing time of a code block, use log_time_block() logger . info ( f \"Loading data for inferencing\" ) with metrics_logger . log_time_block ( metric_name = \"time_data_loading\" ): inference_data = lightgbm . Dataset ( args . data , free_raw_data = False ) . construct () inference_raw_data = inference_data . get_data () # optional: add data shape as property metrics_logger . set_properties ( inference_data_length = inference_data . num_data (), inference_data_width = inference_data . num_feature (), ) # to log executing time of a code block, use log_time_block() logger . info ( f \"Running .predict()\" ) with metrics_logger . log_time_block ( metric_name = \"time_inferencing\" ): booster . predict ( data = inference_raw_data )","title":"SampleScript"},{"location":"references/scripts/sample/sample/#src.scripts.sample.sample.SampleScript.__init__","text":"STEP 3 : Define your benchmark \"task\" in the constructor. This Specific constructor for this SampleScript class. It has no arguments, as it will be called from the helper main() method. In your custom script class, you need to call the super constructor with the parameters below. TODO: pick your task (score, train, generate, compile, ...) TODO: name your framework and version Source code in src/scripts/sample/sample.py def __init__ ( self ): \"\"\" STEP 3 : Define your benchmark \"task\" in the constructor. This Specific constructor for this SampleScript class. It has no arguments, as it will be called from the helper `main()` method. In your custom script class, you need to call the super constructor with the parameters below. TODO: pick your task (score, train, generate, compile, ...) TODO: name your framework and version \"\"\" super () . __init__ ( task = \"sample_task\" , # str framework = \"sample_framework\" , # str framework_version = \"0.0.1\" # str )","title":"__init__()"},{"location":"references/scripts/sample/sample/#src.scripts.sample.sample.SampleScript.get_arg_parser","text":"STEP 4 : Define your arguments This method will be called by the main() function to add your script custom arguments to argparse, on top of standard arguments of the benchmark. TODO: align this section with your requirements. Parameters: Name Type Description Default parser argparse . ArgumentParser an existing argument parser instance None Returns: Name Type Description ArgumentParser the argument parser instance Source code in src/scripts/sample/sample.py @classmethod def get_arg_parser ( cls , parser = None ): \"\"\" STEP 4 : Define your arguments This method will be called by the main() function to add your script custom arguments to argparse, on top of standard arguments of the benchmark. TODO: align this section with your requirements. Args: parser (argparse.ArgumentParser): an existing argument parser instance Returns: ArgumentParser: the argument parser instance \"\"\" # IMPORTANT: call this to add generic benchmark arguments parser = RunnableScript . get_arg_parser ( parser ) # add arguments that are specific to your script # here's a couple examples group_i = parser . add_argument_group ( f \"I/O Arguments [ { __name__ } : { cls . __name__ } ]\" ) group_i . add_argument ( \"--data\" , required = True , type = input_file_path , # use this helper type for a directory containing a single file help = \"Some input location (directory containing a unique file)\" , ) group_i . add_argument ( \"--model\" , required = True , type = input_file_path , # use this helper type for a directory containing a single file help = \"Some input location (directory containing a unique file)\" , ) group_i . add_argument ( \"--output\" , required = True , default = None , type = str , help = \"Some output location (directory)\" , ) # make sure to return parser return parser","title":"get_arg_parser()"},{"location":"references/scripts/sample/sample/#src.scripts.sample.sample.SampleScript.run","text":"STEP 5 : Define your run function. This is the core function of your script. You are required to override this method with your own implementation. Parameters: Name Type Description Default args argparse . namespace command line arguments provided to script required logger logging . logger a logger initialized for this script required metrics_logger common . metrics . MetricLogger to report metrics for this script, already initialized for MLFlow required unknown_args list [ str ] list of arguments not recognized during argparse required Source code in src/scripts/sample/sample.py def run ( self , args , logger , metrics_logger , unknown_args ): \"\"\" STEP 5 : Define your run function. This is the core function of your script. You are required to override this method with your own implementation. Args: args (argparse.namespace): command line arguments provided to script logger (logging.logger): a logger initialized for this script metrics_logger (common.metrics.MetricLogger): to report metrics for this script, already initialized for MLFlow unknown_args (list[str]): list of arguments not recognized during argparse \"\"\" # make sure the output argument exists os . makedirs ( args . output , exist_ok = True ) # and create your own file inside the output args . output = os . path . join ( args . output , \"predictions.txt\" ) # CUSTOM CODE STARTS HERE # below this line is user code logger . info ( f \"Loading model from { args . model } \" ) booster = lightgbm . Booster ( model_file = args . model ) # to log executing time of a code block, use log_time_block() logger . info ( f \"Loading data for inferencing\" ) with metrics_logger . log_time_block ( metric_name = \"time_data_loading\" ): inference_data = lightgbm . Dataset ( args . data , free_raw_data = False ) . construct () inference_raw_data = inference_data . get_data () # optional: add data shape as property metrics_logger . set_properties ( inference_data_length = inference_data . num_data (), inference_data_width = inference_data . num_feature (), ) # to log executing time of a code block, use log_time_block() logger . info ( f \"Running .predict()\" ) with metrics_logger . log_time_block ( metric_name = \"time_inferencing\" ): booster . predict ( data = inference_raw_data )","title":"run()"},{"location":"references/scripts/sample/sample/#src.scripts.sample.sample.get_arg_parser","text":"STEP 2: main function block The section below (get_arg_parser(), main() and main block) should go unchanged, except for the name of your class. Those are standard functions we enforce in order to get some unit tests on the module (arguments parsing mainly). To ensure compatibility with shrike unit tests TODO: just replace SampleScript to the name of your class Source code in src/scripts/sample/sample.py def get_arg_parser ( parser = None ): \"\"\" STEP 2: main function block The section below (get_arg_parser(), main() and main block) should go unchanged, except for the name of your class. Those are standard functions we enforce in order to get some unit tests on the module (arguments parsing mainly). To ensure compatibility with shrike unit tests TODO: just replace SampleScript to the name of your class \"\"\" return SampleScript . get_arg_parser ( parser )","title":"get_arg_parser()"},{"location":"references/scripts/sample/sample/#src.scripts.sample.sample.main","text":"To ensure compatibility with shrike unit tests Source code in src/scripts/sample/sample.py def main ( cli_args = None ): \"\"\" To ensure compatibility with shrike unit tests \"\"\" SampleScript . main ( cli_args )","title":"main()"},{"location":"references/scripts/training/lightgbm_python/","text":"WORK IN PROGRESS","title":"training/lightgbm_python/"},{"location":"results/inferencing/","text":"LightGBM Inferencing Benchmark Note The report below has been automatically generated with results from the lightgbm-benchmark repo . Warning This is work in progress, to check out current work items check the project open inferencing issues . In particular, do not trust these numbers yet until we've removed this header! Variants variant_id index framework version build cpu count num threads machine system lightgbm#0 0 lightgbm PYTHON_API.3.3.0 default 16 1 x86_64 Linux lightgbm#1 1 lightgbm C_API.3.3.0 default 16 1 x86_64 Linux lightgbm#2 2 lightgbm C_API.3.3.0 docker/lightgbm-custom/v330_patch_cpu_mpi_build.dockerfile 16 1 x86_64 Linux lightgbm#3 3 lightgbm C_API.3.2.1 docker/lightgbm-v3.2.1/linux_cpu_mpi_build.dockerfile 16 1 x86_64 Linux lightgbm#4 4 lightgbm C_API.3.2.1 docker/lightgbm-custom/v321_patch_cpu_mpi_build.dockerfile 16 1 x86_64 Linux treelite_python#5 5 treelite_python 1.3.0 default 16 1 x86_64 Linux Metric time_inferencing per prediction (usecs) inferencing task config lightgbm#0 lightgbm#1 lightgbm#2 lightgbm#3 lightgbm#4 treelite_python#5 10 trees 31 leaves 10 cols 6.71442 1.27191 1.88084 1.97014 1.50457 0.299835 10 trees 31 leaves 100 cols 10.0109 1.87281 1.89273 1.51227 1.93901 0.465536 10 trees 31 leaves 1000 cols 37.308 4.32708 4.70362 7.06888 4.72284 2.08173 100 trees 31 leaves 10 cols 18.8272 12.7087 14.9646 10.8278 16.6011 5.27241 100 trees 31 leaves 100 cols 23.524 9.6317 11.2825 15.0675 13.3228 7.3904 100 trees 31 leaves 1000 cols 45.8476 14.3042 18.5159 15.6538 14.9914 7.93605 1000 trees 31 leaves 10 cols 113.854 95.4644 104.575 93.1975 107.137 28.5369 1000 trees 31 leaves 100 cols 173.506 136.601 137.953 137.349 165.446 96.1941 1000 trees 31 leaves 1000 cols 178.49 143.14 143.734 146.814 149.186 98.9669 5000 trees 31 leaves 10 cols 395.046 394.296 425.493 326.193 443.607 251.199 5000 trees 31 leaves 100 cols 467.79 459.998 535.714 537.431 450.346 295.183 5000 trees 31 leaves 1000 cols 645.185 580.791 574.005 643.234 591.006 442.544 Percentile metrics for each variant Some variants above report percentile metrics. Those are reported by computing inferencing latency per request batch (currently, batch size = 1, and number of threads = 1). Not all variants provide those (work in progress). lightgbm#1 inferencing task config p50_usecs p90_usecs p99_usecs 10 trees 31 leaves 10 cols 1.3 1.5 1.6 10 trees 31 leaves 100 cols 1.8 2 3.1 10 trees 31 leaves 1000 cols 4.201 4.5 5.6 100 trees 31 leaves 10 cols 12.6 13.8 19.1 100 trees 31 leaves 100 cols 9.501 10 12.802 100 trees 31 leaves 1000 cols 14.301 15.601 25.001 1000 trees 31 leaves 10 cols 95.1015 98.801 108.803 1000 trees 31 leaves 100 cols 131.001 145.6 215.101 1000 trees 31 leaves 1000 cols 142.601 145.202 157.302 5000 trees 31 leaves 10 cols 383.404 430.905 584.61 5000 trees 31 leaves 100 cols 448.404 504.305 633.407 5000 trees 31 leaves 1000 cols 557.003 640.203 836.145 lightgbm#2 inferencing task config p50_usecs p90_usecs p99_usecs 10 trees 31 leaves 10 cols 1.8 2.1 2.601 10 trees 31 leaves 100 cols 1.9 2 2.10001 10 trees 31 leaves 1000 cols 4.7 4.901 5.4 100 trees 31 leaves 10 cols 13.7 15.4 37.204 100 trees 31 leaves 100 cols 10.8 12.901 17.301 100 trees 31 leaves 1000 cols 17.7 19.001 31.4 1000 trees 31 leaves 10 cols 104.003 108.703 122.603 1000 trees 31 leaves 100 cols 132.501 149.701 221.015 1000 trees 31 leaves 1000 cols 138.702 160.802 219.107 5000 trees 31 leaves 10 cols 425.024 463.626 496.927 5000 trees 31 leaves 100 cols 508.705 588.917 946.39 5000 trees 31 leaves 1000 cols 550.905 624.606 810.269 lightgbm#3 inferencing task config p50_usecs p90_usecs p99_usecs 10 trees 31 leaves 10 cols 1.8 2.3 3.1 10 trees 31 leaves 100 cols 1.5 1.6 1.9 10 trees 31 leaves 1000 cols 6.3 7.2 23.901 100 trees 31 leaves 10 cols 10.8 11.6 12.6 100 trees 31 leaves 100 cols 14.3 15.7 29.903 100 trees 31 leaves 1000 cols 15.1 16.2 27.201 1000 trees 31 leaves 10 cols 85.301 109.901 168.301 1000 trees 31 leaves 100 cols 132.401 149.601 201.402 1000 trees 31 leaves 1000 cols 146.202 148.903 161.503 5000 trees 31 leaves 10 cols 312.703 354.715 505.311 5000 trees 31 leaves 100 cols 537.638 582.651 608.343 5000 trees 31 leaves 1000 cols 641.307 654.907 667.409 lightgbm#4 inferencing task config p50_usecs p90_usecs p99_usecs 10 trees 31 leaves 10 cols 1.3 1.7 2.7 10 trees 31 leaves 100 cols 1.8 2.2 2.6 10 trees 31 leaves 1000 cols 4.7 4.9 5.3 100 trees 31 leaves 10 cols 15.7 17.2 34.9 100 trees 31 leaves 100 cols 12.201 13.501 48.706 100 trees 31 leaves 1000 cols 14.901 16.101 24.701 1000 trees 31 leaves 10 cols 97.301 136.401 201.902 1000 trees 31 leaves 100 cols 164.901 170.101 182.801 1000 trees 31 leaves 1000 cols 148.403 151.003 166.205 5000 trees 31 leaves 10 cols 439.327 492.54 602.444 5000 trees 31 leaves 100 cols 439.432 490.245 605.846 5000 trees 31 leaves 1000 cols 571.902 640.112 827.614","title":"Inferencing"},{"location":"results/inferencing/#lightgbm-inferencing-benchmark","text":"Note The report below has been automatically generated with results from the lightgbm-benchmark repo . Warning This is work in progress, to check out current work items check the project open inferencing issues . In particular, do not trust these numbers yet until we've removed this header!","title":"LightGBM Inferencing Benchmark"},{"location":"results/inferencing/#variants","text":"variant_id index framework version build cpu count num threads machine system lightgbm#0 0 lightgbm PYTHON_API.3.3.0 default 16 1 x86_64 Linux lightgbm#1 1 lightgbm C_API.3.3.0 default 16 1 x86_64 Linux lightgbm#2 2 lightgbm C_API.3.3.0 docker/lightgbm-custom/v330_patch_cpu_mpi_build.dockerfile 16 1 x86_64 Linux lightgbm#3 3 lightgbm C_API.3.2.1 docker/lightgbm-v3.2.1/linux_cpu_mpi_build.dockerfile 16 1 x86_64 Linux lightgbm#4 4 lightgbm C_API.3.2.1 docker/lightgbm-custom/v321_patch_cpu_mpi_build.dockerfile 16 1 x86_64 Linux treelite_python#5 5 treelite_python 1.3.0 default 16 1 x86_64 Linux","title":"Variants"},{"location":"results/inferencing/#metric-time_inferencing-per-prediction-usecs","text":"inferencing task config lightgbm#0 lightgbm#1 lightgbm#2 lightgbm#3 lightgbm#4 treelite_python#5 10 trees 31 leaves 10 cols 6.71442 1.27191 1.88084 1.97014 1.50457 0.299835 10 trees 31 leaves 100 cols 10.0109 1.87281 1.89273 1.51227 1.93901 0.465536 10 trees 31 leaves 1000 cols 37.308 4.32708 4.70362 7.06888 4.72284 2.08173 100 trees 31 leaves 10 cols 18.8272 12.7087 14.9646 10.8278 16.6011 5.27241 100 trees 31 leaves 100 cols 23.524 9.6317 11.2825 15.0675 13.3228 7.3904 100 trees 31 leaves 1000 cols 45.8476 14.3042 18.5159 15.6538 14.9914 7.93605 1000 trees 31 leaves 10 cols 113.854 95.4644 104.575 93.1975 107.137 28.5369 1000 trees 31 leaves 100 cols 173.506 136.601 137.953 137.349 165.446 96.1941 1000 trees 31 leaves 1000 cols 178.49 143.14 143.734 146.814 149.186 98.9669 5000 trees 31 leaves 10 cols 395.046 394.296 425.493 326.193 443.607 251.199 5000 trees 31 leaves 100 cols 467.79 459.998 535.714 537.431 450.346 295.183 5000 trees 31 leaves 1000 cols 645.185 580.791 574.005 643.234 591.006 442.544","title":"Metric time_inferencing per prediction (usecs)"},{"location":"results/inferencing/#percentile-metrics-for-each-variant","text":"Some variants above report percentile metrics. Those are reported by computing inferencing latency per request batch (currently, batch size = 1, and number of threads = 1). Not all variants provide those (work in progress).","title":"Percentile metrics for each variant"},{"location":"results/inferencing/#lightgbm1","text":"inferencing task config p50_usecs p90_usecs p99_usecs 10 trees 31 leaves 10 cols 1.3 1.5 1.6 10 trees 31 leaves 100 cols 1.8 2 3.1 10 trees 31 leaves 1000 cols 4.201 4.5 5.6 100 trees 31 leaves 10 cols 12.6 13.8 19.1 100 trees 31 leaves 100 cols 9.501 10 12.802 100 trees 31 leaves 1000 cols 14.301 15.601 25.001 1000 trees 31 leaves 10 cols 95.1015 98.801 108.803 1000 trees 31 leaves 100 cols 131.001 145.6 215.101 1000 trees 31 leaves 1000 cols 142.601 145.202 157.302 5000 trees 31 leaves 10 cols 383.404 430.905 584.61 5000 trees 31 leaves 100 cols 448.404 504.305 633.407 5000 trees 31 leaves 1000 cols 557.003 640.203 836.145","title":"lightgbm#1"},{"location":"results/inferencing/#lightgbm2","text":"inferencing task config p50_usecs p90_usecs p99_usecs 10 trees 31 leaves 10 cols 1.8 2.1 2.601 10 trees 31 leaves 100 cols 1.9 2 2.10001 10 trees 31 leaves 1000 cols 4.7 4.901 5.4 100 trees 31 leaves 10 cols 13.7 15.4 37.204 100 trees 31 leaves 100 cols 10.8 12.901 17.301 100 trees 31 leaves 1000 cols 17.7 19.001 31.4 1000 trees 31 leaves 10 cols 104.003 108.703 122.603 1000 trees 31 leaves 100 cols 132.501 149.701 221.015 1000 trees 31 leaves 1000 cols 138.702 160.802 219.107 5000 trees 31 leaves 10 cols 425.024 463.626 496.927 5000 trees 31 leaves 100 cols 508.705 588.917 946.39 5000 trees 31 leaves 1000 cols 550.905 624.606 810.269","title":"lightgbm#2"},{"location":"results/inferencing/#lightgbm3","text":"inferencing task config p50_usecs p90_usecs p99_usecs 10 trees 31 leaves 10 cols 1.8 2.3 3.1 10 trees 31 leaves 100 cols 1.5 1.6 1.9 10 trees 31 leaves 1000 cols 6.3 7.2 23.901 100 trees 31 leaves 10 cols 10.8 11.6 12.6 100 trees 31 leaves 100 cols 14.3 15.7 29.903 100 trees 31 leaves 1000 cols 15.1 16.2 27.201 1000 trees 31 leaves 10 cols 85.301 109.901 168.301 1000 trees 31 leaves 100 cols 132.401 149.601 201.402 1000 trees 31 leaves 1000 cols 146.202 148.903 161.503 5000 trees 31 leaves 10 cols 312.703 354.715 505.311 5000 trees 31 leaves 100 cols 537.638 582.651 608.343 5000 trees 31 leaves 1000 cols 641.307 654.907 667.409","title":"lightgbm#3"},{"location":"results/inferencing/#lightgbm4","text":"inferencing task config p50_usecs p90_usecs p99_usecs 10 trees 31 leaves 10 cols 1.3 1.7 2.7 10 trees 31 leaves 100 cols 1.8 2.2 2.6 10 trees 31 leaves 1000 cols 4.7 4.9 5.3 100 trees 31 leaves 10 cols 15.7 17.2 34.9 100 trees 31 leaves 100 cols 12.201 13.501 48.706 100 trees 31 leaves 1000 cols 14.901 16.101 24.701 1000 trees 31 leaves 10 cols 97.301 136.401 201.902 1000 trees 31 leaves 100 cols 164.901 170.101 182.801 1000 trees 31 leaves 1000 cols 148.403 151.003 166.205 5000 trees 31 leaves 10 cols 439.327 492.54 602.444 5000 trees 31 leaves 100 cols 439.432 490.245 605.846 5000 trees 31 leaves 1000 cols 571.902 640.112 827.614","title":"lightgbm#4"},{"location":"results/manual/","text":"Latest Benchmark results STANDARD_DS14_V2 Here's some manual results for Standard DS14-4_v2 (4 vcpus, 112 GiB memory), Linux (ubuntu 20.04) with Premium SSD LRS. Train data shape: (100000, 4001) Test data shape: (10000, 4001) Inference data shape: (100000, 4000) --- time elapsed: data_generation = 48.887274 s [tags: {'task': 'generate'}] --- time elapsed: data_saving = 202.063839 s [tags: {'task': 'generate'}] --- time elapsed: data_loading = 64.472545 s [tags: {'framework': 'lightgbm_python', 'task': 'train', 'lightgbm_version': '3.2.1'}] --- time elapsed: training = 186.153282 s [tags: {'framework': 'lightgbm_python', 'task': 'train', 'lightgbm_version': '3.2.1'}] --- time elapsed: data_loading = 174.565443 s [tags: {'framework': 'lightgbm_python', 'task': 'score', 'lightgbm_version': '3.2.1'}] --- time elapsed: inferencing = 7.100806 s [tags: {'framework': 'lightgbm_python', 'task': 'score', 'lightgbm_version': '3.2.1'}]","title":"Manual"},{"location":"results/manual/#latest-benchmark-results","text":"","title":"Latest Benchmark results"},{"location":"results/manual/#standard_ds14_v2","text":"Here's some manual results for Standard DS14-4_v2 (4 vcpus, 112 GiB memory), Linux (ubuntu 20.04) with Premium SSD LRS. Train data shape: (100000, 4001) Test data shape: (10000, 4001) Inference data shape: (100000, 4000) --- time elapsed: data_generation = 48.887274 s [tags: {'task': 'generate'}] --- time elapsed: data_saving = 202.063839 s [tags: {'task': 'generate'}] --- time elapsed: data_loading = 64.472545 s [tags: {'framework': 'lightgbm_python', 'task': 'train', 'lightgbm_version': '3.2.1'}] --- time elapsed: training = 186.153282 s [tags: {'framework': 'lightgbm_python', 'task': 'train', 'lightgbm_version': '3.2.1'}] --- time elapsed: data_loading = 174.565443 s [tags: {'framework': 'lightgbm_python', 'task': 'score', 'lightgbm_version': '3.2.1'}] --- time elapsed: inferencing = 7.100806 s [tags: {'framework': 'lightgbm_python', 'task': 'score', 'lightgbm_version': '3.2.1'}]","title":"STANDARD_DS14_V2"},{"location":"run/install/","text":"Installation Guide Install Python dependencies To enjoy this repository, you need to have an existing installation of python>=3.8 ( Miniconda or equivalent). Then, we suggest you create a conda environment and install dependencies for this benchmark: # create conda environment conda create --name lightgbmbenchmark python=3.8 -y # activate conda environment conda activate lightgbmbenchmark # install shrike library python -m pip install -r requirements.txt Install az ml cli To be able to provision azure resources, or upload data from the command line, we recommend you to use the Azure CLI v2 with the ml extension. Follow the instructions to install and set up the CLI (v2) . Build local dependencies The benchmark occasionaly relies on locally built dependencies. We will name those here. Run lightgbm train locally (requires mpi) Our lightgbm training script is distributed-ready, and currently using mpi. To be able to use this locally, either for debugging or for benchmarking, you'll need to install LightGBM with mpi support . One easy way is to install lightgbm with mpi option (requires cmake and other build tools): pip install --upgrade pip setuptools wheel pip install cmake==3.21.0 pip install lightgbm==3.2.1 --install-option=--mpi Run scripts under /src/scripts/lightgbm_cli/ Those scripts are intended to run LightGBM from the command line. Using them requires providing the path to the lightgbm executables (ex: lightgbm.exe ). To build those locally, use instructions from LightGBM . Note The /build/ directory has been added to .gitignore to allow you to build local dependencies without pushing them in git.","title":"Install"},{"location":"run/install/#installation-guide","text":"","title":"Installation Guide"},{"location":"run/install/#install-python-dependencies","text":"To enjoy this repository, you need to have an existing installation of python>=3.8 ( Miniconda or equivalent). Then, we suggest you create a conda environment and install dependencies for this benchmark: # create conda environment conda create --name lightgbmbenchmark python=3.8 -y # activate conda environment conda activate lightgbmbenchmark # install shrike library python -m pip install -r requirements.txt","title":"Install Python dependencies"},{"location":"run/install/#install-az-ml-cli","text":"To be able to provision azure resources, or upload data from the command line, we recommend you to use the Azure CLI v2 with the ml extension. Follow the instructions to install and set up the CLI (v2) .","title":"Install az ml cli"},{"location":"run/install/#build-local-dependencies","text":"The benchmark occasionaly relies on locally built dependencies. We will name those here.","title":"Build local dependencies"},{"location":"run/install/#run-lightgbm-train-locally-requires-mpi","text":"Our lightgbm training script is distributed-ready, and currently using mpi. To be able to use this locally, either for debugging or for benchmarking, you'll need to install LightGBM with mpi support . One easy way is to install lightgbm with mpi option (requires cmake and other build tools): pip install --upgrade pip setuptools wheel pip install cmake==3.21.0 pip install lightgbm==3.2.1 --install-option=--mpi","title":"Run lightgbm train locally (requires mpi)"},{"location":"run/install/#run-scripts-under-srcscriptslightgbm_cli","text":"Those scripts are intended to run LightGBM from the command line. Using them requires providing the path to the lightgbm executables (ex: lightgbm.exe ). To build those locally, use instructions from LightGBM . Note The /build/ directory has been added to .gitignore to allow you to build local dependencies without pushing them in git.","title":"Run scripts under /src/scripts/lightgbm_cli/"},{"location":"run/manual-benchmark/","text":"Run benchmark manually Objectives - By following this tutorial, you will be able to: generate synthetic data for running lightgbm run lightgbm training and inferencing scripts to measure wall time Requirements - To enjoy this tutorial, you need to have installed python dependencies locally (see instructions ). Generate synthetic data To generate a synthetic dataset based on sklearn: Bash python src/scripts/data_processing/generate_data/generate.py \\ --train_samples 30000 \\ --test_samples 3000 \\ --inferencing_samples 30000 \\ --n_features 4000 \\ --n_informative 400 \\ --random_state 5 \\ --output_train ./data/synthetic/train/ \\ --output_test ./data/synthetic/test/ \\ --output_inference ./data/synthetic/inference/ \\ --type regression Powershell python src / scripts / data_processing / generate_data / generate . py ` - -train_samples 30000 ` - -test_samples 3000 ` - -inferencing_samples 30000 ` - -n_features 4000 ` - -n_informative 400 ` - -random_state 5 ` - -output_train ./ data / synthetic / train / ` - -output_test ./ data / synthetic / test / ` - -output_inference ./ data / synthetic / inference / ` - -type regression Note Running the synthetic data generation script with these parameter values requires at least 4 GB of RAM available and generates a 754 MB training, a 75 MB testing, and a 744 MB inferencing dataset. Run training on synthetic data Bash python src/scripts/training/lightgbm_python/train.py \\ --train ./data/synthetic/train/ \\ --test ./data/synthetic/test/ \\ --export_model ./data/models/synthetic-100trees-4000cols/ \\ --objective regression \\ --boosting_type gbdt \\ --tree_learner serial \\ --metric rmse \\ --num_trees 100 \\ --num_leaves 100 \\ --min_data_in_leaf 400 \\ --learning_rate 0 .3 \\ --max_bin 16 \\ --feature_fraction 0 .15 \\ --device_type cpu Powershell python src / scripts / training / lightgbm_python / train . py ` - -train ./ data / synthetic / train / ` - -test ./ data / synthetic / test / ` - -export_model ./ data / models / synthetic - 100trees - 4000cols / ` - -objective regression ` - -boosting_type gbdt ` - -tree_learner serial ` - -metric rmse ` - -num_trees 100 ` - -num_leaves 100 ` - -min_data_in_leaf 400 ` - -learning_rate 0 . 3 ` - -max_bin 16 ` - -feature_fraction 0 . 15 ` - -device_type cpu Note --device_type cpu is optional here, if you're running on gpu you can use --device_type gpu instead. Run inferencing on synthetic data (lightgbm python) Bash python src/scripts/inferencing/lightgbm_python/score.py \\ --data ./data/synthetic/inference/ \\ --model ./data/models/synthetic-100trees-4000cols/ \\ --output ./data/outputs/predictions/ \\ --num_threads 1 Powershell python src / scripts / inferencing / lightgbm_python / score . py ` - -data ./ data / synthetic / inference / ` - -model ./ data / models / synthetic - 100trees - 4000cols / ` - -output ./ data / outputs / predictions / ` - -num_threads 1","title":"Run manually"},{"location":"run/manual-benchmark/#run-benchmark-manually","text":"Objectives - By following this tutorial, you will be able to: generate synthetic data for running lightgbm run lightgbm training and inferencing scripts to measure wall time Requirements - To enjoy this tutorial, you need to have installed python dependencies locally (see instructions ).","title":"Run benchmark manually"},{"location":"run/manual-benchmark/#generate-synthetic-data","text":"To generate a synthetic dataset based on sklearn: Bash python src/scripts/data_processing/generate_data/generate.py \\ --train_samples 30000 \\ --test_samples 3000 \\ --inferencing_samples 30000 \\ --n_features 4000 \\ --n_informative 400 \\ --random_state 5 \\ --output_train ./data/synthetic/train/ \\ --output_test ./data/synthetic/test/ \\ --output_inference ./data/synthetic/inference/ \\ --type regression Powershell python src / scripts / data_processing / generate_data / generate . py ` - -train_samples 30000 ` - -test_samples 3000 ` - -inferencing_samples 30000 ` - -n_features 4000 ` - -n_informative 400 ` - -random_state 5 ` - -output_train ./ data / synthetic / train / ` - -output_test ./ data / synthetic / test / ` - -output_inference ./ data / synthetic / inference / ` - -type regression Note Running the synthetic data generation script with these parameter values requires at least 4 GB of RAM available and generates a 754 MB training, a 75 MB testing, and a 744 MB inferencing dataset.","title":"Generate synthetic data"},{"location":"run/manual-benchmark/#run-training-on-synthetic-data","text":"Bash python src/scripts/training/lightgbm_python/train.py \\ --train ./data/synthetic/train/ \\ --test ./data/synthetic/test/ \\ --export_model ./data/models/synthetic-100trees-4000cols/ \\ --objective regression \\ --boosting_type gbdt \\ --tree_learner serial \\ --metric rmse \\ --num_trees 100 \\ --num_leaves 100 \\ --min_data_in_leaf 400 \\ --learning_rate 0 .3 \\ --max_bin 16 \\ --feature_fraction 0 .15 \\ --device_type cpu Powershell python src / scripts / training / lightgbm_python / train . py ` - -train ./ data / synthetic / train / ` - -test ./ data / synthetic / test / ` - -export_model ./ data / models / synthetic - 100trees - 4000cols / ` - -objective regression ` - -boosting_type gbdt ` - -tree_learner serial ` - -metric rmse ` - -num_trees 100 ` - -num_leaves 100 ` - -min_data_in_leaf 400 ` - -learning_rate 0 . 3 ` - -max_bin 16 ` - -feature_fraction 0 . 15 ` - -device_type cpu Note --device_type cpu is optional here, if you're running on gpu you can use --device_type gpu instead.","title":"Run training on synthetic data"},{"location":"run/manual-benchmark/#run-inferencing-on-synthetic-data-lightgbm-python","text":"Bash python src/scripts/inferencing/lightgbm_python/score.py \\ --data ./data/synthetic/inference/ \\ --model ./data/models/synthetic-100trees-4000cols/ \\ --output ./data/outputs/predictions/ \\ --num_threads 1 Powershell python src / scripts / inferencing / lightgbm_python / score . py ` - -data ./ data / synthetic / inference / ` - -model ./ data / models / synthetic - 100trees - 4000cols / ` - -output ./ data / outputs / predictions / ` - -num_threads 1","title":"Run inferencing on synthetic data (lightgbm python)"},{"location":"run/azureml/azure-setup/","text":"Provision an AzureML workspace to run the LightGBM benchmark Objectives - By following this tutorial, you will be able to setup the Azure resources you need to run the pipelines in this repo. Requirements - To enjoy this tutorial, you need to have a working Azure account and subscription (see how to create one , or how to get Visual Studio Enterprise benefits ). Option A. Create an AzureML workspace (manual route) If you don't have one already, create an AzureML workspace . In that workspace, you will need to create compute clusters . Here's what we recommend to get you started with LightGBM training using the pipelines of this repo. The names below are indicative, but we'll keep refering to those in our docs. Cluster Name SKU Node count Description cpu-cluster Standard_DS3_v2 4 A cluster for simple jobs, running on cheap VMs. linux-cpu-d32sv3 Standard_D32s_v3 10 A cluster for LightGBM itself, with a more powerful yet affordable VM. Feel free to provision more or less. linux-gpu-nv6 Standard_NV6 (gpu) 1 Optional: for trying out gpu lightgbm training (work in progress) IMPORTANT: Whenever you create those, set the minimum number of nodes to 0 so that unused clusters will automatically size down and reduce costs. Option B. Create an AzureML workspace for LightGBM using an ARM template Work in progress, feel free to contribute to the discussion on this topic in the github repo .","title":"Azure Setup"},{"location":"run/azureml/azure-setup/#provision-an-azureml-workspace-to-run-the-lightgbm-benchmark","text":"Objectives - By following this tutorial, you will be able to setup the Azure resources you need to run the pipelines in this repo. Requirements - To enjoy this tutorial, you need to have a working Azure account and subscription (see how to create one , or how to get Visual Studio Enterprise benefits ).","title":"Provision an AzureML workspace to run the LightGBM benchmark"},{"location":"run/azureml/azure-setup/#option-a-create-an-azureml-workspace-manual-route","text":"If you don't have one already, create an AzureML workspace . In that workspace, you will need to create compute clusters . Here's what we recommend to get you started with LightGBM training using the pipelines of this repo. The names below are indicative, but we'll keep refering to those in our docs. Cluster Name SKU Node count Description cpu-cluster Standard_DS3_v2 4 A cluster for simple jobs, running on cheap VMs. linux-cpu-d32sv3 Standard_D32s_v3 10 A cluster for LightGBM itself, with a more powerful yet affordable VM. Feel free to provision more or less. linux-gpu-nv6 Standard_NV6 (gpu) 1 Optional: for trying out gpu lightgbm training (work in progress) IMPORTANT: Whenever you create those, set the minimum number of nodes to 0 so that unused clusters will automatically size down and reduce costs.","title":"Option A. Create an AzureML workspace (manual route)"},{"location":"run/azureml/azure-setup/#option-b-create-an-azureml-workspace-for-lightgbm-using-an-arm-template","text":"Work in progress, feel free to contribute to the discussion on this topic in the github repo .","title":"Option B. Create an AzureML workspace for LightGBM using an ARM template"},{"location":"run/azureml/benchmark-inferencing/","text":"Run inferencing benchmark pipeline and analyze results Objectives - By following this tutorial, you will be able to: run lightgbm inferencing on multiple datasets with multiple configuration variants Requirements - To enjoy this tutorial, you need to: have an existing AzureML workspace with relevant compute resource . have setup your local environment to run our benchmarking pipelines. if you plan to use your own data, you'll need to upload custom inferencing data and model into your AzureML workspace if you plan to use synthetic data, you should first run the data generation pipeline and the training benchmark pipeline Check out the inferencing configuration Open the file under conf/experiments/lightgbm-inferencing.yaml . It contains in particular a section lightgbm_inferencing_config that we will look more closely in this section. The following yaml section contains the parameters to run in parallel several inferencing variants on a given set of inferencing data and models. experiment : name : \"lightgbm_inferencing_dev\" lightgbm_inferencing_config : # name of your particular benchmark benchmark_name : \"benchmark-dev\" # override this with a unique name # list all the data/model pairs to run inferencing with tasks : - data : name : \"data-synthetic-regression-100cols-10000samples-inference\" model : name : \"model-synthetic-regression-100cols-10trees-31leaves\" # list all inferencing frameworks and their builds variants : - framework : lightgbm_python # v3.3.0 via pypi - framework : lightgbm_c_api # v3.3.0 with C API prediction - framework : lightgbm_c_api # v3.3.0 with C API prediction build : docker/lightgbm-custom/v330_patch_cpu_mpi_build.dockerfile - framework : lightgbm_c_api # v3.2.1 with C API prediction build : docker/lightgbm-v3.2.1/linux_cpu_mpi_build.dockerfile - framework : lightgbm_c_api # v3.2.1 with C API prediction build : docker/lightgbm-custom/v321_patch_cpu_mpi_build.dockerfile - framework : treelite_python # v1.3.0 List of inferencing tasks The configuration consists in listing tasks which are made of pairs of data and model . You have two possibilities here: Note This actually corresponds to dataclasses inferencing_task and data_input_spec detailed in src/common/tasks.py . Option 1 : use a named registered dataset If your data and model have been uploaded (or generated) and registered as datasets in AzureML, you can provide the name for each : - data : name : \"data-synthetic-regression-100cols-10000samples-inference\" model : name : \"model-synthetic-regression-100cols-10trees-31leaves\" Datasets have versions in AzureML. You can specify which version you want to use under data by adding a version: field. Option 2 : use a datastore + path If your data or model is stored in an external storage, attached to your workspace as a datastore, you can provide the path to the data (works with model as well) : - data : datastore : \"externalstorage\" path : \"/custom/test/inferencing/data.txt\" model : datastore : \"externalstorage\" path : \"/custom/test/model/model100trees.txt\" List of inferencing tasks For each task, the pipeline will run inferencing \"variants\", which are different frameworks you can compare. Note This actually corresponds to dataclass inferencing_variants detailed in src/common/tasks.py . # list all inferencing frameworks and their builds variants : - framework : lightgbm_python # v3.3.0 via pypi - framework : lightgbm_c_api # v3.3.0 with C API prediction - framework : lightgbm_c_api # v3.3.0 with C API prediction build : docker/lightgbm-custom/v330_patch_cpu_mpi_build.dockerfile - framework : lightgbm_c_api # v3.2.1 with C API prediction build : docker/lightgbm-v3.2.1/linux_cpu_mpi_build.dockerfile - framework : lightgbm_c_api # v3.2.1 with C API prediction build : docker/lightgbm-custom/v321_patch_cpu_mpi_build.dockerfile - framework : treelite_python # v1.3.0 We only support a limited set of frameworks. Each framework should correspond to a folder under src/scripts/inferencing/ . For those of those framework, the pipeline will automatically add model transformation steps (ex: treelite needs to pre-compile the model). For each framework, you can override the build by specifying a dockerfile under the build: field. The path to this docker is relative to the repository root. Run the pipeline Warning For this section, we'll use custom as the name for the AzureML reference config files you created during local setup . Running a partial benchmark for testing Running the pipeline consists in launching a python script with the pipeline configuration file. python src/pipelines/azureml/lightgbm_inferencing.py --exp-config conf/experiments/lightgbm-inferencing.yaml The python script will build a pipeline based on the collection of manual scripts, each running in its own python environment. The configuration for the parameters from each scripts will be provided from the configuration file in conf/experiments/lightgbm-inferencing.yaml . Running the python command should open a browser to your workspace opening the experiment view. Running the FULL benchmark pipeline The configuration we explored above is a reduction of the full benchmark configuration that you will find under conf/experiments/benchmarks/lightgbm-inferencing.yaml . IMPORTANT : notice the change of path here, under benchmarks/ . The list in this benchmark config file corresponds to the default settings used when running the data generation pipeline and the training benchmark pipeline . python src/pipelines/azureml/lightgbm_inferencing.py --exp-config conf/experiments/benchmarks/lightgbm-inferencing.yaml Important In the configuration file, set a distinct unique benchmark_name to be able to analyze the results of your run distinctly. Analyze the results of a full run WORK IN PROGRESS","title":"Inferencing"},{"location":"run/azureml/benchmark-inferencing/#run-inferencing-benchmark-pipeline-and-analyze-results","text":"Objectives - By following this tutorial, you will be able to: run lightgbm inferencing on multiple datasets with multiple configuration variants Requirements - To enjoy this tutorial, you need to: have an existing AzureML workspace with relevant compute resource . have setup your local environment to run our benchmarking pipelines. if you plan to use your own data, you'll need to upload custom inferencing data and model into your AzureML workspace if you plan to use synthetic data, you should first run the data generation pipeline and the training benchmark pipeline","title":"Run inferencing benchmark pipeline and analyze results"},{"location":"run/azureml/benchmark-inferencing/#check-out-the-inferencing-configuration","text":"Open the file under conf/experiments/lightgbm-inferencing.yaml . It contains in particular a section lightgbm_inferencing_config that we will look more closely in this section. The following yaml section contains the parameters to run in parallel several inferencing variants on a given set of inferencing data and models. experiment : name : \"lightgbm_inferencing_dev\" lightgbm_inferencing_config : # name of your particular benchmark benchmark_name : \"benchmark-dev\" # override this with a unique name # list all the data/model pairs to run inferencing with tasks : - data : name : \"data-synthetic-regression-100cols-10000samples-inference\" model : name : \"model-synthetic-regression-100cols-10trees-31leaves\" # list all inferencing frameworks and their builds variants : - framework : lightgbm_python # v3.3.0 via pypi - framework : lightgbm_c_api # v3.3.0 with C API prediction - framework : lightgbm_c_api # v3.3.0 with C API prediction build : docker/lightgbm-custom/v330_patch_cpu_mpi_build.dockerfile - framework : lightgbm_c_api # v3.2.1 with C API prediction build : docker/lightgbm-v3.2.1/linux_cpu_mpi_build.dockerfile - framework : lightgbm_c_api # v3.2.1 with C API prediction build : docker/lightgbm-custom/v321_patch_cpu_mpi_build.dockerfile - framework : treelite_python # v1.3.0","title":"Check out the inferencing configuration"},{"location":"run/azureml/benchmark-inferencing/#list-of-inferencing-tasks","text":"The configuration consists in listing tasks which are made of pairs of data and model . You have two possibilities here: Note This actually corresponds to dataclasses inferencing_task and data_input_spec detailed in src/common/tasks.py .","title":"List of inferencing tasks"},{"location":"run/azureml/benchmark-inferencing/#option-1-use-a-named-registered-dataset","text":"If your data and model have been uploaded (or generated) and registered as datasets in AzureML, you can provide the name for each : - data : name : \"data-synthetic-regression-100cols-10000samples-inference\" model : name : \"model-synthetic-regression-100cols-10trees-31leaves\" Datasets have versions in AzureML. You can specify which version you want to use under data by adding a version: field.","title":"Option 1 : use a named registered dataset"},{"location":"run/azureml/benchmark-inferencing/#option-2-use-a-datastore-path","text":"If your data or model is stored in an external storage, attached to your workspace as a datastore, you can provide the path to the data (works with model as well) : - data : datastore : \"externalstorage\" path : \"/custom/test/inferencing/data.txt\" model : datastore : \"externalstorage\" path : \"/custom/test/model/model100trees.txt\"","title":"Option 2 : use a datastore + path"},{"location":"run/azureml/benchmark-inferencing/#list-of-inferencing-tasks_1","text":"For each task, the pipeline will run inferencing \"variants\", which are different frameworks you can compare. Note This actually corresponds to dataclass inferencing_variants detailed in src/common/tasks.py . # list all inferencing frameworks and their builds variants : - framework : lightgbm_python # v3.3.0 via pypi - framework : lightgbm_c_api # v3.3.0 with C API prediction - framework : lightgbm_c_api # v3.3.0 with C API prediction build : docker/lightgbm-custom/v330_patch_cpu_mpi_build.dockerfile - framework : lightgbm_c_api # v3.2.1 with C API prediction build : docker/lightgbm-v3.2.1/linux_cpu_mpi_build.dockerfile - framework : lightgbm_c_api # v3.2.1 with C API prediction build : docker/lightgbm-custom/v321_patch_cpu_mpi_build.dockerfile - framework : treelite_python # v1.3.0 We only support a limited set of frameworks. Each framework should correspond to a folder under src/scripts/inferencing/ . For those of those framework, the pipeline will automatically add model transformation steps (ex: treelite needs to pre-compile the model). For each framework, you can override the build by specifying a dockerfile under the build: field. The path to this docker is relative to the repository root.","title":"List of inferencing tasks"},{"location":"run/azureml/benchmark-inferencing/#run-the-pipeline","text":"Warning For this section, we'll use custom as the name for the AzureML reference config files you created during local setup .","title":"Run the pipeline"},{"location":"run/azureml/benchmark-inferencing/#running-a-partial-benchmark-for-testing","text":"Running the pipeline consists in launching a python script with the pipeline configuration file. python src/pipelines/azureml/lightgbm_inferencing.py --exp-config conf/experiments/lightgbm-inferencing.yaml The python script will build a pipeline based on the collection of manual scripts, each running in its own python environment. The configuration for the parameters from each scripts will be provided from the configuration file in conf/experiments/lightgbm-inferencing.yaml . Running the python command should open a browser to your workspace opening the experiment view.","title":"Running a partial benchmark for testing"},{"location":"run/azureml/benchmark-inferencing/#running-the-full-benchmark-pipeline","text":"The configuration we explored above is a reduction of the full benchmark configuration that you will find under conf/experiments/benchmarks/lightgbm-inferencing.yaml . IMPORTANT : notice the change of path here, under benchmarks/ . The list in this benchmark config file corresponds to the default settings used when running the data generation pipeline and the training benchmark pipeline . python src/pipelines/azureml/lightgbm_inferencing.py --exp-config conf/experiments/benchmarks/lightgbm-inferencing.yaml Important In the configuration file, set a distinct unique benchmark_name to be able to analyze the results of your run distinctly.","title":"Running the FULL benchmark pipeline"},{"location":"run/azureml/benchmark-inferencing/#analyze-the-results-of-a-full-run","text":"WORK IN PROGRESS","title":"Analyze the results of a full run"},{"location":"run/azureml/benchmark-training/","text":"WORK IN PROGRESS","title":"Training"},{"location":"run/azureml/designer-ui/","text":"Use our scripts in AzureML DesignerUI AzureML has a graphical user interface to create and run ML experiments. The components provided in this repository can be used within that UI to compose a lightgbm training pipeline. This tutorial explains how. Objectives - By following this tutorial, you will be able to: run a LightGBM training/inferencing experiment from AzureML UI Requirements - To enjoy this tutorial, you need to: have an existing AzureML workspace with relevant compute resource . Import the components Open the AzureML Components tab, and click on New Component Select the source Github repo and paste the link to the component you want to create. For instance, for lightgbm training use https://github.com/microsoft/lightgbm-benchmark/blob/main/src/scripts/training/lightgbm_python/spec.yaml . Alternatively, you can also use your local clone of this repository. To do that, use Local Files instead and browse your folders to point to the right spec.yaml . The following panel will show the details of the component, its version number, etc. Click on Create to finalize the creation process. You will need to repeat this process for each component you want to import. Below of the list of 5 components we'll use for the rest of this tutorial. Category Component Url data processing generate_data copy this link data processing lightgbm_data2bin copy this link data processing partition_data copy this link training lightgbm_python_train copy this link inferencing lightgbm_python_score copy this link Compose a sample pipeline in Designer UI In AzureML, go in the Designer tab and click + button to create a new pipeline. [See animation below] To compose a pipeline, you can simply drag-n-drop components in the authoring space in the middle of the screen. The components you're creater earlier will be \"custom components\" (as opposted to built-in). Drag-n-drop \"Generate Synthetic Data\" and \"LightGBM Training\" components into the space. Connect outputs from Generate components to inputs of Training component (check their names by hovering mouse). Provide required values (type \"rmse\" in the metric parameter of LightGBM training, leave all other values as default for now) Click on settings (\u2699\ufe0f) and pick default compute cluster cpu-cluster (see AzureML Setup instructions ). Click on \"Submit\" When submitting, a prompt will show up asking for experiment name. Click on Create new and type any name. Then Submit . The pipeline will start running. Components will be \"preparing\", then \"queued\", \"running\" and \"finalizing\" before being marked as \"completed\". Discover training metrics Default values above should provide a very minimal training of LightGBM. Once training is completed, we invite you to discover all the metrics surfaced by the component. Click on the training component, it will open the details tabs. Open the \"Metrics\" panel and check out perf metrics and training/validation metrics available in the component. Note Check out the LightGBM training component in the reference docs for an updated list of parameters and metrics . Next Steps We now invite you to discover the following tutorials: Upload your own data : upload your custom data into AzureML to run data processing, training or inferencing. Train on your own data : using the code-based experience from the repository can let you train LightGBM on custom data with greater flexibility. Check out the LightGBM training component reference documentation . Check AzureML Designer UI docs for more scenarios and tutorials outside of this repository.","title":"Designer UI"},{"location":"run/azureml/designer-ui/#use-our-scripts-in-azureml-designerui","text":"AzureML has a graphical user interface to create and run ML experiments. The components provided in this repository can be used within that UI to compose a lightgbm training pipeline. This tutorial explains how. Objectives - By following this tutorial, you will be able to: run a LightGBM training/inferencing experiment from AzureML UI Requirements - To enjoy this tutorial, you need to: have an existing AzureML workspace with relevant compute resource .","title":"Use our scripts in AzureML DesignerUI"},{"location":"run/azureml/designer-ui/#import-the-components","text":"Open the AzureML Components tab, and click on New Component Select the source Github repo and paste the link to the component you want to create. For instance, for lightgbm training use https://github.com/microsoft/lightgbm-benchmark/blob/main/src/scripts/training/lightgbm_python/spec.yaml . Alternatively, you can also use your local clone of this repository. To do that, use Local Files instead and browse your folders to point to the right spec.yaml . The following panel will show the details of the component, its version number, etc. Click on Create to finalize the creation process. You will need to repeat this process for each component you want to import. Below of the list of 5 components we'll use for the rest of this tutorial. Category Component Url data processing generate_data copy this link data processing lightgbm_data2bin copy this link data processing partition_data copy this link training lightgbm_python_train copy this link inferencing lightgbm_python_score copy this link","title":"Import the components"},{"location":"run/azureml/designer-ui/#compose-a-sample-pipeline-in-designer-ui","text":"In AzureML, go in the Designer tab and click + button to create a new pipeline. [See animation below] To compose a pipeline, you can simply drag-n-drop components in the authoring space in the middle of the screen. The components you're creater earlier will be \"custom components\" (as opposted to built-in). Drag-n-drop \"Generate Synthetic Data\" and \"LightGBM Training\" components into the space. Connect outputs from Generate components to inputs of Training component (check their names by hovering mouse). Provide required values (type \"rmse\" in the metric parameter of LightGBM training, leave all other values as default for now) Click on settings (\u2699\ufe0f) and pick default compute cluster cpu-cluster (see AzureML Setup instructions ). Click on \"Submit\" When submitting, a prompt will show up asking for experiment name. Click on Create new and type any name. Then Submit . The pipeline will start running. Components will be \"preparing\", then \"queued\", \"running\" and \"finalizing\" before being marked as \"completed\".","title":"Compose a sample pipeline in Designer UI"},{"location":"run/azureml/designer-ui/#discover-training-metrics","text":"Default values above should provide a very minimal training of LightGBM. Once training is completed, we invite you to discover all the metrics surfaced by the component. Click on the training component, it will open the details tabs. Open the \"Metrics\" panel and check out perf metrics and training/validation metrics available in the component. Note Check out the LightGBM training component in the reference docs for an updated list of parameters and metrics .","title":"Discover training metrics"},{"location":"run/azureml/designer-ui/#next-steps","text":"We now invite you to discover the following tutorials: Upload your own data : upload your custom data into AzureML to run data processing, training or inferencing. Train on your own data : using the code-based experience from the repository can let you train LightGBM on custom data with greater flexibility. Check out the LightGBM training component reference documentation . Check AzureML Designer UI docs for more scenarios and tutorials outside of this repository.","title":"Next Steps"},{"location":"run/azureml/generate-synthetic-data/","text":"Generate synthetic data for benchmarking Objectives - By following this tutorial, you will be able to: generate multiple synthetic regression datasets to benchmark lightgbm training or inferencing Requirements - To enjoy this tutorial, you need to: have an existing AzureML workspace with relevant compute resource . have setup your local environment to run our benchmarking pipelines. Check out the generation configuration Open the file under conf/experiments/data-generation.yaml . It contains in particular a section data_generation_config that we will look more closely in this section. The following yaml section contains the parameters to run a pipeline that will automatically generate synthetic data for various tasks, at various sizes. experiment : name : \"data_generation_dev\" data_generation_config : # name of your particular benchmark benchmark_name : \"benchmark-dev\" # override this with a unique name # DATA tasks : - task : \"regression\" train_samples : 100000 test_samples : 10000 inferencing_samples : 10000 n_features : 10 n_informative : 10 - task : \"regression\" train_samples : 100000 test_samples : 10000 inferencing_samples : 10000 n_features : 100 n_informative : 100 - task : \"regression\" train_samples : 100000 test_samples : 10000 inferencing_samples : 10000 n_features : 1000 n_informative : 1000 register_outputs : false register_outputs_prefix : \"data-synthetic\" In particular, the configuration consists in listing tasks which are made of key data generation arguments: - task : <regression or classification> train_samples : <number of training rows> test_samples : <number of testing rows> inferencing_samples : <number of inferencing rows> n_features : <number of features> n_informative : <how many features are informative> Note This actually corresponds to a dataclass data_generation_task detailed in src/common/tasks.py . All the items are required except n_informative . The current list corresponds to the default settings required to run the training benchmark pipeline and the inferencing benchmark pipeline . The option register_outputs can be turned to true if you want the pipeline to automatically register its outputs with a naming convention {prefix}-{task}-{n_features}cols-{samples}samples-{train|test|inference} that will be used in the next steps. register_outputs : false register_outputs_prefix : \"data-synthetic\" Run the pipeline Warning For this section, we'll use custom as the name for the AzureML reference config files you created during local setup . Running the data generation pipeline consists in launching a python script with the pipeline configuration file. python pipelines/azureml/pipelines/data_generation.py --exp-config pipelines/azureml/conf/experiments/data-generation.yaml The python script will build a pipeline based on the collection of manual scripts, each running in its own python environment. The configuration for the parameters from each scripts will be provided from the configuration file in conf/experiments/data-generation.yaml . Running the python command should open a browser to your workspace opening the experiment view. To activate output registration, you can either modify it in data-generation.yaml , or override it from the command line: python pipelines/azureml/pipelines/data_generation.py --exp-config pipelines/azureml/conf/experiments/data-generation.yaml data_generation.register_outputs = True To find the resulting datasets, go into your workspace under the Datasets tab. Next Steps When the pipeline completes and you can see the registered dataset in your workspace, you are now ready to test running the training benchmark pipeline .","title":"Generate data"},{"location":"run/azureml/generate-synthetic-data/#generate-synthetic-data-for-benchmarking","text":"Objectives - By following this tutorial, you will be able to: generate multiple synthetic regression datasets to benchmark lightgbm training or inferencing Requirements - To enjoy this tutorial, you need to: have an existing AzureML workspace with relevant compute resource . have setup your local environment to run our benchmarking pipelines.","title":"Generate synthetic data for benchmarking"},{"location":"run/azureml/generate-synthetic-data/#check-out-the-generation-configuration","text":"Open the file under conf/experiments/data-generation.yaml . It contains in particular a section data_generation_config that we will look more closely in this section. The following yaml section contains the parameters to run a pipeline that will automatically generate synthetic data for various tasks, at various sizes. experiment : name : \"data_generation_dev\" data_generation_config : # name of your particular benchmark benchmark_name : \"benchmark-dev\" # override this with a unique name # DATA tasks : - task : \"regression\" train_samples : 100000 test_samples : 10000 inferencing_samples : 10000 n_features : 10 n_informative : 10 - task : \"regression\" train_samples : 100000 test_samples : 10000 inferencing_samples : 10000 n_features : 100 n_informative : 100 - task : \"regression\" train_samples : 100000 test_samples : 10000 inferencing_samples : 10000 n_features : 1000 n_informative : 1000 register_outputs : false register_outputs_prefix : \"data-synthetic\" In particular, the configuration consists in listing tasks which are made of key data generation arguments: - task : <regression or classification> train_samples : <number of training rows> test_samples : <number of testing rows> inferencing_samples : <number of inferencing rows> n_features : <number of features> n_informative : <how many features are informative> Note This actually corresponds to a dataclass data_generation_task detailed in src/common/tasks.py . All the items are required except n_informative . The current list corresponds to the default settings required to run the training benchmark pipeline and the inferencing benchmark pipeline . The option register_outputs can be turned to true if you want the pipeline to automatically register its outputs with a naming convention {prefix}-{task}-{n_features}cols-{samples}samples-{train|test|inference} that will be used in the next steps. register_outputs : false register_outputs_prefix : \"data-synthetic\"","title":"Check out the generation configuration"},{"location":"run/azureml/generate-synthetic-data/#run-the-pipeline","text":"Warning For this section, we'll use custom as the name for the AzureML reference config files you created during local setup . Running the data generation pipeline consists in launching a python script with the pipeline configuration file. python pipelines/azureml/pipelines/data_generation.py --exp-config pipelines/azureml/conf/experiments/data-generation.yaml The python script will build a pipeline based on the collection of manual scripts, each running in its own python environment. The configuration for the parameters from each scripts will be provided from the configuration file in conf/experiments/data-generation.yaml . Running the python command should open a browser to your workspace opening the experiment view. To activate output registration, you can either modify it in data-generation.yaml , or override it from the command line: python pipelines/azureml/pipelines/data_generation.py --exp-config pipelines/azureml/conf/experiments/data-generation.yaml data_generation.register_outputs = True To find the resulting datasets, go into your workspace under the Datasets tab.","title":"Run the pipeline"},{"location":"run/azureml/generate-synthetic-data/#next-steps","text":"When the pipeline completes and you can see the registered dataset in your workspace, you are now ready to test running the training benchmark pipeline .","title":"Next Steps"},{"location":"run/azureml/local-setup/","text":"Local Setup: run a sample benchmark pipeline on AzureML Objectives - By following this tutorial, you will be able to setup resources in Azure to be able to run the pipelines in this repo.: Requirements - To enjoy this tutorial, you first need to: - install the local python requirements . - provision Azure resources first , and have a working AzureML workspace. A. Edit config files to point to your AzureML workspace To be able to submit the benchmark pipelines in AzureML, you need to edit some configuration files with the right references to connect to your AzureML resources. Edit file under conf/aml/custom.yaml to match with your AzureML workspace references: # @package _group_ subscription_id : TODO resource_group : TODO workspace_name : TODO tenant : TODO auth : \"interactive\" Edit file under conf/compute/custom.yaml to match with the name of your compute targets in AzureML. Check below for reference. If you haven't created a gpu cluster, you can leave the config file as is for the gpu lines. # @package _group_ linux_cpu : \"cpu-cluster\" linux_gpu : \"linux-gpu-nv6\" windows_cpu : \"win-cpu\" Note Configs the repo asusme you use custom as name to find your aml/compute config. If in the future you have multiple aml/compute configs (ex: myotheraml.yaml ), when you'll want to run a pipeline, use arguments aml=myotheraml compute=myotheraml to override. B. Verify your setup: run a sample pipeline in your workspace Running a pipeline consists in launching a python script with a pipeline configuration file. For instance, when you run: python pipelines/azureml/pipelines/data_generation.py --exp-config pipelines/azureml/conf/experiments/data-generation.yaml The python script will build a pipeline based on the collection of manual scripts, each running in its own python environment. The configuration for the parameters from each scripts will be provided from the configuration file in conf/experiments/data-generation.yaml . # This experiment generates multiple synthetic datasets for regression # with varying number of features # # to execute: # > python src/pipelines/azureml/data_generation.py --exp-config conf/experiments/data-generation.yaml defaults : - aml : custom - compute : custom ### CUSTOM PARAMETERS ### experiment : name : \"data_generation_dev\" description : \"something interesting to say about this\" data_generation_config : # name of your particular benchmark benchmark_name : \"benchmark-dev\" # override this with a unique name # DATA tasks : - task : \"regression\" train_samples : 100000 test_samples : 10000 inferencing_samples : 10000 n_features : 10 n_informative : 10 - task : \"lambdarank\" train_samples : 100 test_samples : 100 inferencing_samples : 100 n_features : 10 n_informative : 13 n_label_classes : 5 docs_per_query : 10 train_partitions : 7 - task : \"classification\" train_samples : 100 test_samples : 100 inferencing_samples : 100 n_features : 10 n_informative : 13 n_label_classes : 3 register_outputs : false register_outputs_prefix : \"data-synthetic\" # \"{prefix}-{task}-{n_features}cols-{samples}samples-{train|test|inference}\" Running the python command should open a browser to your workspace opening the experiment view.","title":"Local Setup"},{"location":"run/azureml/local-setup/#local-setup-run-a-sample-benchmark-pipeline-on-azureml","text":"Objectives - By following this tutorial, you will be able to setup resources in Azure to be able to run the pipelines in this repo.: Requirements - To enjoy this tutorial, you first need to: - install the local python requirements . - provision Azure resources first , and have a working AzureML workspace.","title":"Local Setup: run a sample benchmark pipeline on AzureML"},{"location":"run/azureml/local-setup/#a-edit-config-files-to-point-to-your-azureml-workspace","text":"To be able to submit the benchmark pipelines in AzureML, you need to edit some configuration files with the right references to connect to your AzureML resources. Edit file under conf/aml/custom.yaml to match with your AzureML workspace references: # @package _group_ subscription_id : TODO resource_group : TODO workspace_name : TODO tenant : TODO auth : \"interactive\" Edit file under conf/compute/custom.yaml to match with the name of your compute targets in AzureML. Check below for reference. If you haven't created a gpu cluster, you can leave the config file as is for the gpu lines. # @package _group_ linux_cpu : \"cpu-cluster\" linux_gpu : \"linux-gpu-nv6\" windows_cpu : \"win-cpu\" Note Configs the repo asusme you use custom as name to find your aml/compute config. If in the future you have multiple aml/compute configs (ex: myotheraml.yaml ), when you'll want to run a pipeline, use arguments aml=myotheraml compute=myotheraml to override.","title":"A. Edit config files to point to your AzureML workspace"},{"location":"run/azureml/local-setup/#b-verify-your-setup-run-a-sample-pipeline-in-your-workspace","text":"Running a pipeline consists in launching a python script with a pipeline configuration file. For instance, when you run: python pipelines/azureml/pipelines/data_generation.py --exp-config pipelines/azureml/conf/experiments/data-generation.yaml The python script will build a pipeline based on the collection of manual scripts, each running in its own python environment. The configuration for the parameters from each scripts will be provided from the configuration file in conf/experiments/data-generation.yaml . # This experiment generates multiple synthetic datasets for regression # with varying number of features # # to execute: # > python src/pipelines/azureml/data_generation.py --exp-config conf/experiments/data-generation.yaml defaults : - aml : custom - compute : custom ### CUSTOM PARAMETERS ### experiment : name : \"data_generation_dev\" description : \"something interesting to say about this\" data_generation_config : # name of your particular benchmark benchmark_name : \"benchmark-dev\" # override this with a unique name # DATA tasks : - task : \"regression\" train_samples : 100000 test_samples : 10000 inferencing_samples : 10000 n_features : 10 n_informative : 10 - task : \"lambdarank\" train_samples : 100 test_samples : 100 inferencing_samples : 100 n_features : 10 n_informative : 13 n_label_classes : 5 docs_per_query : 10 train_partitions : 7 - task : \"classification\" train_samples : 100 test_samples : 100 inferencing_samples : 100 n_features : 10 n_informative : 13 n_label_classes : 3 register_outputs : false register_outputs_prefix : \"data-synthetic\" # \"{prefix}-{task}-{n_features}cols-{samples}samples-{train|test|inference}\" Running the python command should open a browser to your workspace opening the experiment view.","title":"B. Verify your setup: run a sample pipeline in your workspace"},{"location":"run/azureml/train-on-your-data/","text":"Run LightGBM Training on your own data in AzureML Objectives - By following this tutorial, you will be able to: run lightgbm training pipeline on your own train/test data in AzureML Requirements - To enjoy this tutorial, you need to: - have installed the local python requirements . - have an existing AzureML workspace with relevant compute resource . - have edited your config files to run the pipelines in your workspace. Get your data into AzureML There are two ways you could simply get your data into your AzureML workspace. Option A: Upload your local data into AzureML Option B: Create a dataset from an existing storage For each of those, you need to create a File dataset with your training and testing data, each provided as one unique file. Run training on your train/test datasets 1. Check out the file `conf/experiments/lightgbm_training/cpu.yaml (see below): # to execute: # > python src/pipelines/azureml/lightgbm_training.py --exp-config conf/experiments/lightgbm_training/cpu.yaml defaults : - aml : custom - compute : custom ### CUSTOM PARAMETERS ### experiment : name : \"lightgbm_training_dev\" description : \"something interesting to say about this\" lightgbm_training_config : # name of your particular benchmark benchmark_name : \"benchmark-dev\" # override this with a unique name # list all the train/test pairs to train on tasks : - train : name : \"data-synthetic-regression-100cols-100000samples-train\" test : name : \"data-synthetic-regression-100cols-10000samples-test\" task_key : \"synthetic-regression-100cols\" # optional, user to register outputs # NOTE: this example uses only 1 training (reference) # see other config files for creating training variants reference : framework : lightgbm_python # input parameters data : auto_partitioning : True # inserts partitioning to match expected number of partitions (if nodes*processes > 1) pre_convert_to_binary : False # inserts convertion of train/test data into binary to speed up training (not compatible with auto_partitioning yet) header : false label_column : \"0\" group_column : null # lightgbm training parameters training : objective : \"regression\" metric : \"rmse\" boosting : \"gbdt\" tree_learner : \"data\" num_iterations : 100 num_leaves : 31 min_data_in_leaf : 20 learning_rate : 0.1 max_bin : 255 feature_fraction : 1.0 # compute parameters device_type : \"cpu\" # you can add anything under custom_params, it will be sent as a dictionary # to the lightgbm training module to override its parameters (see lightgbm docs for list) custom_params : deterministic : True use_two_round_loading : True # compute parameters runtime : #target: null # optional: force target for this training job nodes : 1 processes : 1 # model registration # naming convention: \"{register_model_prefix}-{task_key}-{num_iterations}trees-{num_leaves}leaves-{register_model_suffix}\" output : register_model : False #register_model_prefix: \"model\" #register_model_suffix: null 2. Modify the lines below to reflect the name of your input train/test datasets: # list all the train/test pairs to train on tasks : - train : name : \"NAME OF YOUR TRAINING DATASET HERE\" test : name : \"NAME OF YOUR TESTING DATASET HERE\" Hint tasks is actually a list, if you provide multiple entries, the pipeline will train one model per train/test pair. 4. If you want the pipeline to save your model as a dataset, turn register_model to True and uncomment the lines below to name the output according to the naming convention: lightgbm_training_config : reference : # model registration # naming convention: \"{register_model_prefix}-{task_key}-{num_iterations}trees-{num_leaves}leaves-{register_model_suffix}\" output : register_model : False #register_model_prefix: \"model\" #register_model_suffix: null Hint you can decide to register the output of the pipeline later manually from the AzureML portal. 5. Run the training pipeline: python src/pipelines/azureml/lightgbm_training.py --exp-config conf/experiments/lightgbm_training/cpu.yaml That's it. Options to modify the training parameters The benchmark training pipeline is entirely configurable. There are a few key parameters in the config yaml file that will provide interesting training scenarios. We've provided a couple of typical setups in distinct config files. Feel free to explore all of them and come up with your own set of parameters. Scalable multi node training using mpi Hint Check out example config file conf/experiments/lightgbm_training/cpu.yaml . To enable multi-node training, simple modify the number of nodes under runtime : lightgbm_training_config : reference : runtime : nodes : 1 When running the pipeline, it will automatically partition the data to match with the number of nodes, and create multi-node training provisioning the required number of nodes. python src/pipelines/azureml/lightgbm_training.py --exp-config conf/experiments/lightgbm_training/cpu.yaml Gpu training (experimental) Hint Check out example config file conf/experiments/lightgbm_training/gpu.yaml . To enable gpu training, modify the options below to build a GPU-ready docker image and turn on gpu in LightGBM training: lightgbm_training_config : reference : training : device_type : \"gpu\" runtime : build : \"docker/lightgbm-v3.3.0/linux_gpu_pip.dockerfile\" When running the pipeline, it will automatically run on the gpu cluster you've named in your compute/custom.yaml file. python src/pipelines/azureml/lightgbm_training.py --exp-config conf/experiments/lightgbm_training/gpu.yaml Running a custom lightgbm build (experimental) Hint Check out example config file conf/experiments/lightgbm_training/cpu-custom.yaml . To enable training on a custom build, modify the options below: lightgbm_training_config : reference : runtime : build : \"dockers/lightgbm_cpu_mpi_custom.dockerfile\" # relative to lightgbm_python folder When running the pipeline, it will build the container from this custom dockerfile and use it to run your job. python src/pipelines/azureml/lightgbm_training.py --exp-config conf/experiments/lightgbm_training/cpu-custom.yaml Hyperarameter search using Sweep AzureML has a feature to tune model hyperparameters , we've implemented it in this training pipeline. Hint Check out example config file conf/experiments/lightgbm_training/sweep.yaml . To enable parameter sweep, just change the \"sweepable\" parameters (see below) to use the syntax allowed by AzureML sweep : lightgbm_training_config : reference : training : # \"sweepable\" training parameters num_iterations : \"choice(100, 200)\" num_leaves : \"choice(10,20,30)\" min_data_in_leaf : 20 learning_rate : 0.1 max_bin : 255 feature_fraction : 1.0 Running the pipeline with this config will automatically try multiple values for the parameters and return the best model. python src/pipelines/azureml/lightgbm_training.py --exp-config conf/experiments/lightgbm_training/sweep.yaml You can also modify the parameters of Sweep itself, see documentation on the role of each of those settings : lightgbm_training_config : reference : sweep : #primary_metric: \"node_0/valid_0.rmse\" # if you comment it out, will use \"node_0/valid_0.METRIC\" goal : \"minimize\" algorithm : \"random\" early_termination : policy_type : \"median_stopping\" evaluation_interval : 1 delay_evaluation : 5 truncation_percentage : 20 limits : max_total_trials : 100 max_concurrent_trials : 10 timeout_minutes : 60 Running multiple variants of training parameters The training pipeline allows you do benchmark multiple variants of the training parameters. The structure of lightgbm_training_config settings relies on 3 main sections: - tasks : a list of train/test dataset pairs - reference_training : parameters used as reference for lightgbm training - variants : a list of parameter overrides that apply on top of reference_training parameters. So you can create as many tasks and variants as you'd like and run them all into one single pipeline. An example use case is training on cpu versus gpu. See the example file training-cpu-vs-gpu.yaml . In this file, the variant just consists in overriding the num_iterations : lightgbm_training_config : benchmark_name : \"benchmark-cpu-num-trees\" # list all the train/test pairs to train on tasks : - train : name : \"data-synthetic-regression-10cols-100000samples-train\" test: : name : \"data-synthetic-regression-10cols-10000samples-test\" task_key : \"synthetic-regression-10cols\" # optional, user to register outputs - train : name : \"data-synthetic-regression-100cols-100000samples-train\" test: : name : \"data-synthetic-regression-100cols-10000samples-test\" task_key : \"synthetic-regression-100cols\" # optional, user to register outputs - train : name : \"data-synthetic-regression-1000cols-100000samples-train\" test: : name : \"data-synthetic-regression-1000cols-10000samples-test\" task_key : \"synthetic-regression-1000cols\" # optional, user to register outputs # reference settings for the benchmark # all variants will be based on this reference : # lots of other params here training : num_iterations : 100 # variant settings override what is in reference_training variants : - training : num_iterations : 10 - training : num_iterations : 1000 - training : num_iterations : 5000","title":"Train on your data"},{"location":"run/azureml/train-on-your-data/#run-lightgbm-training-on-your-own-data-in-azureml","text":"Objectives - By following this tutorial, you will be able to: run lightgbm training pipeline on your own train/test data in AzureML Requirements - To enjoy this tutorial, you need to: - have installed the local python requirements . - have an existing AzureML workspace with relevant compute resource . - have edited your config files to run the pipelines in your workspace.","title":"Run LightGBM Training on your own data in AzureML"},{"location":"run/azureml/train-on-your-data/#get-your-data-into-azureml","text":"There are two ways you could simply get your data into your AzureML workspace. Option A: Upload your local data into AzureML Option B: Create a dataset from an existing storage For each of those, you need to create a File dataset with your training and testing data, each provided as one unique file.","title":"Get your data into AzureML"},{"location":"run/azureml/train-on-your-data/#run-training-on-your-traintest-datasets","text":"1. Check out the file `conf/experiments/lightgbm_training/cpu.yaml (see below): # to execute: # > python src/pipelines/azureml/lightgbm_training.py --exp-config conf/experiments/lightgbm_training/cpu.yaml defaults : - aml : custom - compute : custom ### CUSTOM PARAMETERS ### experiment : name : \"lightgbm_training_dev\" description : \"something interesting to say about this\" lightgbm_training_config : # name of your particular benchmark benchmark_name : \"benchmark-dev\" # override this with a unique name # list all the train/test pairs to train on tasks : - train : name : \"data-synthetic-regression-100cols-100000samples-train\" test : name : \"data-synthetic-regression-100cols-10000samples-test\" task_key : \"synthetic-regression-100cols\" # optional, user to register outputs # NOTE: this example uses only 1 training (reference) # see other config files for creating training variants reference : framework : lightgbm_python # input parameters data : auto_partitioning : True # inserts partitioning to match expected number of partitions (if nodes*processes > 1) pre_convert_to_binary : False # inserts convertion of train/test data into binary to speed up training (not compatible with auto_partitioning yet) header : false label_column : \"0\" group_column : null # lightgbm training parameters training : objective : \"regression\" metric : \"rmse\" boosting : \"gbdt\" tree_learner : \"data\" num_iterations : 100 num_leaves : 31 min_data_in_leaf : 20 learning_rate : 0.1 max_bin : 255 feature_fraction : 1.0 # compute parameters device_type : \"cpu\" # you can add anything under custom_params, it will be sent as a dictionary # to the lightgbm training module to override its parameters (see lightgbm docs for list) custom_params : deterministic : True use_two_round_loading : True # compute parameters runtime : #target: null # optional: force target for this training job nodes : 1 processes : 1 # model registration # naming convention: \"{register_model_prefix}-{task_key}-{num_iterations}trees-{num_leaves}leaves-{register_model_suffix}\" output : register_model : False #register_model_prefix: \"model\" #register_model_suffix: null 2. Modify the lines below to reflect the name of your input train/test datasets: # list all the train/test pairs to train on tasks : - train : name : \"NAME OF YOUR TRAINING DATASET HERE\" test : name : \"NAME OF YOUR TESTING DATASET HERE\" Hint tasks is actually a list, if you provide multiple entries, the pipeline will train one model per train/test pair. 4. If you want the pipeline to save your model as a dataset, turn register_model to True and uncomment the lines below to name the output according to the naming convention: lightgbm_training_config : reference : # model registration # naming convention: \"{register_model_prefix}-{task_key}-{num_iterations}trees-{num_leaves}leaves-{register_model_suffix}\" output : register_model : False #register_model_prefix: \"model\" #register_model_suffix: null Hint you can decide to register the output of the pipeline later manually from the AzureML portal. 5. Run the training pipeline: python src/pipelines/azureml/lightgbm_training.py --exp-config conf/experiments/lightgbm_training/cpu.yaml That's it.","title":"Run training on your train/test datasets"},{"location":"run/azureml/train-on-your-data/#options-to-modify-the-training-parameters","text":"The benchmark training pipeline is entirely configurable. There are a few key parameters in the config yaml file that will provide interesting training scenarios. We've provided a couple of typical setups in distinct config files. Feel free to explore all of them and come up with your own set of parameters.","title":"Options to modify the training parameters"},{"location":"run/azureml/train-on-your-data/#scalable-multi-node-training-using-mpi","text":"Hint Check out example config file conf/experiments/lightgbm_training/cpu.yaml . To enable multi-node training, simple modify the number of nodes under runtime : lightgbm_training_config : reference : runtime : nodes : 1 When running the pipeline, it will automatically partition the data to match with the number of nodes, and create multi-node training provisioning the required number of nodes. python src/pipelines/azureml/lightgbm_training.py --exp-config conf/experiments/lightgbm_training/cpu.yaml","title":"Scalable multi node training using mpi"},{"location":"run/azureml/train-on-your-data/#gpu-training-experimental","text":"Hint Check out example config file conf/experiments/lightgbm_training/gpu.yaml . To enable gpu training, modify the options below to build a GPU-ready docker image and turn on gpu in LightGBM training: lightgbm_training_config : reference : training : device_type : \"gpu\" runtime : build : \"docker/lightgbm-v3.3.0/linux_gpu_pip.dockerfile\" When running the pipeline, it will automatically run on the gpu cluster you've named in your compute/custom.yaml file. python src/pipelines/azureml/lightgbm_training.py --exp-config conf/experiments/lightgbm_training/gpu.yaml","title":"Gpu training (experimental)"},{"location":"run/azureml/train-on-your-data/#running-a-custom-lightgbm-build-experimental","text":"Hint Check out example config file conf/experiments/lightgbm_training/cpu-custom.yaml . To enable training on a custom build, modify the options below: lightgbm_training_config : reference : runtime : build : \"dockers/lightgbm_cpu_mpi_custom.dockerfile\" # relative to lightgbm_python folder When running the pipeline, it will build the container from this custom dockerfile and use it to run your job. python src/pipelines/azureml/lightgbm_training.py --exp-config conf/experiments/lightgbm_training/cpu-custom.yaml","title":"Running a custom lightgbm build (experimental)"},{"location":"run/azureml/train-on-your-data/#hyperarameter-search-using-sweep","text":"AzureML has a feature to tune model hyperparameters , we've implemented it in this training pipeline. Hint Check out example config file conf/experiments/lightgbm_training/sweep.yaml . To enable parameter sweep, just change the \"sweepable\" parameters (see below) to use the syntax allowed by AzureML sweep : lightgbm_training_config : reference : training : # \"sweepable\" training parameters num_iterations : \"choice(100, 200)\" num_leaves : \"choice(10,20,30)\" min_data_in_leaf : 20 learning_rate : 0.1 max_bin : 255 feature_fraction : 1.0 Running the pipeline with this config will automatically try multiple values for the parameters and return the best model. python src/pipelines/azureml/lightgbm_training.py --exp-config conf/experiments/lightgbm_training/sweep.yaml You can also modify the parameters of Sweep itself, see documentation on the role of each of those settings : lightgbm_training_config : reference : sweep : #primary_metric: \"node_0/valid_0.rmse\" # if you comment it out, will use \"node_0/valid_0.METRIC\" goal : \"minimize\" algorithm : \"random\" early_termination : policy_type : \"median_stopping\" evaluation_interval : 1 delay_evaluation : 5 truncation_percentage : 20 limits : max_total_trials : 100 max_concurrent_trials : 10 timeout_minutes : 60","title":"Hyperarameter search using Sweep"},{"location":"run/azureml/train-on-your-data/#running-multiple-variants-of-training-parameters","text":"The training pipeline allows you do benchmark multiple variants of the training parameters. The structure of lightgbm_training_config settings relies on 3 main sections: - tasks : a list of train/test dataset pairs - reference_training : parameters used as reference for lightgbm training - variants : a list of parameter overrides that apply on top of reference_training parameters. So you can create as many tasks and variants as you'd like and run them all into one single pipeline. An example use case is training on cpu versus gpu. See the example file training-cpu-vs-gpu.yaml . In this file, the variant just consists in overriding the num_iterations : lightgbm_training_config : benchmark_name : \"benchmark-cpu-num-trees\" # list all the train/test pairs to train on tasks : - train : name : \"data-synthetic-regression-10cols-100000samples-train\" test: : name : \"data-synthetic-regression-10cols-10000samples-test\" task_key : \"synthetic-regression-10cols\" # optional, user to register outputs - train : name : \"data-synthetic-regression-100cols-100000samples-train\" test: : name : \"data-synthetic-regression-100cols-10000samples-test\" task_key : \"synthetic-regression-100cols\" # optional, user to register outputs - train : name : \"data-synthetic-regression-1000cols-100000samples-train\" test: : name : \"data-synthetic-regression-1000cols-10000samples-test\" task_key : \"synthetic-regression-1000cols\" # optional, user to register outputs # reference settings for the benchmark # all variants will be based on this reference : # lots of other params here training : num_iterations : 100 # variant settings override what is in reference_training variants : - training : num_iterations : 10 - training : num_iterations : 1000 - training : num_iterations : 5000","title":"Running multiple variants of training parameters"},{"location":"run/azureml/upload-your-data/","text":"Upload data sources into AzureML to run the benchmark Objectives - By following this tutorial, you will be able to: upload sample or custom data into AzureML have a train/test dataset ready to run a LightGBM training Requirements - To enjoy this tutorial, you need to: - have an existing AzureML workspace with relevant compute resource . - have installed the az ml cli (python and az ml cli). Get data into AzureML There are multiple ways to get your data into your AzureML workspace. Here's a couple: Option A: use the az ml cli to upload files from the commandline Option B: use the AzureML UI to upload your local data Option C: use the AzurEML UI to create a dataset from an existing storage Options B and C are documented in the AzureML documentation (links above). We'll show option A in the following, as we provide some yaml templates to upload standard datasets into your provisioned AzureML workspace for running our benchmark. Warning The data/ folder of our repository has been added to .gitignore to avoid uploading your own data in git. But please be careful when adding your own data into the repository folder to not commit is mistakenly. Upload a sample dataset using az ml cli Our repo has a minimal set of sample data we use for unit testing. We'll demo how to add those to your workspace as a way to show how to upload your own files using the command line. In a terminal: 1. If you haven't already, connect by typing az login To avoid having to add your workspace/resource group every time, set those as defaults: az account set --subscription <subscription ID> az configure --defaults workspace = <Azure Machine Learning workspace name> group = <resource group> 2. From the repository root, type: # to upload dummy train dataset az ml dataset create --file data/sample/unittests-regression-train.yml # to upload dummy test dataset az ml dataset create --file data/sample/unittests-regression-test.yml This will use the sample config file below to create a dataset uploading the file specified in local_path into your workspace. $schema : https://azuremlschemas.azureedge.net/latest/dataset.schema.json name : unittests-regression-sample-train local_path : ../../tests/data/regression/train/ description : Data used in lightgbm-benchmark repo as unittest sample for regression (train data) 3. To find it in the AzureML UI, get into your workspace under the Datasets tab. You'll now be able to consume this data as an input of lightgbm training or inferencing pipelines. Feel free to edit this sample file to upload your own data into AzureML from local files and folders. Upload standard benchmark datasets into AzureML Work in progress, feel free to contribute to the discussion on this topic in the github repo .","title":"Upload your data"},{"location":"run/azureml/upload-your-data/#upload-data-sources-into-azureml-to-run-the-benchmark","text":"Objectives - By following this tutorial, you will be able to: upload sample or custom data into AzureML have a train/test dataset ready to run a LightGBM training Requirements - To enjoy this tutorial, you need to: - have an existing AzureML workspace with relevant compute resource . - have installed the az ml cli (python and az ml cli).","title":"Upload data sources into AzureML to run the benchmark"},{"location":"run/azureml/upload-your-data/#get-data-into-azureml","text":"There are multiple ways to get your data into your AzureML workspace. Here's a couple: Option A: use the az ml cli to upload files from the commandline Option B: use the AzureML UI to upload your local data Option C: use the AzurEML UI to create a dataset from an existing storage Options B and C are documented in the AzureML documentation (links above). We'll show option A in the following, as we provide some yaml templates to upload standard datasets into your provisioned AzureML workspace for running our benchmark. Warning The data/ folder of our repository has been added to .gitignore to avoid uploading your own data in git. But please be careful when adding your own data into the repository folder to not commit is mistakenly.","title":"Get data into AzureML"},{"location":"run/azureml/upload-your-data/#upload-a-sample-dataset-using-az-ml-cli","text":"Our repo has a minimal set of sample data we use for unit testing. We'll demo how to add those to your workspace as a way to show how to upload your own files using the command line. In a terminal: 1. If you haven't already, connect by typing az login To avoid having to add your workspace/resource group every time, set those as defaults: az account set --subscription <subscription ID> az configure --defaults workspace = <Azure Machine Learning workspace name> group = <resource group> 2. From the repository root, type: # to upload dummy train dataset az ml dataset create --file data/sample/unittests-regression-train.yml # to upload dummy test dataset az ml dataset create --file data/sample/unittests-regression-test.yml This will use the sample config file below to create a dataset uploading the file specified in local_path into your workspace. $schema : https://azuremlschemas.azureedge.net/latest/dataset.schema.json name : unittests-regression-sample-train local_path : ../../tests/data/regression/train/ description : Data used in lightgbm-benchmark repo as unittest sample for regression (train data) 3. To find it in the AzureML UI, get into your workspace under the Datasets tab. You'll now be able to consume this data as an input of lightgbm training or inferencing pipelines. Feel free to edit this sample file to upload your own data into AzureML from local files and folders.","title":"Upload a sample dataset using az ml cli"},{"location":"run/azureml/upload-your-data/#upload-standard-benchmark-datasets-into-azureml","text":"Work in progress, feel free to contribute to the discussion on this topic in the github repo .","title":"Upload standard benchmark datasets into AzureML"}]}