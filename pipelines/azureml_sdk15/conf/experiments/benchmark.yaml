# to execute, run from /pipelines/azureml_sdk15/
# > python pipelines/benchmark.py --config-dir ./conf --config-name experiments/benchmark run.submit=True

defaults:
  - aml: sample
  - compute: sample

modules:
  manifest:
    # BASELINE PART 1
    - name: "lightgbm_python_score"
      version: null
      yaml: "lightgbm_python_score_spec.yaml"
    - name: "lightgbm_python_score_win"
      version: null
      yaml: "lightgbm_python_score_win_spec.yaml"
    - name: "treelite_python_score"
      version: null
      yaml: "treelite_python_score_spec.yaml"

# run parameters are command line arguments for running your experiment
run: # params for running pipeline
  experiment_name: "lightgbmbenchmark-dev" # IMPORTANT
  regenerate_outputs: false
  continue_on_failure: false
  verbose: false
  submit: false
  resume: false
  canary: false
  silent: false
  wait: false

module_loader: # module loading params
  use_local: "*"
  force_default_module_version: null
  force_all_module_version: null
  local_steps_folder: "../../../components/"

lightgbm_benchmark:
  data: "synthetic_inference_4000col"
  model: "model_4000col_100000train_synthetic"
  predict_disable_shape_check: true
  os: "linux"