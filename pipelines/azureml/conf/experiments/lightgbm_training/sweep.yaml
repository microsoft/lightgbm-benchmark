# to execute, run from /pipelines/azureml/
# > python pipelines/lightgbm_training.py --config-dir ./conf --config-name experiments/lightgbm_training/sweep run.submit=True

defaults:
  - aml: sample
  - compute: sample
  - modules: benchmark

# run parameters are command line arguments for running your experiment
run: # params for running pipeline
  experiment_name: "lightgbm-training-dev" # IMPORTANT
  regenerate_outputs: false
  continue_on_failure: false
  verbose: false
  submit: false
  resume: false
  canary: false
  silent: false
  wait: false

module_loader: # module loading params
  use_local: "*"
  force_default_module_version: null
  force_all_module_version: null
  local_steps_folder: "../../../../../src/scripts/"

### CUSTOM PARAMETERS ###

lightgbm_training:
  benchmark_name: "benchmark-dev"

  # list all the train/test pairs to train on
  tasks:
    - train_dataset: "data-synthetic-regression-100cols-100000samples-train"
      test_dataset: "data-synthetic-regression-100cols-10000samples-test"
      task_key: "synthetic-regression-100cols" # optional, user to register outputs

  # NOTE: this example uses only 1 training (reference)
  # see other config files for creating training variants
  reference:
    data:
      auto_partitioning: True # inserts partitioning to match expected number of partitions (if nodes*processes > 1)
      pre_convert_to_binary: True # inserts convertion of train/test data into binary to speed up training (not compatible with auto_partitioning yet)
      header: false
      label_column: "0"
      group_column: null

    # lightgbm training parameters
    training:
      # fixed values
      objective: "regression"
      metric: "rmse"
      boosting: "gbdt"
      tree_learner: "data"

      # "sweepable" training parameters
      num_iterations: "choice(100, 200)"
      num_leaves: "choice(10,20,30)"
      min_data_in_leaf: 20
      learning_rate: 0.1
      max_bin: 255
      feature_fraction: 1.0

      # compute parameters (fixed)
      device_type: "cpu"

      # you can add anything under custom_params, it will be sent as a dictionary
      # to the lightgbm training module to override its parameters (see lightgbm docs for list)
      #custom_params:
      #    deterministic: True
      #    ndcg_eval_at: "1,3,5"
      #    use_two_round_loading: True

    # compute parameters
    environment:
      #target: null # optional: force target for this training job
      nodes: 1
      processes: 1

    # model registration
    # naming convention: "{register_model_prefix}-{task_key}-{num_iterations}trees-{num_leaves}leaves-{register_model_suffix}"
    output:
      register_model: True
      register_model_prefix: "model"
      register_model_suffix: "cpu-sweep"

    # SWEEP
    sweep:
        #primary_metric: "rmse"
        goal: "minimize"
        algorithm: "random"
        early_termination:
          policy_type: "median_stopping"
          evaluation_interval: 1
          delay_evaluation: 5
          truncation_percentage: 20
        limits:
          max_total_trials: 100
          max_concurrent_trials: 10
          timeout_minutes: 60
