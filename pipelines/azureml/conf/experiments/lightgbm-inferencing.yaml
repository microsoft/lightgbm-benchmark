# This experiment runs multiple variants of lightgbm inferencing + treelite
# on a given user-defined dataset and model
#
# to execute, run from /pipelines/azureml/
# > python pipelines/lightgbm_inferencing.py --config-dir ./conf --config-name experiments/lightgbm-inferencing run.submit=True

defaults:
  - aml: sample
  - compute: sample
  - modules: benchmark

# run parameters are command line arguments for running your experiment
run: # params for running pipeline
  experiment_name: "benchmark-lightgbm-inferencing-dev" # IMPORTANT
  regenerate_outputs: false
  continue_on_failure: false
  verbose: false
  submit: false
  resume: false
  canary: false
  silent: false
  wait: false

module_loader: # module loading params
  use_local: "*"
  force_default_module_version: null
  force_all_module_version: null
  local_steps_folder: "../../../../../src/scripts/"

### CUSTOM PARAMETERS ###

lightgbm_inferencing:
  # name of your particular benchmark
  benchmark_name: "lightgbm-inferencing-dev" # override this with a unique name

  # list all the data/model pairs to run inferencing with
  tasks:
    - data:
        name: "data-synthetic-regression-100cols-10000samples-inference"
      model:
        name: "model-synthetic-regression-100cols-10trees-31leaves"

  # list all inferencing frameworks and their builds
  variants:
    - framework: lightgbm_python # v3.3.0 via pypi
    - framework: lightgbm_c_api # v3.3.0 with C API prediction
    - framework: lightgbm_c_api # v3.3.0 with C API prediction
      build: docker/lightgbm-custom/v330_patch_cpu_mpi_build.dockerfile
    - framework: lightgbm_c_api # v3.2.1 with C API prediction
      build: docker/lightgbm-v3.2.1/linux_cpu_mpi_build.dockerfile
    - framework: lightgbm_c_api # v3.2.1 with C API prediction
      build: docker/lightgbm-custom/v321_patch_cpu_mpi_build.dockerfile
    - framework: treelite_python # v1.3.0

