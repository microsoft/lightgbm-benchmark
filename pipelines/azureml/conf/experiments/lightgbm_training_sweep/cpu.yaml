# to execute, run from /pipelines/azureml/
# > python pipelines/lightgbm_training.py --config-dir ./conf --config-name experiments/lightgbm_training_sweep/cpu run.submit=True

defaults:
  - aml: sample
  - compute: sample
  - modules: benchmark

# run parameters are command line arguments for running your experiment
run: # params for running pipeline
  experiment_name: "lightgbm-training-dev" # IMPORTANT
  regenerate_outputs: false
  continue_on_failure: false
  verbose: false
  submit: false
  resume: false
  canary: false
  silent: false
  wait: false

module_loader: # module loading params
  use_local: "*"
  force_default_module_version: null
  force_all_module_version: null
  local_steps_folder: "../../../../../src/scripts/"

### CUSTOM PARAMETERS ###

lightgbm_training:
  benchmark_name: "benchmark-dev"

  # DATA
  train_dataset: "synthetic-regr-4000cols-train"
  # train_dataset_version: null # use latest if not specified
  test_dataset: "synthetic-regr-4000cols-test"
  # test_dataset_version: null # use latest if not specified

  # TRAINING
  # fixed training parameters
  objective: "regression"
  metric: "rmse"
  boosting: "gbdt"
  tree_learner: "data"
  #training_register_model_as: "synthetic-regr-4000cols-model-sweep"

  # "sweepable" training parameters
  num_iterations: "choice(100, 200)"
  num_leaves: "choice(10,20,30)"
  min_data_in_leaf: 20
  learning_rate: 0.1
  max_bin: 255
  feature_fraction: 1.0

  # COMPUTE PARAMS
  #target: null
  device_type: "cpu"
  nodes: 1
  processes: 1
  #override_docker: "../../../../../src/scripts/lightgbm_python/dockers/lightgbm_cpu_mpi_build.dockerfile"

  # SWEEP
  sweep_algorithm: "random"
  sweep_goal: "minimize"
  sweep_max_total_trials: 10
  sweep_max_concurrent_trials: 10
  sweep_timeout_minutes: 60
