$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: microsoft_aims_lightgbm_train_python
version: 2.0.0
display_name: "LightGBM Training (Python API Basic)"
type: command
distribution: 
  type: mpi
  process_count_per_instance: 1
description: >-
  **Train a LightGBM Model**
  
  This script can leverage distributed training using sockets.
  
  Many of the parameters are documented in [LightGBM parameter docs](https://lightgbm.readthedocs.io/en/latest/Parameters.html).

  To try this one in Designer UI, follow the tutorial in the [lightgbm-benchmark documentation](https://microsoft.github.io/lightgbm-benchmark/run/azureml/designer-ui/). 

is_deterministic: true

tags:
  git: https://github.com/microsoft/lightgbm-benchmark
  docs: https://microsoft.github.io/lightgbm-benchmark
  framework: lightgbm
  framework_version: 3.3.0-basic

inputs:
  # Inputs
  train:
    type: uri_folder
    description: Directory to the training data
    optional: false
  test:
    type: uri_folder
    description: Directory to the testing data
    optional: false
  parser_config_file:
    type: uri_folder
    description: directory to the transform parser config
    optional: true
  construct:
    type: boolean
    description: "Use lazy intialization during data loading phase (both train and test datasets)"
    default: True

  # Input Parameters
  header:
    type: boolean
    default: False
    description: "Does data have a header? (ex: csv/tsv)"
  label_column:
    type: string
    default: "0"
    description: "Specify label column (default=0)"
  group_column:
    type: string
    optional: true
    description: "Specify group/query column (default '')"

  # Learning Parameters
  objective:
    type: string
    default: "regression"
    enum:
      - regression
      - regression_l1
      - huber
      - fair
      - poisson
      - quantile
      - mape
      - gamma
      - tweedie
      - binary
      - multiclass
      - multiclassova
      - cross_entropy
      - cross_entropy_lambda
      - lambdarank
      - rank_xendcg
    description: "Objective function to use for training."
  metric:
    # let's not use string here to allow for custom metrics
    type: string
    default: ''
    description: "Name of the metric (ex: rmse, ndcg)"
  boosting:
    type: string
    default: "gbdt"
    enum:
      - gbdt
      - rf
      - dart
      - goss
    description: "Boosting technique"
  tree_learner:
    type: string
    default: "serial"
    enum:
      - serial
      - feature
      - data
      - voting
    description: "For distributed learning"
  label_gain:
    type: string
    optional: True
    description: "Relevant gain for labels (used only in lambdarank application)"
  num_iterations:
    type: integer
    min: 1
    default: 100
    description: "Number of trees (alias num_trees)"
  num_leaves:
    type: integer
    min: 1
    default: 31
    description: "Number of leaves"
  min_data_in_leaf:
    type: integer
    min: 1
    default: 20
    description: "Minimum data in leaves"
  learning_rate:
    type: number
    default: 0.1
    description: "Learning/shrinkage rate"
  max_bin:
    type: integer
    min: 1
    default: 255
    description: "Max number of bins that feature values will be bucketed in"
  feature_fraction:
    type: number
    default: 1.0
    description: "LightGBM will randomly select a subset of features on each iteration (tree) if feature_fraction is smaller than 1.0. For example, if you set it to 0.8, LightGBM will select 80% of features before training each tree."
  device_type:
    type: string
    default: 'cpu'
    enum:
      - cpu
      - gpu
      - cuda
    description: "Device for the tree learning, you can use GPU to achieve the faster learning (to use gpu/cuda you need a specific dockerfile)"
  multinode_driver:
    type: string
    default: 'mpi'
    enum:
      - socket
      - mpi
    description: "Communication framework used for multi-node training (socket for lightgbm basic, mpi for lightgbm compiled for mpi)"
  custom_params:
    type: string
    optional: true
    description: "Provide as a json dictionary (ex: {\"foo\":\"bar\"}) any additional custom lightgbm parameter"

  # generic benchmark parameters
  verbose:
    type: boolean
    default: False
    description: "Show debug logs"
  custom_properties:
    type: string
    description: "For benchmark analysis, provide as a json dictionary (ex: {\"foo\":\"bar\"}) anything that will be added as tags to the job"
    optional: true

  model_filename:
    type: string
    description: "Name of the model output file"
    optional: false
    default: 'model.txt'

outputs:
  model:
    type: path
    description: "LightGBM model trained"

code: "../../../"

command: >-
    python scripts/training/lightgbm_python/train.py
    --train ${{inputs.train}}
    --test ${{inputs.test}}
    $[[--parser_config_file ${{inputs.parser_config_file}}]]
    --construct ${{inputs.construct}}    
    --header ${{inputs.header}}
    --label_column ${{inputs.label_column}}
    $[[--group_column ${{inputs.group_column}}]]
    --device_type ${{inputs.device_type}}
    --multinode_driver ${{inputs.multinode_driver}}
    --objective ${{inputs.objective}}
    --metric ${{inputs.metric}}
    $[[--label_gain ${{inputs.label_gain}}]]
    $[[--custom_params "${{inputs.custom_params}}"]]
    --boosting ${{inputs.boosting}}
    --tree_learner ${{inputs.tree_learner}}
    --num_trees ${{inputs.num_iterations}}
    --num_leaves ${{inputs.num_leaves}}
    --min_data_in_leaf ${{inputs.min_data_in_leaf}}
    --learning_rate ${{inputs.learning_rate}}
    --max_bin ${{inputs.max_bin}}
    --feature_fraction ${{inputs.feature_fraction}}
    --export_model ${{outputs.model}}
    --model_filename ${{inputs.model_filename}}
    --verbose ${{inputs.verbose}}
    $[[--custom_properties "${{inputs.custom_properties}}"]]

environment:
  name: lightgbm-docker
  build:
    path: ./
    dockerfile_path: default.dockerfile
  os_type: linux
