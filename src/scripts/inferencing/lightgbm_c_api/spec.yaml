$schema: http://azureml/sdk-2-0/CommandComponent.json
name: lightgbm_c_api_score
version: 1.0.1
display_name: "LightGBM Inferencing Probe (C API)"
type: CommandComponent
description: "LightGBM inferencing using the C API, this component is intended to measure latency, not to use for production inferencing scenarios."
is_deterministic: true
inputs:
  data:
    type: AnyDirectory
    description: directory to the inference data
    optional: false
  model:
    type: AnyDirectory
    description: directory to the model
    optional: false
  predict_disable_shape_check:
    type: Boolean
    description: "control whether or not LightGBM raises an error when you try to predict on data with a different number of features than the training data"
    default: False
  n_threads:
    type: Integer
    default: 1
  verbose:
    type: Boolean
    default: False
  custom_properties:
    type: String
    description: additional custom tags for the job
    optional: true

outputs:
  predictions:
    type: AnyDirectory

command: >-
  python score.py
  --data {inputs.data}
  --model {inputs.model}
  --num_threads {inputs.n_threads}
  --output {outputs.predictions}
  --predict_disable_shape_check {inputs.predict_disable_shape_check}
  --verbose {inputs.verbose}
  [--custom_properties {inputs.custom_properties}]

environment:
  docker:
    build:
      # file path is resolved after additional includes
      dockerfile: file:./default.dockerfile
  conda:
    userManagedDependencies: true
  os: "Linux"
