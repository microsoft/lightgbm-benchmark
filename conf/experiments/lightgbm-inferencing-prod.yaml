# This experiment runs multiple variants of lightgbm inferencing + treelite
# on a given user-defined dataset and model
#
# to execute:
# > python src/pipelines/azureml/lightgbm_inferencing.py --exp-config conf/experiments/lightgbm-inferencing.yaml

defaults:
  - aml: custom_prod
  - compute: custom_prod

### CUSTOM PARAMETERS ###

experiment:
  name: "lightgbm_inferencing_prod"
  description: "something interesting to say about this"

run:
  submit: true

lightgbm_inferencing_config:
  # name of your particular benchmark
  benchmark_name: "benchmark-prod" # override this with a unique name

  # list all the data/model pairs to run inferencing with
  tasks:
    - data:
        name: "NiR4_OFE_FR_NOFF_DATA"
      model:
        name: "NiR4_OFE_LGBM"
    - data:
        name: "NiR4_HRS_FR_NOFF_DATA"
      model:
        name: "NiR4_HRS_LGBM"

  # list all inferencing frameworks and their builds
  variants:
    # - framework: lightgbm_python # v3.3.0 via pypi
    # - framework: lightgbm_c_api # v3.3.0 with C API prediction
    # - framework: lightgbm_c_api # v3.3.0 with C API prediction
    #   build: docker/lightgbm-custom/v330_patch_cpu_mpi_build.dockerfile
    # - framework: lightgbm_c_api # v3.2.1 with C API prediction
    #   build: docker/lightgbm-v3.2.1/linux_cpu_mpi_build.dockerfile
    # - framework: lightgbm_c_api # v3.2.1 with C API prediction
    #   build: docker/lightgbm-custom/v321_patch_cpu_mpi_build.dockerfile
    # - framework: lightgbm_ray # ray implementation
    - framework: lightgbm_ort # ONNX RT implementation
    - framework: treelite_python # v1.3.0
    
    # to use custom_win_cli, you need to compile your own binaries
    # see src/scripts/inferencing/custom_win_cli/static_binaries/README.md
    #- framework: custom_win_cli
