# This experiment runs multiple variants of lightgbm inferencing + treelite
# on a given user-defined dataset and model
#
# to execute, run from /pipelines/azureml/
# > python pipelines/azureml/pipelines/lightgbm_inferencing.py --exp-config pipelines/azureml/conf/experiments/lightgbm-inferencing.yaml

defaults:
  - aml: sample

compute:
  linux_cpu: "cpu-cluster"
  linux_gpu: "linux-gpu-nv6"
  windows_cpu: "win-cpu"

### CUSTOM PARAMETERS ###

experiment:
  name: "lightgbm_inferencing_dev"
  description: "something interesting to say about this"

lightgbm_inferencing_config:
  # name of your particular benchmark
  benchmark_name: "benchmark-dev" # override this with a unique name

  # list all the data/model pairs to run inferencing with
  tasks:
    - data:
        name: "data-synthetic-regression-100cols-10000samples-inference"
      model:
        name: "model-synthetic-regression-100cols-10trees-31leaves"

  # list all inferencing frameworks and their builds
  variants:
    - framework: lightgbm_python # v3.3.0 via pypi
    - framework: lightgbm_c_api # v3.3.0 with C API prediction
    - framework: lightgbm_c_api # v3.3.0 with C API prediction
      build: docker/lightgbm-custom/v330_patch_cpu_mpi_build.dockerfile
    - framework: lightgbm_c_api # v3.2.1 with C API prediction
      build: docker/lightgbm-v3.2.1/linux_cpu_mpi_build.dockerfile
    - framework: lightgbm_c_api # v3.2.1 with C API prediction
      build: docker/lightgbm-custom/v321_patch_cpu_mpi_build.dockerfile
    - framework: treelite_python # v1.3.0
    
    # to use custom_win_cli, you need to compile your own binaries
    # see src/scripts/inferencing/custom_win_cli/static_binaries/README.md
    #- framework: custom_win_cli
